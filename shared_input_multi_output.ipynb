{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T21:14:56.570442Z",
     "start_time": "2019-04-11T21:13:14.483094Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "original train data shape: 502732,\t60 \n",
      "\n",
      " \t:\n",
      "original prediction data shape: 333925,\t60 \n",
      "\n",
      " \t:\n",
      "total training / valdation shape                       id   era data_type  feature1  feature2  feature3  \\\n",
      "0       n0003126ff2349f6  era1     train   0.54836   0.31077   0.37524   \n",
      "1       n003d773d29b57ec  era1     train   0.34712   0.40275   0.42747   \n",
      "2       n0074df2dc6810b6  era1     train   0.50871   0.48639   0.47544   \n",
      "3       n0090630f530903e  era1     train   0.61363   0.40268   0.53779   \n",
      "4       n00af19089546fe9  era1     train   0.30704   0.47273   0.54495   \n",
      "5       n011d2da12b1e735  era1     train   0.52336   0.59136   0.60506   \n",
      "6       n014149cadeee55d  era1     train   0.30875   0.62510   0.35229   \n",
      "7       n0148a4dcf539aba  era1     train   0.40632   0.30590   0.43227   \n",
      "8       n015855690d31908  era1     train   0.48193   0.27060   0.50228   \n",
      "9       n0169447f4d6a10e  era1     train   0.51191   0.53663   0.42109   \n",
      "10      n01703ba4eff8fe7  era1     train   0.51829   0.52928   0.41085   \n",
      "11      n01b43e631083764  era1     train   0.61895   0.44075   0.61466   \n",
      "12      n01d1368b061433f  era1     train   0.45529   0.28276   0.61272   \n",
      "13      n01d5bfde31734cd  era1     train   0.48717   0.28982   0.38631   \n",
      "14      n0236fa89e606631  era1     train   0.42585   0.56616   0.62732   \n",
      "15      n023dfa017b93739  era1     train   0.51657   0.52352   0.41695   \n",
      "16      n0294f04019898e8  era1     train   0.45073   0.56795   0.25294   \n",
      "17      n029adf0fa156932  era1     train   0.53833   0.43619   0.58892   \n",
      "18      n02ad6457589c4af  era1     train   0.33576   0.54933   0.39919   \n",
      "19      n02bd240c01b24bd  era1     train   0.47469   0.38517   0.53743   \n",
      "20      n02c48c74a9c9719  era1     train   0.83431   0.36147   0.69324   \n",
      "21      n02cc6384ea5a025  era1     train   0.49314   0.31000   0.36538   \n",
      "22      n02e0356c793b120  era1     train   0.65183   0.40688   0.52271   \n",
      "23      n02e96eeb774238f  era1     train   0.67869   0.46261   0.38758   \n",
      "24      n030964ace7cfa93  era1     train   0.59925   0.43286   0.46698   \n",
      "25      n0311b410c7f8b9d  era1     train   0.37634   0.43578   0.43619   \n",
      "26      n03158868e546c31  era1     train   0.50558   0.47651   0.53912   \n",
      "27      n032f0f9107b0115  era1     train   0.46657   0.28710   0.44637   \n",
      "28      n03484f670162e63  era1     train   0.33571   0.45678   0.25402   \n",
      "29      n03534209c17edf5  era1     train   0.35224   0.48464   0.47109   \n",
      "...                  ...   ...       ...       ...       ...       ...   \n",
      "333895  nfe21236ca18efd6  eraX      live   0.52122   0.50453   0.59905   \n",
      "333896  nfe21ec82b2f1926  eraX      live   0.41414   0.48677   0.60896   \n",
      "333897  nfe3d1415af28cc6  eraX      live   0.71139   0.35889   0.71324   \n",
      "333898  nfe4c8dc8acdccde  eraX      live   0.50915   0.57461   0.53146   \n",
      "333899  nfe73e58b58b7090  eraX      live   0.47553   0.52872   0.41721   \n",
      "333900  nfe802b9bc95f827  eraX      live   0.68140   0.44625   0.57745   \n",
      "333901  nfe89b7568dfa36d  eraX      live   0.42321   0.42074   0.52695   \n",
      "333902  nfe9282907757e58  eraX      live   0.55282   0.52573   0.47914   \n",
      "333903  nfeab30d83fcf30d  eraX      live   0.36989   0.74019   0.67701   \n",
      "333904  nfeb7a05db0bf1a8  eraX      live   0.55757   0.58408   0.63926   \n",
      "333905  nfecd83c1fca107a  eraX      live   0.35391   0.40353   0.54034   \n",
      "333906  nfee37fbd5643e8f  eraX      live   0.38514   0.50010   0.56808   \n",
      "333907  nff095f12eed21c5  eraX      live   0.48082   0.36199   0.49564   \n",
      "333908  nff10e532282523f  eraX      live   0.45475   0.56606   0.63381   \n",
      "333909  nff185012ad15e40  eraX      live   0.63625   0.44036   0.59375   \n",
      "333910  nff1e111416ea0cb  eraX      live   0.23209   0.60640   0.49953   \n",
      "333911  nff3fde3349c3fa7  eraX      live   0.36373   0.24481   0.34591   \n",
      "333912  nff53ea682a4d232  eraX      live   0.71255   0.48826   0.57378   \n",
      "333913  nff6387a5fefd74b  eraX      live   0.74419   0.48956   0.48069   \n",
      "333914  nff6f775dfcfee2b  eraX      live   0.44564   0.24384   0.47070   \n",
      "333915  nff70a32033531d3  eraX      live   0.51020   0.52153   0.46950   \n",
      "333916  nffb679ad56c5e01  eraX      live   0.50417   0.55222   0.51376   \n",
      "333917  nffbb4d4734584ea  eraX      live   0.50035   0.45322   0.30718   \n",
      "333918  nffcbbb870146fb1  eraX      live   0.72354   0.33096   0.48361   \n",
      "333919  nffd2fbb2ca971c9  eraX      live   0.50463   0.43650   0.61034   \n",
      "333920  nffd5c84195da62a  eraX      live   0.25365   0.50848   0.46706   \n",
      "333921  nffda5075214e3cc  eraX      live   0.18931   0.53099   0.63145   \n",
      "333922  nffe61094f1141b4  eraX      live   0.49618   0.37097   0.63796   \n",
      "333923  nffea7cccc729790  eraX      live   0.35559   0.46143   0.61118   \n",
      "333924  nfffba6cc2dd7e66  eraX      live   0.21329   0.57872   0.76775   \n",
      "\n",
      "        feature4  feature5  feature6  feature7  ...  feature48  feature49  \\\n",
      "0        0.49490   0.53217   0.48388   0.50220  ...    0.55239    0.64054   \n",
      "1        0.44006   0.47866   0.44055   0.59182  ...    0.46029    0.62941   \n",
      "2        0.40306   0.53436   0.64028   0.51420  ...    0.40596    0.54731   \n",
      "3        0.37045   0.58711   0.59900   0.62428  ...    0.53878    0.47776   \n",
      "4        0.48692   0.47348   0.34695   0.41506  ...    0.46431    0.49482   \n",
      "5        0.30085   0.41742   0.47290   0.56301  ...    0.55917    0.31260   \n",
      "6        0.48021   0.71347   0.36977   0.57899  ...    0.40538    0.52690   \n",
      "7        0.61999   0.51016   0.52714   0.74017  ...    0.77664    0.51623   \n",
      "8        0.63037   0.50734   0.45545   0.41927  ...    0.52887    0.44384   \n",
      "9        0.38838   0.51222   0.53249   0.70187  ...    0.58353    0.42566   \n",
      "10       0.44842   0.49727   0.66198   0.53513  ...    0.54271    0.43434   \n",
      "11       0.51365   0.45339   0.51101   0.39488  ...    0.33293    0.30265   \n",
      "12       0.48615   0.54590   0.59111   0.55893  ...    0.55932    0.46798   \n",
      "13       0.58569   0.46994   0.51753   0.41116  ...    0.55347    0.42172   \n",
      "14       0.40727   0.36868   0.51526   0.29604  ...    0.53480    0.48525   \n",
      "15       0.39198   0.54481   0.55880   0.48240  ...    0.45665    0.50711   \n",
      "16       0.37918   0.65944   0.61828   0.62431  ...    0.44610    0.57642   \n",
      "17       0.53740   0.32951   0.44737   0.39056  ...    0.64044    0.22801   \n",
      "18       0.51449   0.44705   0.53717   0.53424  ...    0.53355    0.49124   \n",
      "19       0.46013   0.63523   0.48265   0.50777  ...    0.48970    0.41920   \n",
      "20       0.49827   0.22947   0.46324   0.49544  ...    0.75846    0.31290   \n",
      "21       0.49332   0.68842   0.59665   0.57461  ...    0.54245    0.40880   \n",
      "22       0.69467   0.62700   0.50894   0.44084  ...    0.64244    0.17882   \n",
      "23       0.33676   0.43296   0.56874   0.67007  ...    0.63132    0.46647   \n",
      "24       0.39413   0.58728   0.38304   0.60579  ...    0.50472    0.33878   \n",
      "25       0.45756   0.39833   0.58444   0.48982  ...    0.58255    0.52865   \n",
      "26       0.48904   0.43819   0.53960   0.55395  ...    0.49973    0.47976   \n",
      "27       0.50982   0.63898   0.45895   0.50982  ...    0.52851    0.43489   \n",
      "28       0.47962   0.50144   0.58369   0.62196  ...    0.79120    0.72922   \n",
      "29       0.41567   0.57632   0.34524   0.58772  ...    0.47177    0.55393   \n",
      "...          ...       ...       ...       ...  ...        ...        ...   \n",
      "333895   0.55424   0.50785   0.48992   0.43461  ...    0.56556    0.18851   \n",
      "333896   0.24298   0.51481   0.64144   0.57336  ...    0.63589    0.50188   \n",
      "333897   0.62141   0.39953   0.50117   0.38673  ...    0.43218    0.29483   \n",
      "333898   0.48220   0.43471   0.43595   0.41659  ...    0.46864    0.46443   \n",
      "333899   0.41378   0.42050   0.52153   0.51155  ...    0.53254    0.50771   \n",
      "333900   0.50875   0.45746   0.66390   0.39603  ...    0.51026    0.34917   \n",
      "333901   0.41065   0.43218   0.45313   0.54766  ...    0.50390    0.47608   \n",
      "333902   0.30024   0.53908   0.46539   0.63140  ...    0.54333    0.44089   \n",
      "333903   0.38120   0.46426   0.50050   0.53517  ...    0.42189    0.42427   \n",
      "333904   0.22444   0.50235   0.62751   0.44797  ...    0.53568    0.37471   \n",
      "333905   0.46693   0.45546   0.31799   0.20849  ...    0.56820    0.40034   \n",
      "333906   0.38002   0.46173   0.55701   0.43784  ...    0.62778    0.29370   \n",
      "333907   0.56243   0.64222   0.45990   0.65490  ...    0.47359    0.57969   \n",
      "333908   0.32118   0.45341   0.46590   0.71699  ...    0.55491    0.41783   \n",
      "333909   0.54115   0.54770   0.45458   0.47165  ...    0.36397    0.28323   \n",
      "333910   0.38615   0.54330   0.31493   0.59189  ...    0.58707    0.64331   \n",
      "333911   0.52657   0.67164   0.33474   0.59808  ...    0.71367    0.57020   \n",
      "333912   0.42519   0.45787   0.59789   0.48950  ...    0.70612    0.54224   \n",
      "333913   0.35161   0.50736   0.65030   0.56786  ...    0.61832    0.39870   \n",
      "333914   0.79470   0.36341   0.33101   0.58867  ...    0.36063    0.53175   \n",
      "333915   0.17914   0.46065   0.58877   0.38333  ...    0.25012    0.47906   \n",
      "333916   0.35170   0.33689   0.49312   0.52409  ...    0.62969    0.47749   \n",
      "333917   0.42726   0.33704   0.64439   0.69713  ...    0.71348    0.56586   \n",
      "333918   0.35055   0.60968   0.61715   0.63803  ...    0.54201    0.62339   \n",
      "333919   0.52311   0.52897   0.54492   0.49094  ...    0.46759    0.48536   \n",
      "333920   0.41834   0.48237   0.47138   0.46319  ...    0.45665    0.44475   \n",
      "333921   0.50408   0.58139   0.33963   0.31451  ...    0.40300    0.46188   \n",
      "333922   0.52798   0.56292   0.49298   0.53647  ...    0.50421    0.47972   \n",
      "333923   0.39542   0.44457   0.40371   0.63040  ...    0.68662    0.45291   \n",
      "333924   0.46303   0.49603   0.37155   0.36905  ...    0.39985    0.47919   \n",
      "\n",
      "        feature50  target_bernie  target_elizabeth  target_jordan  target_ken  \\\n",
      "0         0.52182            1.0               1.0            1.0         1.0   \n",
      "1         0.55010            1.0               1.0            1.0         1.0   \n",
      "2         0.39061            0.0               0.0            1.0         0.0   \n",
      "3         0.36835            0.0               0.0            0.0         0.0   \n",
      "4         0.60452            1.0               1.0            1.0         1.0   \n",
      "5         0.35691            0.0               0.0            0.0         0.0   \n",
      "6         0.39767            1.0               1.0            1.0         1.0   \n",
      "7         0.49104            0.0               0.0            0.0         0.0   \n",
      "8         0.57269            0.0               0.0            0.0         0.0   \n",
      "9         0.50082            1.0               1.0            1.0         1.0   \n",
      "10        0.52720            0.0               0.0            0.0         0.0   \n",
      "11        0.45951            0.0               0.0            0.0         0.0   \n",
      "12        0.53359            1.0               1.0            1.0         1.0   \n",
      "13        0.72972            0.0               0.0            0.0         0.0   \n",
      "14        0.59524            1.0               1.0            1.0         1.0   \n",
      "15        0.59955            0.0               0.0            0.0         0.0   \n",
      "16        0.62569            0.0               0.0            0.0         0.0   \n",
      "17        0.44203            1.0               1.0            1.0         1.0   \n",
      "18        0.39731            1.0               1.0            1.0         1.0   \n",
      "19        0.38751            0.0               0.0            0.0         0.0   \n",
      "20        0.61816            1.0               1.0            1.0         1.0   \n",
      "21        0.53372            0.0               1.0            0.0         0.0   \n",
      "22        0.40123            0.0               0.0            0.0         0.0   \n",
      "23        0.53402            0.0               0.0            0.0         0.0   \n",
      "24        0.56262            0.0               0.0            0.0         0.0   \n",
      "25        0.63495            0.0               0.0            0.0         0.0   \n",
      "26        0.44513            0.0               0.0            0.0         0.0   \n",
      "27        0.48806            0.0               0.0            0.0         1.0   \n",
      "28        0.49451            0.0               0.0            0.0         0.0   \n",
      "29        0.49058            1.0               1.0            1.0         1.0   \n",
      "...           ...            ...               ...            ...         ...   \n",
      "333895    0.38337            NaN               NaN            NaN         NaN   \n",
      "333896    0.65093            NaN               NaN            NaN         NaN   \n",
      "333897    0.38152            NaN               NaN            NaN         NaN   \n",
      "333898    0.40054            NaN               NaN            NaN         NaN   \n",
      "333899    0.34019            NaN               NaN            NaN         NaN   \n",
      "333900    0.60540            NaN               NaN            NaN         NaN   \n",
      "333901    0.29951            NaN               NaN            NaN         NaN   \n",
      "333902    0.59300            NaN               NaN            NaN         NaN   \n",
      "333903    0.67124            NaN               NaN            NaN         NaN   \n",
      "333904    0.38034            NaN               NaN            NaN         NaN   \n",
      "333905    0.44061            NaN               NaN            NaN         NaN   \n",
      "333906    0.61356            NaN               NaN            NaN         NaN   \n",
      "333907    0.51950            NaN               NaN            NaN         NaN   \n",
      "333908    0.62633            NaN               NaN            NaN         NaN   \n",
      "333909    0.38023            NaN               NaN            NaN         NaN   \n",
      "333910    0.47371            NaN               NaN            NaN         NaN   \n",
      "333911    0.59949            NaN               NaN            NaN         NaN   \n",
      "333912    0.57141            NaN               NaN            NaN         NaN   \n",
      "333913    0.61005            NaN               NaN            NaN         NaN   \n",
      "333914    0.63213            NaN               NaN            NaN         NaN   \n",
      "333915    0.57808            NaN               NaN            NaN         NaN   \n",
      "333916    0.51791            NaN               NaN            NaN         NaN   \n",
      "333917    0.55149            NaN               NaN            NaN         NaN   \n",
      "333918    0.40328            NaN               NaN            NaN         NaN   \n",
      "333919    0.82365            NaN               NaN            NaN         NaN   \n",
      "333920    0.54657            NaN               NaN            NaN         NaN   \n",
      "333921    0.61674            NaN               NaN            NaN         NaN   \n",
      "333922    0.61761            NaN               NaN            NaN         NaN   \n",
      "333923    0.38017            NaN               NaN            NaN         NaN   \n",
      "333924    0.73788            NaN               NaN            NaN         NaN   \n",
      "\n",
      "        target_charles  target_frank  target_hillary  \n",
      "0                  1.0           1.0             1.0  \n",
      "1                  1.0           1.0             1.0  \n",
      "2                  0.0           0.0             0.0  \n",
      "3                  0.0           0.0             0.0  \n",
      "4                  1.0           1.0             1.0  \n",
      "5                  0.0           0.0             0.0  \n",
      "6                  1.0           1.0             1.0  \n",
      "7                  0.0           0.0             0.0  \n",
      "8                  0.0           0.0             0.0  \n",
      "9                  1.0           1.0             1.0  \n",
      "10                 0.0           0.0             0.0  \n",
      "11                 0.0           0.0             0.0  \n",
      "12                 1.0           1.0             1.0  \n",
      "13                 0.0           0.0             0.0  \n",
      "14                 1.0           1.0             1.0  \n",
      "15                 1.0           1.0             0.0  \n",
      "16                 0.0           0.0             0.0  \n",
      "17                 1.0           1.0             1.0  \n",
      "18                 1.0           1.0             1.0  \n",
      "19                 0.0           0.0             1.0  \n",
      "20                 1.0           1.0             1.0  \n",
      "21                 0.0           1.0             1.0  \n",
      "22                 0.0           0.0             0.0  \n",
      "23                 0.0           0.0             0.0  \n",
      "24                 0.0           0.0             0.0  \n",
      "25                 0.0           0.0             0.0  \n",
      "26                 0.0           0.0             0.0  \n",
      "27                 0.0           0.0             1.0  \n",
      "28                 0.0           0.0             0.0  \n",
      "29                 1.0           1.0             1.0  \n",
      "...                ...           ...             ...  \n",
      "333895             NaN           NaN             NaN  \n",
      "333896             NaN           NaN             NaN  \n",
      "333897             NaN           NaN             NaN  \n",
      "333898             NaN           NaN             NaN  \n",
      "333899             NaN           NaN             NaN  \n",
      "333900             NaN           NaN             NaN  \n",
      "333901             NaN           NaN             NaN  \n",
      "333902             NaN           NaN             NaN  \n",
      "333903             NaN           NaN             NaN  \n",
      "333904             NaN           NaN             NaN  \n",
      "333905             NaN           NaN             NaN  \n",
      "333906             NaN           NaN             NaN  \n",
      "333907             NaN           NaN             NaN  \n",
      "333908             NaN           NaN             NaN  \n",
      "333909             NaN           NaN             NaN  \n",
      "333910             NaN           NaN             NaN  \n",
      "333911             NaN           NaN             NaN  \n",
      "333912             NaN           NaN             NaN  \n",
      "333913             NaN           NaN             NaN  \n",
      "333914             NaN           NaN             NaN  \n",
      "333915             NaN           NaN             NaN  \n",
      "333916             NaN           NaN             NaN  \n",
      "333917             NaN           NaN             NaN  \n",
      "333918             NaN           NaN             NaN  \n",
      "333919             NaN           NaN             NaN  \n",
      "333920             NaN           NaN             NaN  \n",
      "333921             NaN           NaN             NaN  \n",
      "333922             NaN           NaN             NaN  \n",
      "333923             NaN           NaN             NaN  \n",
      "333924             NaN           NaN             NaN  \n",
      "\n",
      "[836657 rows x 60 columns]\n",
      "['feature1', 'feature2', 'feature3', 'feature4', 'feature5', 'feature6', 'feature7', 'feature8', 'feature9', 'feature10', 'feature11', 'feature12', 'feature13', 'feature14', 'feature15', 'feature16', 'feature17', 'feature18', 'feature19', 'feature20', 'feature21', 'feature22', 'feature23', 'feature24', 'feature25', 'feature26', 'feature27', 'feature28', 'feature29', 'feature30', 'feature31', 'feature32', 'feature33', 'feature34', 'feature35', 'feature36', 'feature37', 'feature38', 'feature39', 'feature40', 'feature41', 'feature42', 'feature43', 'feature44', 'feature45', 'feature46', 'feature47', 'feature48', 'feature49', 'feature50']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_170 (Dense)               (None, 5)            255         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_171 (Dense)               (None, 5)            30          dense_170[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 5)            0           dense_171[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_172 (Dense)               (None, 10)           60          dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_184 (Dense)               (None, 20)           1020        input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_173 (Dense)               (None, 10)           110         dense_172[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_185 (Dense)               (None, 20)           420         dense_184[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_174 (Dense)               (None, 2)            22          dense_173[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_186 (Dense)               (None, 20)           420         dense_185[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_175 (Dense)               (None, 10)           30          dense_174[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_187 (Dense)               (None, 10)           210         dense_186[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_176 (Dense)               (None, 10)           110         dense_175[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_188 (Dense)               (None, 10)           110         dense_187[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_177 (Dense)               (None, 10)           110         dense_176[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_189 (Dense)               (None, 10)           110         dense_188[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_178 (Dense)               (None, 10)           110         dense_177[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_190 (Dense)               (None, 2)            22          dense_189[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_179 (Dense)               (None, 50)           550         dense_178[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_191 (Dense)               (None, 5)            15          dense_190[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 55)           0           dense_179[0][0]                  \n",
      "                                                                 dense_191[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_192 (Dense)               (None, 2)            112         concatenate_6[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 3,826\n",
      "Trainable params: 3,826\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 336830 samples, validate on 165902 samples\n",
      "Epoch 1/30\n",
      "336830/336830 [==============================] - 3s 9us/step - loss: 0.6932 - val_loss: 0.6931\n",
      "Epoch 2/30\n",
      "336830/336830 [==============================] - 2s 7us/step - loss: 0.6930 - val_loss: 0.6932\n",
      "Epoch 3/30\n",
      "336830/336830 [==============================] - 2s 6us/step - loss: 0.6929 - val_loss: 0.6929\n",
      "Epoch 4/30\n",
      "336830/336830 [==============================] - 2s 6us/step - loss: 0.6928 - val_loss: 0.6929\n",
      "Epoch 5/30\n",
      "336830/336830 [==============================] - 2s 6us/step - loss: 0.6928 - val_loss: 0.6929\n",
      "Epoch 6/30\n",
      "336830/336830 [==============================] - 2s 7us/step - loss: 0.6927 - val_loss: 0.6929\n",
      "Epoch 7/30\n",
      "336830/336830 [==============================] - 2s 6us/step - loss: 0.6927 - val_loss: 0.6931\n",
      "Epoch 8/30\n",
      "336830/336830 [==============================] - 2s 7us/step - loss: 0.6926 - val_loss: 0.6930\n",
      "Epoch 9/30\n",
      "336830/336830 [==============================] - 2s 6us/step - loss: 0.6926 - val_loss: 0.6929\n",
      "Epoch 10/30\n",
      "336830/336830 [==============================] - 2s 6us/step - loss: 0.6926 - val_loss: 0.6936\n",
      "Epoch 11/30\n",
      "336830/336830 [==============================] - 2s 6us/step - loss: 0.6926 - val_loss: 0.6931\n",
      "Epoch 12/30\n",
      "336830/336830 [==============================] - 2s 6us/step - loss: 0.6925 - val_loss: 0.6935\n",
      "Epoch 13/30\n",
      "336830/336830 [==============================] - 2s 6us/step - loss: 0.6926 - val_loss: 0.6940\n",
      "Epoch 14/30\n",
      "336830/336830 [==============================] - 2s 7us/step - loss: 0.6925 - val_loss: 0.6929\n",
      "Epoch 15/30\n",
      "336830/336830 [==============================] - 2s 7us/step - loss: 0.6925 - val_loss: 0.6928\n",
      "Epoch 16/30\n",
      "336830/336830 [==============================] - 2s 7us/step - loss: 0.6925 - val_loss: 0.6930\n",
      "Epoch 17/30\n",
      "336830/336830 [==============================] - 2s 7us/step - loss: 0.6925 - val_loss: 0.6931\n",
      "Epoch 18/30\n",
      "336830/336830 [==============================] - 2s 7us/step - loss: 0.6925 - val_loss: 0.6930\n",
      "Epoch 19/30\n",
      "336830/336830 [==============================] - 2s 7us/step - loss: 0.6924 - val_loss: 0.6934\n",
      "Epoch 20/30\n",
      "336830/336830 [==============================] - 2s 6us/step - loss: 0.6924 - val_loss: 0.6930\n",
      "Epoch 21/30\n",
      "336830/336830 [==============================] - 2s 6us/step - loss: 0.6924 - val_loss: 0.6930\n",
      "Epoch 22/30\n",
      "336830/336830 [==============================] - 2s 6us/step - loss: 0.6924 - val_loss: 0.6930\n",
      "Epoch 23/30\n",
      "336830/336830 [==============================] - 2s 6us/step - loss: 0.6924 - val_loss: 0.6932\n",
      "Epoch 24/30\n",
      "336830/336830 [==============================] - 2s 6us/step - loss: 0.6923 - val_loss: 0.6936\n",
      "Epoch 25/30\n",
      "336830/336830 [==============================] - 2s 6us/step - loss: 0.6923 - val_loss: 0.6932\n",
      "Epoch 26/30\n",
      "336830/336830 [==============================] - 2s 7us/step - loss: 0.6923 - val_loss: 0.6939\n",
      "Epoch 27/30\n",
      "336830/336830 [==============================] - 2s 6us/step - loss: 0.6923 - val_loss: 0.6941\n",
      "Epoch 28/30\n",
      "336830/336830 [==============================] - 2s 6us/step - loss: 0.6923 - val_loss: 0.6934\n",
      "Epoch 29/30\n",
      "336830/336830 [==============================] - 2s 6us/step - loss: 0.6923 - val_loss: 0.6937\n",
      "Epoch 30/30\n",
      "336830/336830 [==============================] - 2s 6us/step - loss: 0.6923 - val_loss: 0.6930\n",
      "333925/333925 [==============================] - 6s 17us/step\n",
      "era121: loss - 0.6922805333780102 consistent: True\n",
      "era122: loss - 0.6922805333780102 consistent: True\n",
      "era123: loss - 0.6922805333780102 consistent: True\n",
      "era124: loss - 0.6922805333780102 consistent: True\n",
      "era125: loss - 0.6922805333780102 consistent: True\n",
      "era126: loss - 0.6922805333780102 consistent: True\n",
      "era127: loss - 0.6922805333780102 consistent: True\n",
      "era128: loss - 0.6922805333780102 consistent: True\n",
      "era129: loss - 0.6922805333780102 consistent: True\n",
      "era130: loss - 0.6922805333780102 consistent: True\n",
      "era131: loss - 0.6922805333780102 consistent: True\n",
      "era132: loss - 0.6922805333780102 consistent: True\n",
      "era133: loss - 0.6922805333780102 consistent: True\n",
      "era134: loss - 0.6922805333780102 consistent: True\n",
      "era135: loss - 0.6922805333780102 consistent: True\n",
      "era136: loss - 0.6922805333780102 consistent: True\n",
      "era137: loss - 0.6922805333780102 consistent: True\n",
      "era138: loss - 0.6922805333780102 consistent: True\n",
      "era139: loss - 0.6922805333780102 consistent: True\n",
      "era140: loss - 0.6922805333780102 consistent: True\n",
      "era141: loss - 0.6922805333780102 consistent: True\n",
      "era142: loss - 0.6922805333780102 consistent: True\n",
      "era143: loss - 0.6922805333780102 consistent: True\n",
      "era144: loss - 0.6922805333780102 consistent: True\n",
      "era145: loss - 0.6922805333780102 consistent: True\n",
      "era146: loss - 0.6922805333780102 consistent: True\n",
      "era147: loss - 0.6922805333780102 consistent: True\n",
      "era148: loss - 0.6922805333780102 consistent: True\n",
      "era149: loss - 0.6922805333780102 consistent: True\n",
      "era150: loss - 0.6922805333780102 consistent: True\n",
      "era151: loss - 0.6922805333780102 consistent: True\n",
      "era152: loss - 0.6922805333780102 consistent: True\n",
      "era153: loss - 0.6922805333780102 consistent: True\n",
      "era154: loss - 0.6922805333780102 consistent: True\n",
      "era155: loss - 0.6922805333780102 consistent: True\n",
      "era156: loss - 0.6922805333780102 consistent: True\n",
      "era157: loss - 0.6922805333780102 consistent: True\n",
      "era158: loss - 0.6922805333780102 consistent: True\n",
      "era159: loss - 0.6922805333780102 consistent: True\n",
      "era160: loss - 0.6922805333780102 consistent: True\n",
      "era161: loss - 0.6922805333780102 consistent: True\n",
      "era162: loss - 0.6922805333780102 consistent: True\n",
      "era163: loss - 0.6922805333780102 consistent: True\n",
      "era164: loss - 0.6922805333780102 consistent: True\n",
      "era165: loss - 0.6922805333780102 consistent: True\n",
      "era166: loss - 0.6922805333780102 consistent: True\n",
      "era167: loss - 0.6922805333780102 consistent: True\n",
      "era168: loss - 0.6922805333780102 consistent: True\n",
      "era169: loss - 0.6922805333780102 consistent: True\n",
      "era170: loss - 0.6922805333780102 consistent: True\n",
      "era171: loss - 0.6922805333780102 consistent: True\n",
      "era172: loss - 0.6922805333780102 consistent: True\n",
      "era173: loss - 0.6922805333780102 consistent: True\n",
      "era174: loss - 0.6922805333780102 consistent: True\n",
      "era175: loss - 0.6922805333780102 consistent: True\n",
      "era176: loss - 0.6922805333780102 consistent: True\n",
      "era177: loss - 0.6922805333780102 consistent: True\n",
      "era178: loss - 0.6922805333780102 consistent: True\n",
      "era179: loss - 0.6922805333780102 consistent: True\n",
      "era180: loss - 0.6922805333780102 consistent: True\n",
      "era181: loss - 0.6922805333780102 consistent: True\n",
      "era182: loss - 0.6922805333780102 consistent: True\n",
      "era183: loss - 0.6922805333780102 consistent: True\n",
      "era184: loss - 0.6922805333780102 consistent: True\n",
      "era185: loss - 0.6922805333780102 consistent: True\n",
      "era186: loss - 0.6922805333780102 consistent: True\n",
      "eraX: loss - 0.6922805333780102 consistent: True\n",
      "Consistency: 1.0\n",
      "- probabilities: [0.4921982  0.46572313 0.48963478 0.48198497 0.480516  ]\n",
      "- target: 1    1.0\n",
      "2    0.0\n",
      "3    0.0\n",
      "4    0.0\n",
      "5    1.0\n",
      "Name: target_charles, dtype: float64\n",
      "- rounded probability: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "- accuracy:  0.08487834094482294\n",
      "- charles vs elizabeth corr: [[nan nan]\n",
      " [nan nan]]\n",
      "- elizabeth using charles: 0.08535749045444337\n",
      "333925/333925 [==============================] - 6s 17us/step\n",
      "- validation logloss: 0.6922805333780102\n",
      "Writing predictions to predictions.csv\n",
      "\n",
      "Writing predictions to predictions_2019-04-11_21h14m55s_Shared_input_multi_output_single_merge_1.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND' ] = 'tensorflow'\n",
    "os.environ['MKL_THREADING_LAYER'] = 'GNU'\n",
    "import keras as ks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import TensorBoard\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import Dropout, Activation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time \n",
    "\n",
    "from keras.layers.merge import concatenate\n",
    "def main():\n",
    "\n",
    "    # Set seed for reproducibility\n",
    "    NAME = \"Shared_input_multi_output_single_merge\"\n",
    "    np.random.seed(0)\n",
    "    print(\"Loading data...\")\n",
    "    # Load the data from the CSV files\n",
    "    training_data = pd.read_csv('numerai_training_data.csv', header=0)\n",
    "    print('original train data shape: {},\\t{} \\n\\n \\t:'.format(training_data.shape[0],training_data.shape[1]))\n",
    "\n",
    "    prediction_data = pd.read_csv('numerai_tournament_data.csv', header=0)\n",
    "    print('original prediction data shape: {},\\t{} \\n\\n \\t:'.format(prediction_data.shape[0],prediction_data.shape[1]))\n",
    "    \n",
    "    complete_training_data = pd.concat([training_data, prediction_data])\n",
    "    print('total training / valdation shape {}'.format(complete_training_data))\n",
    "    \n",
    "    # Transform the loaded CSV data into numpy arrays\n",
    "\n",
    "    features = [f for f in list(training_data) if \"feature\" in f]\n",
    "    print(features)\n",
    "\n",
    "    X = training_data[features]\n",
    "\n",
    "    mini= MinMaxScaler(feature_range=(0,1)) \n",
    "    X = mini.fit_transform(X)\n",
    "\n",
    "    Y = training_data[\"target_bernie\"]\n",
    "\n",
    "    Y= keras.utils.to_categorical(Y,2) \n",
    "\n",
    "    x_prediction = prediction_data[features]\n",
    "    x_prediction = mini.fit_transform(x_prediction)\n",
    "\n",
    "\n",
    "    ids = prediction_data[\"id\"]  \n",
    "\n",
    "    batch_size = 710\n",
    "\n",
    "    dropout = 0.2\n",
    "\n",
    "    dropout = 0.2\n",
    "    visible = Input(shape=(50,))\n",
    "    m1 = Dense(5, activation='relu')(visible)\n",
    "    m1 = Dense(5, activation='relu')(m1)\n",
    "    m1 = Dropout(dropout)(m1)\n",
    "    \n",
    "    m2 = Dense(5, activation='relu')(visible)\n",
    "    m2 = Dense(5, activation='relu')(m2)\n",
    "    \n",
    "    m3 = Dense(5, activation='relu')(visible)\n",
    "    m3 = Dense(5, activation='relu')(m3)\n",
    "    m3 = Dropout(dropout)(m3)\n",
    "    \n",
    "    \n",
    "    m4 = Dense(10, activation='relu')(m3)\n",
    "    m4 = Dense(10, activation='relu')(m4)\n",
    "    output1 = Dense(2, activation='sigmoid')(m4)\n",
    "    \n",
    "    #second shared input multi merge/output\n",
    "    m5 = Dense(10, activation='relu')(output1)\n",
    "    m5 = Dense(10, activation='relu')(m5)\n",
    "    m6 = Dense(10, activation='relu')(m5)\n",
    "    m6 = Dense(10,activation='relu')(m6)\n",
    "    \n",
    "    m7 = Dense(50, activation='relu')(m6)\n",
    "    output = Dense(2, activation='sigmoid')(m7)\n",
    "    \n",
    "    sig1 = Dense(20, activation='relu')(output)\n",
    "    sig1 = Dense(20, activation='relu')(sig1)\n",
    "    sig2 = Dense(20, activation='relu')(sig1)\n",
    "    \n",
    "    sig2 = Dense(20, activation='relu')(visible) \n",
    "    sig2 = Dense(20, activation='relu')(sig2)\n",
    "    sig3 = Dense(20, activation='relu')(sig2)\n",
    "    sig3 = Dense(10, activation='relu')(sig3)\n",
    "    \n",
    "    sig4 = Dense(10, activation='relu')(sig3)\n",
    "    sig4 = Dense(10, activation='relu')(sig4)\n",
    "    sig_output1 = Dense(2, activation='sigmoid')(sig4)\n",
    "    \n",
    "    #second shared input multi sig_merge/sig_output\n",
    "    sig5 = Dense(5, activation='relu')(sig_output1)\n",
    "    merge1 = concatenate([m7,sig5],axis=1)\n",
    "    sig_output = Dense(2, activation='sigmoid')(merge1)\n",
    "\n",
    "    model = Model(inputs=visible, outputs=sig_output)   \n",
    "    model.compile(loss='binary_crossentropy',optimizer='rmsprop')\n",
    "    model.summary()\n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "    model.fit(X,Y,batch_size=batch_size,epochs=30,validation_split=0.33,callbacks=[tensorboard])\n",
    "    \n",
    "\n",
    "    y_prediction = model.predict(x_prediction)\n",
    "    evaluate = model.evaluate(x_prediction,y_prediction)\n",
    "    \n",
    "    eras = prediction_data.era.unique()\n",
    "    count = 0\n",
    "    count_consistent = 0\n",
    "        \n",
    "    for era in eras:\n",
    "        count += 1\n",
    "        current_valid_data = prediction_data[prediction_data.era==era]\n",
    "        features = [f for f in list(complete_training_data) if \"feature\" in f]\n",
    "        X_valid = current_valid_data[features]\n",
    "        Y_valid = current_valid_data[\"target_bernie\"]\n",
    "        loss = evaluate\n",
    "        if (loss < -np.log(.5)):\n",
    "            consistent = True\n",
    "            count_consistent += 1\n",
    "        else:\n",
    "            consistent = False\n",
    "        print(\"{}: loss - {} consistent: {}\".format(era, loss, consistent))\n",
    "    print (\"Consistency: {}\".format(count_consistent/count))\n",
    "        \n",
    "    \n",
    "    probabilities = y_prediction[:, 1]\n",
    "    print(\"- probabilities:\", probabilities[1:6])\n",
    "\n",
    "    # We can see the probability does seem to be good at predicting the\n",
    "    # true target correctly.\n",
    "    print(\"- target:\", prediction_data['target_bernie'][1:6])\n",
    "    print(\"- rounded probability:\", [np.round(p) for p in probabilities][1:6])\n",
    "\n",
    "    # But overall the accuracy is very low.\n",
    "    correct = [\n",
    "        np.round(x) == y\n",
    "        for (x, y) in zip(probabilities, prediction_data['target_bernie'])\n",
    "    ]\n",
    "    print(\"- accuracy: \", sum(correct) / float(prediction_data.shape[0]))\n",
    "\n",
    "    tournament_corr = np.corrcoef(prediction_data['target_bernie'],\n",
    "                                  prediction_data['target_elizabeth'])\n",
    "    print(\"- bernie vs elizabeth corr:\", tournament_corr)\n",
    "    # You can see that target_elizabeth is accurate using the bernie model as well.\n",
    "    correct = [\n",
    "        np.round(x) == y\n",
    "        for (x, y) in zip(probabilities, prediction_data['target_elizabeth'])\n",
    "    ]\n",
    "    print(\"- elizabeth using bernie:\",\n",
    "          sum(correct) / float(prediction_data.shape[0]))\n",
    "\n",
    "    # Numerai measures models on logloss instead of accuracy. The lower the logloss the better.\n",
    "    # Numerai only pays models with logloss < 0.693 on the live portion of the tournament data.)\n",
    "\n",
    "    print(\"- validation logloss:\",\n",
    "          model.evaluate(x_prediction,y_prediction))\n",
    "    \n",
    "    results = y_prediction[:, 1]\n",
    "    results_df = pd.DataFrame(data={'probability_bernie':results})\n",
    "\n",
    "    joined = pd.DataFrame(ids).join(results_df)\n",
    "    pd.DataFrame(joined[:5])\n",
    "\n",
    "\n",
    "    print(\"Writing predictions to predictions.csv\")\n",
    "    path = 'predictions_{:}_{}_1'.format(time.strftime(\"%Y-%m-%d_%Hh%Mm%Ss\", time.gmtime()),NAME) + '.csv'\n",
    "    print()\n",
    "    print(\"Writing predictions to \" + path.strip())\n",
    "    joined.to_csv(path,float_format='%.15f', index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
