{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T17:13:21.650502Z",
     "start_time": "2019-04-12T17:12:19.555413Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "original train data shape: 502732,\t60 \n",
      "\n",
      " \t:\n",
      "original prediction data shape: 333925,\t60 \n",
      "\n",
      " \t:\n",
      "total training / valdation shape                       id   era data_type  feature1  feature2  feature3  \\\n",
      "0       n0003126ff2349f6  era1     train   0.54836   0.31077   0.37524   \n",
      "1       n003d773d29b57ec  era1     train   0.34712   0.40275   0.42747   \n",
      "2       n0074df2dc6810b6  era1     train   0.50871   0.48639   0.47544   \n",
      "3       n0090630f530903e  era1     train   0.61363   0.40268   0.53779   \n",
      "4       n00af19089546fe9  era1     train   0.30704   0.47273   0.54495   \n",
      "5       n011d2da12b1e735  era1     train   0.52336   0.59136   0.60506   \n",
      "6       n014149cadeee55d  era1     train   0.30875   0.62510   0.35229   \n",
      "7       n0148a4dcf539aba  era1     train   0.40632   0.30590   0.43227   \n",
      "8       n015855690d31908  era1     train   0.48193   0.27060   0.50228   \n",
      "9       n0169447f4d6a10e  era1     train   0.51191   0.53663   0.42109   \n",
      "10      n01703ba4eff8fe7  era1     train   0.51829   0.52928   0.41085   \n",
      "11      n01b43e631083764  era1     train   0.61895   0.44075   0.61466   \n",
      "12      n01d1368b061433f  era1     train   0.45529   0.28276   0.61272   \n",
      "13      n01d5bfde31734cd  era1     train   0.48717   0.28982   0.38631   \n",
      "14      n0236fa89e606631  era1     train   0.42585   0.56616   0.62732   \n",
      "15      n023dfa017b93739  era1     train   0.51657   0.52352   0.41695   \n",
      "16      n0294f04019898e8  era1     train   0.45073   0.56795   0.25294   \n",
      "17      n029adf0fa156932  era1     train   0.53833   0.43619   0.58892   \n",
      "18      n02ad6457589c4af  era1     train   0.33576   0.54933   0.39919   \n",
      "19      n02bd240c01b24bd  era1     train   0.47469   0.38517   0.53743   \n",
      "20      n02c48c74a9c9719  era1     train   0.83431   0.36147   0.69324   \n",
      "21      n02cc6384ea5a025  era1     train   0.49314   0.31000   0.36538   \n",
      "22      n02e0356c793b120  era1     train   0.65183   0.40688   0.52271   \n",
      "23      n02e96eeb774238f  era1     train   0.67869   0.46261   0.38758   \n",
      "24      n030964ace7cfa93  era1     train   0.59925   0.43286   0.46698   \n",
      "25      n0311b410c7f8b9d  era1     train   0.37634   0.43578   0.43619   \n",
      "26      n03158868e546c31  era1     train   0.50558   0.47651   0.53912   \n",
      "27      n032f0f9107b0115  era1     train   0.46657   0.28710   0.44637   \n",
      "28      n03484f670162e63  era1     train   0.33571   0.45678   0.25402   \n",
      "29      n03534209c17edf5  era1     train   0.35224   0.48464   0.47109   \n",
      "...                  ...   ...       ...       ...       ...       ...   \n",
      "333895  nfe21236ca18efd6  eraX      live   0.52122   0.50453   0.59905   \n",
      "333896  nfe21ec82b2f1926  eraX      live   0.41414   0.48677   0.60896   \n",
      "333897  nfe3d1415af28cc6  eraX      live   0.71139   0.35889   0.71324   \n",
      "333898  nfe4c8dc8acdccde  eraX      live   0.50915   0.57461   0.53146   \n",
      "333899  nfe73e58b58b7090  eraX      live   0.47553   0.52872   0.41721   \n",
      "333900  nfe802b9bc95f827  eraX      live   0.68140   0.44625   0.57745   \n",
      "333901  nfe89b7568dfa36d  eraX      live   0.42321   0.42074   0.52695   \n",
      "333902  nfe9282907757e58  eraX      live   0.55282   0.52573   0.47914   \n",
      "333903  nfeab30d83fcf30d  eraX      live   0.36989   0.74019   0.67701   \n",
      "333904  nfeb7a05db0bf1a8  eraX      live   0.55757   0.58408   0.63926   \n",
      "333905  nfecd83c1fca107a  eraX      live   0.35391   0.40353   0.54034   \n",
      "333906  nfee37fbd5643e8f  eraX      live   0.38514   0.50010   0.56808   \n",
      "333907  nff095f12eed21c5  eraX      live   0.48082   0.36199   0.49564   \n",
      "333908  nff10e532282523f  eraX      live   0.45475   0.56606   0.63381   \n",
      "333909  nff185012ad15e40  eraX      live   0.63625   0.44036   0.59375   \n",
      "333910  nff1e111416ea0cb  eraX      live   0.23209   0.60640   0.49953   \n",
      "333911  nff3fde3349c3fa7  eraX      live   0.36373   0.24481   0.34591   \n",
      "333912  nff53ea682a4d232  eraX      live   0.71255   0.48826   0.57378   \n",
      "333913  nff6387a5fefd74b  eraX      live   0.74419   0.48956   0.48069   \n",
      "333914  nff6f775dfcfee2b  eraX      live   0.44564   0.24384   0.47070   \n",
      "333915  nff70a32033531d3  eraX      live   0.51020   0.52153   0.46950   \n",
      "333916  nffb679ad56c5e01  eraX      live   0.50417   0.55222   0.51376   \n",
      "333917  nffbb4d4734584ea  eraX      live   0.50035   0.45322   0.30718   \n",
      "333918  nffcbbb870146fb1  eraX      live   0.72354   0.33096   0.48361   \n",
      "333919  nffd2fbb2ca971c9  eraX      live   0.50463   0.43650   0.61034   \n",
      "333920  nffd5c84195da62a  eraX      live   0.25365   0.50848   0.46706   \n",
      "333921  nffda5075214e3cc  eraX      live   0.18931   0.53099   0.63145   \n",
      "333922  nffe61094f1141b4  eraX      live   0.49618   0.37097   0.63796   \n",
      "333923  nffea7cccc729790  eraX      live   0.35559   0.46143   0.61118   \n",
      "333924  nfffba6cc2dd7e66  eraX      live   0.21329   0.57872   0.76775   \n",
      "\n",
      "        feature4  feature5  feature6  feature7  ...  feature48  feature49  \\\n",
      "0        0.49490   0.53217   0.48388   0.50220  ...    0.55239    0.64054   \n",
      "1        0.44006   0.47866   0.44055   0.59182  ...    0.46029    0.62941   \n",
      "2        0.40306   0.53436   0.64028   0.51420  ...    0.40596    0.54731   \n",
      "3        0.37045   0.58711   0.59900   0.62428  ...    0.53878    0.47776   \n",
      "4        0.48692   0.47348   0.34695   0.41506  ...    0.46431    0.49482   \n",
      "5        0.30085   0.41742   0.47290   0.56301  ...    0.55917    0.31260   \n",
      "6        0.48021   0.71347   0.36977   0.57899  ...    0.40538    0.52690   \n",
      "7        0.61999   0.51016   0.52714   0.74017  ...    0.77664    0.51623   \n",
      "8        0.63037   0.50734   0.45545   0.41927  ...    0.52887    0.44384   \n",
      "9        0.38838   0.51222   0.53249   0.70187  ...    0.58353    0.42566   \n",
      "10       0.44842   0.49727   0.66198   0.53513  ...    0.54271    0.43434   \n",
      "11       0.51365   0.45339   0.51101   0.39488  ...    0.33293    0.30265   \n",
      "12       0.48615   0.54590   0.59111   0.55893  ...    0.55932    0.46798   \n",
      "13       0.58569   0.46994   0.51753   0.41116  ...    0.55347    0.42172   \n",
      "14       0.40727   0.36868   0.51526   0.29604  ...    0.53480    0.48525   \n",
      "15       0.39198   0.54481   0.55880   0.48240  ...    0.45665    0.50711   \n",
      "16       0.37918   0.65944   0.61828   0.62431  ...    0.44610    0.57642   \n",
      "17       0.53740   0.32951   0.44737   0.39056  ...    0.64044    0.22801   \n",
      "18       0.51449   0.44705   0.53717   0.53424  ...    0.53355    0.49124   \n",
      "19       0.46013   0.63523   0.48265   0.50777  ...    0.48970    0.41920   \n",
      "20       0.49827   0.22947   0.46324   0.49544  ...    0.75846    0.31290   \n",
      "21       0.49332   0.68842   0.59665   0.57461  ...    0.54245    0.40880   \n",
      "22       0.69467   0.62700   0.50894   0.44084  ...    0.64244    0.17882   \n",
      "23       0.33676   0.43296   0.56874   0.67007  ...    0.63132    0.46647   \n",
      "24       0.39413   0.58728   0.38304   0.60579  ...    0.50472    0.33878   \n",
      "25       0.45756   0.39833   0.58444   0.48982  ...    0.58255    0.52865   \n",
      "26       0.48904   0.43819   0.53960   0.55395  ...    0.49973    0.47976   \n",
      "27       0.50982   0.63898   0.45895   0.50982  ...    0.52851    0.43489   \n",
      "28       0.47962   0.50144   0.58369   0.62196  ...    0.79120    0.72922   \n",
      "29       0.41567   0.57632   0.34524   0.58772  ...    0.47177    0.55393   \n",
      "...          ...       ...       ...       ...  ...        ...        ...   \n",
      "333895   0.55424   0.50785   0.48992   0.43461  ...    0.56556    0.18851   \n",
      "333896   0.24298   0.51481   0.64144   0.57336  ...    0.63589    0.50188   \n",
      "333897   0.62141   0.39953   0.50117   0.38673  ...    0.43218    0.29483   \n",
      "333898   0.48220   0.43471   0.43595   0.41659  ...    0.46864    0.46443   \n",
      "333899   0.41378   0.42050   0.52153   0.51155  ...    0.53254    0.50771   \n",
      "333900   0.50875   0.45746   0.66390   0.39603  ...    0.51026    0.34917   \n",
      "333901   0.41065   0.43218   0.45313   0.54766  ...    0.50390    0.47608   \n",
      "333902   0.30024   0.53908   0.46539   0.63140  ...    0.54333    0.44089   \n",
      "333903   0.38120   0.46426   0.50050   0.53517  ...    0.42189    0.42427   \n",
      "333904   0.22444   0.50235   0.62751   0.44797  ...    0.53568    0.37471   \n",
      "333905   0.46693   0.45546   0.31799   0.20849  ...    0.56820    0.40034   \n",
      "333906   0.38002   0.46173   0.55701   0.43784  ...    0.62778    0.29370   \n",
      "333907   0.56243   0.64222   0.45990   0.65490  ...    0.47359    0.57969   \n",
      "333908   0.32118   0.45341   0.46590   0.71699  ...    0.55491    0.41783   \n",
      "333909   0.54115   0.54770   0.45458   0.47165  ...    0.36397    0.28323   \n",
      "333910   0.38615   0.54330   0.31493   0.59189  ...    0.58707    0.64331   \n",
      "333911   0.52657   0.67164   0.33474   0.59808  ...    0.71367    0.57020   \n",
      "333912   0.42519   0.45787   0.59789   0.48950  ...    0.70612    0.54224   \n",
      "333913   0.35161   0.50736   0.65030   0.56786  ...    0.61832    0.39870   \n",
      "333914   0.79470   0.36341   0.33101   0.58867  ...    0.36063    0.53175   \n",
      "333915   0.17914   0.46065   0.58877   0.38333  ...    0.25012    0.47906   \n",
      "333916   0.35170   0.33689   0.49312   0.52409  ...    0.62969    0.47749   \n",
      "333917   0.42726   0.33704   0.64439   0.69713  ...    0.71348    0.56586   \n",
      "333918   0.35055   0.60968   0.61715   0.63803  ...    0.54201    0.62339   \n",
      "333919   0.52311   0.52897   0.54492   0.49094  ...    0.46759    0.48536   \n",
      "333920   0.41834   0.48237   0.47138   0.46319  ...    0.45665    0.44475   \n",
      "333921   0.50408   0.58139   0.33963   0.31451  ...    0.40300    0.46188   \n",
      "333922   0.52798   0.56292   0.49298   0.53647  ...    0.50421    0.47972   \n",
      "333923   0.39542   0.44457   0.40371   0.63040  ...    0.68662    0.45291   \n",
      "333924   0.46303   0.49603   0.37155   0.36905  ...    0.39985    0.47919   \n",
      "\n",
      "        feature50  target_bernie  target_elizabeth  target_jordan  target_ken  \\\n",
      "0         0.52182            1.0               1.0            1.0         1.0   \n",
      "1         0.55010            1.0               1.0            1.0         1.0   \n",
      "2         0.39061            0.0               0.0            1.0         0.0   \n",
      "3         0.36835            0.0               0.0            0.0         0.0   \n",
      "4         0.60452            1.0               1.0            1.0         1.0   \n",
      "5         0.35691            0.0               0.0            0.0         0.0   \n",
      "6         0.39767            1.0               1.0            1.0         1.0   \n",
      "7         0.49104            0.0               0.0            0.0         0.0   \n",
      "8         0.57269            0.0               0.0            0.0         0.0   \n",
      "9         0.50082            1.0               1.0            1.0         1.0   \n",
      "10        0.52720            0.0               0.0            0.0         0.0   \n",
      "11        0.45951            0.0               0.0            0.0         0.0   \n",
      "12        0.53359            1.0               1.0            1.0         1.0   \n",
      "13        0.72972            0.0               0.0            0.0         0.0   \n",
      "14        0.59524            1.0               1.0            1.0         1.0   \n",
      "15        0.59955            0.0               0.0            0.0         0.0   \n",
      "16        0.62569            0.0               0.0            0.0         0.0   \n",
      "17        0.44203            1.0               1.0            1.0         1.0   \n",
      "18        0.39731            1.0               1.0            1.0         1.0   \n",
      "19        0.38751            0.0               0.0            0.0         0.0   \n",
      "20        0.61816            1.0               1.0            1.0         1.0   \n",
      "21        0.53372            0.0               1.0            0.0         0.0   \n",
      "22        0.40123            0.0               0.0            0.0         0.0   \n",
      "23        0.53402            0.0               0.0            0.0         0.0   \n",
      "24        0.56262            0.0               0.0            0.0         0.0   \n",
      "25        0.63495            0.0               0.0            0.0         0.0   \n",
      "26        0.44513            0.0               0.0            0.0         0.0   \n",
      "27        0.48806            0.0               0.0            0.0         1.0   \n",
      "28        0.49451            0.0               0.0            0.0         0.0   \n",
      "29        0.49058            1.0               1.0            1.0         1.0   \n",
      "...           ...            ...               ...            ...         ...   \n",
      "333895    0.38337            NaN               NaN            NaN         NaN   \n",
      "333896    0.65093            NaN               NaN            NaN         NaN   \n",
      "333897    0.38152            NaN               NaN            NaN         NaN   \n",
      "333898    0.40054            NaN               NaN            NaN         NaN   \n",
      "333899    0.34019            NaN               NaN            NaN         NaN   \n",
      "333900    0.60540            NaN               NaN            NaN         NaN   \n",
      "333901    0.29951            NaN               NaN            NaN         NaN   \n",
      "333902    0.59300            NaN               NaN            NaN         NaN   \n",
      "333903    0.67124            NaN               NaN            NaN         NaN   \n",
      "333904    0.38034            NaN               NaN            NaN         NaN   \n",
      "333905    0.44061            NaN               NaN            NaN         NaN   \n",
      "333906    0.61356            NaN               NaN            NaN         NaN   \n",
      "333907    0.51950            NaN               NaN            NaN         NaN   \n",
      "333908    0.62633            NaN               NaN            NaN         NaN   \n",
      "333909    0.38023            NaN               NaN            NaN         NaN   \n",
      "333910    0.47371            NaN               NaN            NaN         NaN   \n",
      "333911    0.59949            NaN               NaN            NaN         NaN   \n",
      "333912    0.57141            NaN               NaN            NaN         NaN   \n",
      "333913    0.61005            NaN               NaN            NaN         NaN   \n",
      "333914    0.63213            NaN               NaN            NaN         NaN   \n",
      "333915    0.57808            NaN               NaN            NaN         NaN   \n",
      "333916    0.51791            NaN               NaN            NaN         NaN   \n",
      "333917    0.55149            NaN               NaN            NaN         NaN   \n",
      "333918    0.40328            NaN               NaN            NaN         NaN   \n",
      "333919    0.82365            NaN               NaN            NaN         NaN   \n",
      "333920    0.54657            NaN               NaN            NaN         NaN   \n",
      "333921    0.61674            NaN               NaN            NaN         NaN   \n",
      "333922    0.61761            NaN               NaN            NaN         NaN   \n",
      "333923    0.38017            NaN               NaN            NaN         NaN   \n",
      "333924    0.73788            NaN               NaN            NaN         NaN   \n",
      "\n",
      "        target_charles  target_frank  target_hillary  \n",
      "0                  1.0           1.0             1.0  \n",
      "1                  1.0           1.0             1.0  \n",
      "2                  0.0           0.0             0.0  \n",
      "3                  0.0           0.0             0.0  \n",
      "4                  1.0           1.0             1.0  \n",
      "5                  0.0           0.0             0.0  \n",
      "6                  1.0           1.0             1.0  \n",
      "7                  0.0           0.0             0.0  \n",
      "8                  0.0           0.0             0.0  \n",
      "9                  1.0           1.0             1.0  \n",
      "10                 0.0           0.0             0.0  \n",
      "11                 0.0           0.0             0.0  \n",
      "12                 1.0           1.0             1.0  \n",
      "13                 0.0           0.0             0.0  \n",
      "14                 1.0           1.0             1.0  \n",
      "15                 1.0           1.0             0.0  \n",
      "16                 0.0           0.0             0.0  \n",
      "17                 1.0           1.0             1.0  \n",
      "18                 1.0           1.0             1.0  \n",
      "19                 0.0           0.0             1.0  \n",
      "20                 1.0           1.0             1.0  \n",
      "21                 0.0           1.0             1.0  \n",
      "22                 0.0           0.0             0.0  \n",
      "23                 0.0           0.0             0.0  \n",
      "24                 0.0           0.0             0.0  \n",
      "25                 0.0           0.0             0.0  \n",
      "26                 0.0           0.0             0.0  \n",
      "27                 0.0           0.0             1.0  \n",
      "28                 0.0           0.0             0.0  \n",
      "29                 1.0           1.0             1.0  \n",
      "...                ...           ...             ...  \n",
      "333895             NaN           NaN             NaN  \n",
      "333896             NaN           NaN             NaN  \n",
      "333897             NaN           NaN             NaN  \n",
      "333898             NaN           NaN             NaN  \n",
      "333899             NaN           NaN             NaN  \n",
      "333900             NaN           NaN             NaN  \n",
      "333901             NaN           NaN             NaN  \n",
      "333902             NaN           NaN             NaN  \n",
      "333903             NaN           NaN             NaN  \n",
      "333904             NaN           NaN             NaN  \n",
      "333905             NaN           NaN             NaN  \n",
      "333906             NaN           NaN             NaN  \n",
      "333907             NaN           NaN             NaN  \n",
      "333908             NaN           NaN             NaN  \n",
      "333909             NaN           NaN             NaN  \n",
      "333910             NaN           NaN             NaN  \n",
      "333911             NaN           NaN             NaN  \n",
      "333912             NaN           NaN             NaN  \n",
      "333913             NaN           NaN             NaN  \n",
      "333914             NaN           NaN             NaN  \n",
      "333915             NaN           NaN             NaN  \n",
      "333916             NaN           NaN             NaN  \n",
      "333917             NaN           NaN             NaN  \n",
      "333918             NaN           NaN             NaN  \n",
      "333919             NaN           NaN             NaN  \n",
      "333920             NaN           NaN             NaN  \n",
      "333921             NaN           NaN             NaN  \n",
      "333922             NaN           NaN             NaN  \n",
      "333923             NaN           NaN             NaN  \n",
      "333924             NaN           NaN             NaN  \n",
      "\n",
      "[836657 rows x 60 columns]\n",
      "['feature1', 'feature2', 'feature3', 'feature4', 'feature5', 'feature6', 'feature7', 'feature8', 'feature9', 'feature10', 'feature11', 'feature12', 'feature13', 'feature14', 'feature15', 'feature16', 'feature17', 'feature18', 'feature19', 'feature20', 'feature21', 'feature22', 'feature23', 'feature24', 'feature25', 'feature26', 'feature27', 'feature28', 'feature29', 'feature30', 'feature31', 'feature32', 'feature33', 'feature34', 'feature35', 'feature36', 'feature37', 'feature38', 'feature39', 'feature40', 'feature41', 'feature42', 'feature43', 'feature44', 'feature45', 'feature46', 'feature47', 'feature48', 'feature49', 'feature50']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\peace\\Anaconda3_2\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\peace\\Anaconda3_2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 6)            306         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 6)            306         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 6)            42          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 6)            306         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 6)            42          dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 6)            0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 6)            42          dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 6)            0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 18)           0           dropout_1[0][0]                  \n",
      "                                                                 dense_4[0][0]                    \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 2)            38          concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,082\n",
      "Trainable params: 1,082\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peace\\Anaconda3_2\\lib\\site-packages\\dask\\config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\peace\\Anaconda3_2\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 301639 samples, validate on 201093 samples\n",
      "Epoch 1/30\n",
      "301639/301639 [==============================] - 1s 5us/step - loss: 0.7082 - val_loss: 0.6931\n",
      "Epoch 2/30\n",
      "301639/301639 [==============================] - 1s 4us/step - loss: 0.6934 - val_loss: 0.6930\n",
      "Epoch 3/30\n",
      "301639/301639 [==============================] - 1s 4us/step - loss: 0.6931 - val_loss: 0.6931\n",
      "Epoch 4/30\n",
      "301639/301639 [==============================] - 1s 4us/step - loss: 0.6930 - val_loss: 0.6929\n",
      "Epoch 5/30\n",
      "301639/301639 [==============================] - 1s 4us/step - loss: 0.6930 - val_loss: 0.6929\n",
      "Epoch 6/30\n",
      "301639/301639 [==============================] - 1s 4us/step - loss: 0.6929 - val_loss: 0.6928\n",
      "Epoch 7/30\n",
      "301639/301639 [==============================] - 1s 4us/step - loss: 0.6929 - val_loss: 0.6928\n",
      "Epoch 8/30\n",
      "301639/301639 [==============================] - 1s 4us/step - loss: 0.6928 - val_loss: 0.6928\n",
      "Epoch 9/30\n",
      "301639/301639 [==============================] - 1s 4us/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 10/30\n",
      "301639/301639 [==============================] - 1s 4us/step - loss: 0.6927 - val_loss: 0.6928\n",
      "Epoch 11/30\n",
      "301639/301639 [==============================] - 1s 4us/step - loss: 0.6927 - val_loss: 0.6927\n",
      "Epoch 12/30\n",
      "301639/301639 [==============================] - 1s 4us/step - loss: 0.6927 - val_loss: 0.6927\n",
      "Epoch 13/30\n",
      "301639/301639 [==============================] - 1s 4us/step - loss: 0.6927 - val_loss: 0.6927\n",
      "Epoch 14/30\n",
      "301639/301639 [==============================] - 1s 4us/step - loss: 0.6926 - val_loss: 0.6927\n",
      "Epoch 15/30\n",
      "301639/301639 [==============================] - 1s 4us/step - loss: 0.6926 - val_loss: 0.6927\n",
      "Epoch 16/30\n",
      "301639/301639 [==============================] - 1s 4us/step - loss: 0.6926 - val_loss: 0.6930\n",
      "Epoch 17/30\n",
      "301639/301639 [==============================] - 1s 4us/step - loss: 0.6926 - val_loss: 0.6927\n",
      "Epoch 18/30\n",
      "301639/301639 [==============================] - 1s 4us/step - loss: 0.6926 - val_loss: 0.6928\n",
      "Epoch 19/30\n",
      "301639/301639 [==============================] - 1s 4us/step - loss: 0.6926 - val_loss: 0.6927\n",
      "Epoch 20/30\n",
      "301639/301639 [==============================] - 1s 4us/step - loss: 0.6926 - val_loss: 0.6927\n",
      "Epoch 21/30\n",
      "301639/301639 [==============================] - 1s 4us/step - loss: 0.6925 - val_loss: 0.6927\n",
      "Epoch 22/30\n",
      "301639/301639 [==============================] - 1s 4us/step - loss: 0.6925 - val_loss: 0.6928\n",
      "Epoch 23/30\n",
      "301639/301639 [==============================] - 1s 4us/step - loss: 0.6925 - val_loss: 0.6927\n",
      "Epoch 24/30\n",
      "301639/301639 [==============================] - 1s 4us/step - loss: 0.6925 - val_loss: 0.6927\n",
      "Epoch 25/30\n",
      "301639/301639 [==============================] - 1s 4us/step - loss: 0.6925 - val_loss: 0.6928\n",
      "Epoch 26/30\n",
      "301639/301639 [==============================] - 1s 4us/step - loss: 0.6925 - val_loss: 0.6928\n",
      "Epoch 27/30\n",
      "301639/301639 [==============================] - 1s 4us/step - loss: 0.6925 - val_loss: 0.6928\n",
      "Epoch 28/30\n",
      "301639/301639 [==============================] - 1s 4us/step - loss: 0.6925 - val_loss: 0.6930\n",
      "Epoch 29/30\n",
      "301639/301639 [==============================] - 1s 4us/step - loss: 0.6925 - val_loss: 0.6932\n",
      "Epoch 30/30\n",
      "301639/301639 [==============================] - 1s 4us/step - loss: 0.6925 - val_loss: 0.6928\n",
      "333925/333925 [==============================] - 3s 9us/step\n",
      "era121: loss - 0.6924828147092087 consistent: True - better than random 1\n",
      "era122: loss - 0.6924828147092087 consistent: True - better than random 2\n",
      "era123: loss - 0.6924828147092087 consistent: True - better than random 3\n",
      "era124: loss - 0.6924828147092087 consistent: True - better than random 4\n",
      "era125: loss - 0.6924828147092087 consistent: True - better than random 5\n",
      "era126: loss - 0.6924828147092087 consistent: True - better than random 6\n",
      "era127: loss - 0.6924828147092087 consistent: True - better than random 7\n",
      "era128: loss - 0.6924828147092087 consistent: True - better than random 8\n",
      "era129: loss - 0.6924828147092087 consistent: True - better than random 9\n",
      "era130: loss - 0.6924828147092087 consistent: True - better than random 10\n",
      "era131: loss - 0.6924828147092087 consistent: True - better than random 11\n",
      "era132: loss - 0.6924828147092087 consistent: True - better than random 12\n",
      "era133: loss - 0.6924828147092087 consistent: True - better than random 13\n",
      "era134: loss - 0.6924828147092087 consistent: True - better than random 14\n",
      "era135: loss - 0.6924828147092087 consistent: True - better than random 15\n",
      "era136: loss - 0.6924828147092087 consistent: True - better than random 16\n",
      "era137: loss - 0.6924828147092087 consistent: True - better than random 17\n",
      "era138: loss - 0.6924828147092087 consistent: True - better than random 18\n",
      "era139: loss - 0.6924828147092087 consistent: True - better than random 19\n",
      "era140: loss - 0.6924828147092087 consistent: True - better than random 20\n",
      "era141: loss - 0.6924828147092087 consistent: True - better than random 21\n",
      "era142: loss - 0.6924828147092087 consistent: True - better than random 22\n",
      "era143: loss - 0.6924828147092087 consistent: True - better than random 23\n",
      "era144: loss - 0.6924828147092087 consistent: True - better than random 24\n",
      "era145: loss - 0.6924828147092087 consistent: True - better than random 25\n",
      "era146: loss - 0.6924828147092087 consistent: True - better than random 26\n",
      "era147: loss - 0.6924828147092087 consistent: True - better than random 27\n",
      "era148: loss - 0.6924828147092087 consistent: True - better than random 28\n",
      "era149: loss - 0.6924828147092087 consistent: True - better than random 29\n",
      "era150: loss - 0.6924828147092087 consistent: True - better than random 30\n",
      "era151: loss - 0.6924828147092087 consistent: True - better than random 31\n",
      "era152: loss - 0.6924828147092087 consistent: True - better than random 32\n",
      "era153: loss - 0.6924828147092087 consistent: True - better than random 33\n",
      "era154: loss - 0.6924828147092087 consistent: True - better than random 34\n",
      "era155: loss - 0.6924828147092087 consistent: True - better than random 35\n",
      "era156: loss - 0.6924828147092087 consistent: True - better than random 36\n",
      "era157: loss - 0.6924828147092087 consistent: True - better than random 37\n",
      "era158: loss - 0.6924828147092087 consistent: True - better than random 38\n",
      "era159: loss - 0.6924828147092087 consistent: True - better than random 39\n",
      "era160: loss - 0.6924828147092087 consistent: True - better than random 40\n",
      "era161: loss - 0.6924828147092087 consistent: True - better than random 41\n",
      "era162: loss - 0.6924828147092087 consistent: True - better than random 42\n",
      "era163: loss - 0.6924828147092087 consistent: True - better than random 43\n",
      "era164: loss - 0.6924828147092087 consistent: True - better than random 44\n",
      "era165: loss - 0.6924828147092087 consistent: True - better than random 45\n",
      "era166: loss - 0.6924828147092087 consistent: True - better than random 46\n",
      "era167: loss - 0.6924828147092087 consistent: True - better than random 47\n",
      "era168: loss - 0.6924828147092087 consistent: True - better than random 48\n",
      "era169: loss - 0.6924828147092087 consistent: True - better than random 49\n",
      "era170: loss - 0.6924828147092087 consistent: True - better than random 50\n",
      "era171: loss - 0.6924828147092087 consistent: True - better than random 51\n",
      "era172: loss - 0.6924828147092087 consistent: True - better than random 52\n",
      "era173: loss - 0.6924828147092087 consistent: True - better than random 53\n",
      "era174: loss - 0.6924828147092087 consistent: True - better than random 54\n",
      "era175: loss - 0.6924828147092087 consistent: True - better than random 55\n",
      "era176: loss - 0.6924828147092087 consistent: True - better than random 56\n",
      "era177: loss - 0.6924828147092087 consistent: True - better than random 57\n",
      "era178: loss - 0.6924828147092087 consistent: True - better than random 58\n",
      "era179: loss - 0.6924828147092087 consistent: True - better than random 59\n",
      "era180: loss - 0.6924828147092087 consistent: True - better than random 60\n",
      "era181: loss - 0.6924828147092087 consistent: True - better than random 61\n",
      "era182: loss - 0.6924828147092087 consistent: True - better than random 62\n",
      "era183: loss - 0.6924828147092087 consistent: True - better than random 63\n",
      "era184: loss - 0.6924828147092087 consistent: True - better than random 64\n",
      "era185: loss - 0.6924828147092087 consistent: True - better than random 65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "era186: loss - 0.6924828147092087 consistent: True - better than random 66\n",
      "eraX: loss - 0.6924828147092087 consistent: True - better than random 67\n",
      "Consistency: 1.0\n",
      "- probabilities: [0.4934881  0.47265428 0.48932266 0.48251784 0.48494077]\n",
      "- target: 1    1.0\n",
      "2    0.0\n",
      "3    0.0\n",
      "4    0.0\n",
      "5    1.0\n",
      "Name: target_charles, dtype: float64\n",
      "- rounded probability: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "- accuracy:  0.08622295425619525\n",
      "- charles vs ken corr: [[nan nan]\n",
      " [nan nan]]\n",
      "- ken using charles: 0.0863367522647301\n",
      "333925/333925 [==============================] - 3s 9us/step\n",
      "- validation logloss: 0.6924828147092087\n",
      "Writing predictions to predictions.csv\n",
      "\n",
      "Writing predictions to predictions_2019-04-12_17h13m20s_Shared_input_layer_1.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND' ] = 'tensorflow'\n",
    "os.environ['MKL_THREADING_LAYER'] = 'GNU'\n",
    "import keras as ks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import TensorBoard\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense,Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import Dropout, Activation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.layers.recurrent import LSTM\n",
    "import time \n",
    "from keras.layers import Input, Dense, LSTM, MaxPooling1D, Conv1D\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "def main():\n",
    "\n",
    "    # Set seed for reproducibility\n",
    "    NAME = \"Shared_input_layer\" #this is the name for the Tensor Graph\n",
    "    np.random.seed(0)\n",
    "    print(\"Loading data...\")\n",
    "    # Load the data from the CSV files\n",
    "    training_data = pd.read_csv('numerai_training_data.csv', header=0)\n",
    "    \n",
    "    print('original train data shape: {},\\t{} \\n\\n \\t:'.format(training_data.shape[0],\n",
    "                                                               training_data.shape[1]))  #Training Data\n",
    "\n",
    "    prediction_data = pd.read_csv('numerai_tournament_data.csv', header=0)\n",
    "    print('original prediction data shape: {},\\t{} \\n\\n \\t:'.format(prediction_data.shape[0],\n",
    "                                                                    prediction_data.shape[1])) # Test Data\n",
    "    \n",
    "    complete_training_data = pd.concat([training_data, prediction_data])\n",
    "    print('total training / valdation shape {}'.format(complete_training_data)) # Concatenated Training/Test Data\n",
    "    \n",
    "    # Transform the loaded CSV data into numpy arrays\n",
    "    features = [f for f in list(training_data) if \"feature\" in f] #Features for training\n",
    "    print(features)\n",
    "    #Determine Labels vs Features\n",
    "    \n",
    "    #Scale All features \n",
    "    X = training_data[features] \n",
    "    mini= MinMaxScaler(feature_range=(0,1)) \n",
    "    X = mini.fit_transform(X)\n",
    "    \n",
    "    #Define Categorical Variables\n",
    "    Y = training_data[\"target_bernie\"]\n",
    "    Y= keras.utils.to_categorical(Y,2) \n",
    "    \n",
    "    #Define Prediction Labels\n",
    "    x_prediction = prediction_data[features]\n",
    "    x_prediction = mini.fit_transform(x_prediction)\n",
    "    \n",
    "    #Id's for prediction Labels \n",
    "    ids = prediction_data[\"id\"]  \n",
    "    #Define Model\n",
    "    batch_size = 710\n",
    "    dropout = 0.666666\n",
    "     \n",
    "    visible = Input(shape=(50,))\n",
    "    m1 = Dense(6, activation='sigmoid')(visible)\n",
    "    m1 = Dense(6, activation='sigmoid')(m1)\n",
    "    m1 = Dropout(dropout)(m1)\n",
    "    \n",
    "    m2 = Dense(6, activation='sigmoid')(visible)\n",
    "    m2 = Dense(6, activation='sigmoid')(m2)\n",
    "    \n",
    "    m3 = Dense(6, activation='sigmoid')(visible)\n",
    "    m3 = Dense(6, activation='sigmoid')(m3)\n",
    "    m3 = Dropout(dropout)(m3)\n",
    "    \n",
    "    merge = concatenate([m1,m2,m3],axis=1)\n",
    "    \n",
    "    output = Dense(2, activation='sigmoid')(merge)\n",
    "    model = Model(inputs=visible, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy',optimizer='rmsprop')\n",
    "    model.summary()\n",
    "    \n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "    model.fit(X,Y,batch_size=batch_size,epochs=30,\n",
    "              validation_split=0.4,\n",
    "              callbacks=[tensorboard])\n",
    "    \n",
    "    y_prediction = model.predict(x_prediction)\n",
    "\n",
    "    evaluate = model.evaluate(x_prediction,y_prediction)\n",
    "    \n",
    "    eras = prediction_data.era.unique()\n",
    "    count = 0\n",
    "    count_consistent = 0\n",
    "    better_than_random_era_count = 0\n",
    "    for era in eras:\n",
    "        count += 1\n",
    "        current_valid_data = prediction_data[prediction_data.era==era]\n",
    "        features = [f for f in list(complete_training_data) if \"feature\" in f]\n",
    "        X_valid = current_valid_data[features]\n",
    "        Y_valid = current_valid_data[\"target_bernie\"]\n",
    "        loss = evaluate\n",
    "        if (loss < -np.log(.5)):\n",
    "            consistent = True\n",
    "            count_consistent += 1\n",
    "            better_than_random_era_count += 1\n",
    "        else:\n",
    "            consistent = False\n",
    "        print(\"{}: loss - {} consistent: {} - better than random {}\".format(era, loss, consistent,better_than_random_era_count))\n",
    "    print (\"Consistency: {}\".format(count_consistent/count))\n",
    "        \n",
    "    \n",
    "    probabilities = y_prediction[:, 1]\n",
    "    print(\"- probabilities:\", probabilities[1:6])\n",
    "\n",
    "    # We can see the probability does seem to be good at predicting the\n",
    "    # true target correctly.\n",
    "    print(\"- target:\", prediction_data['target_bernie'][1:6])\n",
    "    print(\"- rounded probability:\", [np.round(p) for p in probabilities][1:6])\n",
    "\n",
    "    # But overall the accuracy is very low.\n",
    "    correct = [\n",
    "        np.round(x) == y\n",
    "        for (x, y) in zip(probabilities, prediction_data['target_bernie'])\n",
    "    ]\n",
    "    print(\"- accuracy: \", np.sum(correct) / np.float(prediction_data.shape[0]))\n",
    "\n",
    "    tournament_corr = np.corrcoef(prediction_data['target_bernie'],\n",
    "                                  prediction_data['target_ken'])\n",
    "    print(\"- bernie vs ken corr:\", tournament_corr)\n",
    "    # You can see that target_ken is accurate using the bernie model as well.\n",
    "    correct = [\n",
    "        np.round(x) == y\n",
    "        for (x, y) in zip(probabilities, prediction_data['target_ken'])\n",
    "    ]\n",
    "    print(\"- ken using bernie:\",\n",
    "          np.sum(correct) / np.float(prediction_data.shape[0]))\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "    print(\"- validation logloss:\",\n",
    "          model.evaluate(x_prediction,y_prediction))\n",
    "    \n",
    "    results = y_prediction[:, 1]\n",
    "    results_df = pd.DataFrame(data={'probability_bernie':results})\n",
    "\n",
    "    joined = pd.DataFrame(ids).join(results_df)\n",
    "    pd.DataFrame(joined[:5])\n",
    "\n",
    "\n",
    "    print(\"Writing predictions to predictions.csv\")\n",
    "    path = 'predictions_{:}_{}_1'.format(time.strftime(\"%Y-%m-%d_%Hh%Mm%Ss\", time.gmtime()),NAME) + '.csv'\n",
    "    print()\n",
    "    print(\"Writing predictions to \" + path.strip())\n",
    "    joined.to_csv(path,float_format='%.15f', index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
