{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T16:02:13.539221Z",
     "start_time": "2019-04-05T16:01:32.364671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "original train data shape: 502732,\t60 \n",
      "\n",
      " \t:\n",
      "original prediction data shape: 333917,\t60 \n",
      "\n",
      " \t:\n",
      "total training / valdation shape                       id   era data_type  feature1  feature2  feature3  \\\n",
      "0       n0003126ff2349f6  era1     train   0.54836   0.31077   0.37524   \n",
      "1       n003d773d29b57ec  era1     train   0.34712   0.40275   0.42747   \n",
      "2       n0074df2dc6810b6  era1     train   0.50871   0.48639   0.47544   \n",
      "3       n0090630f530903e  era1     train   0.61363   0.40268   0.53779   \n",
      "4       n00af19089546fe9  era1     train   0.30704   0.47273   0.54495   \n",
      "5       n011d2da12b1e735  era1     train   0.52336   0.59136   0.60506   \n",
      "6       n014149cadeee55d  era1     train   0.30875   0.62510   0.35229   \n",
      "7       n0148a4dcf539aba  era1     train   0.40632   0.30590   0.43227   \n",
      "8       n015855690d31908  era1     train   0.48193   0.27060   0.50228   \n",
      "9       n0169447f4d6a10e  era1     train   0.51191   0.53663   0.42109   \n",
      "10      n01703ba4eff8fe7  era1     train   0.51829   0.52928   0.41085   \n",
      "11      n01b43e631083764  era1     train   0.61895   0.44075   0.61466   \n",
      "12      n01d1368b061433f  era1     train   0.45529   0.28276   0.61272   \n",
      "13      n01d5bfde31734cd  era1     train   0.48717   0.28982   0.38631   \n",
      "14      n0236fa89e606631  era1     train   0.42585   0.56616   0.62732   \n",
      "15      n023dfa017b93739  era1     train   0.51657   0.52352   0.41695   \n",
      "16      n0294f04019898e8  era1     train   0.45073   0.56795   0.25294   \n",
      "17      n029adf0fa156932  era1     train   0.53833   0.43619   0.58892   \n",
      "18      n02ad6457589c4af  era1     train   0.33576   0.54933   0.39919   \n",
      "19      n02bd240c01b24bd  era1     train   0.47469   0.38517   0.53743   \n",
      "20      n02c48c74a9c9719  era1     train   0.83431   0.36147   0.69324   \n",
      "21      n02cc6384ea5a025  era1     train   0.49314   0.31000   0.36538   \n",
      "22      n02e0356c793b120  era1     train   0.65183   0.40688   0.52271   \n",
      "23      n02e96eeb774238f  era1     train   0.67869   0.46261   0.38758   \n",
      "24      n030964ace7cfa93  era1     train   0.59925   0.43286   0.46698   \n",
      "25      n0311b410c7f8b9d  era1     train   0.37634   0.43578   0.43619   \n",
      "26      n03158868e546c31  era1     train   0.50558   0.47651   0.53912   \n",
      "27      n032f0f9107b0115  era1     train   0.46657   0.28710   0.44637   \n",
      "28      n03484f670162e63  era1     train   0.33571   0.45678   0.25402   \n",
      "29      n03534209c17edf5  era1     train   0.35224   0.48464   0.47109   \n",
      "...                  ...   ...       ...       ...       ...       ...   \n",
      "333887  nfe90ce695609608  eraX      live   0.22473   0.31813   0.36786   \n",
      "333888  nfe932f6fe046898  eraX      live   0.57944   0.41360   0.34797   \n",
      "333889  nfe93cb92e1f3004  eraX      live   0.62594   0.53724   0.51025   \n",
      "333890  nfe943152f1a5550  eraX      live   0.18081   0.50046   0.44782   \n",
      "333891  nfe9f241941633d3  eraX      live   0.44638   0.41328   0.59583   \n",
      "333892  nfed818ba71b4665  eraX      live   0.45843   0.49678   0.51053   \n",
      "333893  nfeda644836d563c  eraX      live   0.42282   0.57715   0.32823   \n",
      "333894  nfee50cc929893b4  eraX      live   0.22692   0.41532   0.47914   \n",
      "333895  nfeeb39596d24005  eraX      live   0.67745   0.47704   0.69068   \n",
      "333896  nfeee0c1087730ac  eraX      live   0.30302   0.41184   0.37730   \n",
      "333897  nfef761c30da657a  eraX      live   0.77022   0.39439   0.61509   \n",
      "333898  nff0947a55e1d394  eraX      live   0.61578   0.35275   0.51831   \n",
      "333899  nff1774c41a64a26  eraX      live   0.47422   0.46354   0.53371   \n",
      "333900  nff1ea0914cf90f4  eraX      live   0.30684   0.49977   0.64083   \n",
      "333901  nff3fa4347689e62  eraX      live   0.31594   0.43230   0.58978   \n",
      "333902  nff548af92ef8dc6  eraX      live   0.59936   0.51865   0.36845   \n",
      "333903  nff646fb2b7d329e  eraX      live   0.47254   0.33300   0.49866   \n",
      "333904  nff70242e4559ef9  eraX      live   0.35272   0.34020   0.42336   \n",
      "333905  nff77343157e0ab0  eraX      live   0.50806   0.52088   0.40247   \n",
      "333906  nff86f7979a4483f  eraX      live   0.43828   0.50917   0.31763   \n",
      "333907  nff9843ac1881c92  eraX      live   0.33373   0.45027   0.51017   \n",
      "333908  nffa24ee2fa8d8ae  eraX      live   0.58084   0.60522   0.52742   \n",
      "333909  nffb20c86d6252bf  eraX      live   0.38466   0.36028   0.38189   \n",
      "333910  nffb86f0b9948cab  eraX      live   0.20704   0.65434   0.75437   \n",
      "333911  nffc091bd2914bc3  eraX      live   0.17362   0.44136   0.45643   \n",
      "333912  nffc4e712f2a7c7e  eraX      live   0.34347   0.32474   0.39716   \n",
      "333913  nffd1353a5e2baad  eraX      live   0.55615   0.49798   0.46160   \n",
      "333914  nffd9965e9b4fc2f  eraX      live   0.54227   0.40336   0.30860   \n",
      "333915  nffe587f7f3af1e7  eraX      live   0.26457   0.38589   0.48547   \n",
      "333916  nfffc562888c5ef0  eraX      live   0.59965   0.50705   0.46930   \n",
      "\n",
      "        feature4  feature5  feature6  feature7       ...        feature48  \\\n",
      "0        0.49490   0.53217   0.48388   0.50220       ...          0.55239   \n",
      "1        0.44006   0.47866   0.44055   0.59182       ...          0.46029   \n",
      "2        0.40306   0.53436   0.64028   0.51420       ...          0.40596   \n",
      "3        0.37045   0.58711   0.59900   0.62428       ...          0.53878   \n",
      "4        0.48692   0.47348   0.34695   0.41506       ...          0.46431   \n",
      "5        0.30085   0.41742   0.47290   0.56301       ...          0.55917   \n",
      "6        0.48021   0.71347   0.36977   0.57899       ...          0.40538   \n",
      "7        0.61999   0.51016   0.52714   0.74017       ...          0.77664   \n",
      "8        0.63037   0.50734   0.45545   0.41927       ...          0.52887   \n",
      "9        0.38838   0.51222   0.53249   0.70187       ...          0.58353   \n",
      "10       0.44842   0.49727   0.66198   0.53513       ...          0.54271   \n",
      "11       0.51365   0.45339   0.51101   0.39488       ...          0.33293   \n",
      "12       0.48615   0.54590   0.59111   0.55893       ...          0.55932   \n",
      "13       0.58569   0.46994   0.51753   0.41116       ...          0.55347   \n",
      "14       0.40727   0.36868   0.51526   0.29604       ...          0.53480   \n",
      "15       0.39198   0.54481   0.55880   0.48240       ...          0.45665   \n",
      "16       0.37918   0.65944   0.61828   0.62431       ...          0.44610   \n",
      "17       0.53740   0.32951   0.44737   0.39056       ...          0.64044   \n",
      "18       0.51449   0.44705   0.53717   0.53424       ...          0.53355   \n",
      "19       0.46013   0.63523   0.48265   0.50777       ...          0.48970   \n",
      "20       0.49827   0.22947   0.46324   0.49544       ...          0.75846   \n",
      "21       0.49332   0.68842   0.59665   0.57461       ...          0.54245   \n",
      "22       0.69467   0.62700   0.50894   0.44084       ...          0.64244   \n",
      "23       0.33676   0.43296   0.56874   0.67007       ...          0.63132   \n",
      "24       0.39413   0.58728   0.38304   0.60579       ...          0.50472   \n",
      "25       0.45756   0.39833   0.58444   0.48982       ...          0.58255   \n",
      "26       0.48904   0.43819   0.53960   0.55395       ...          0.49973   \n",
      "27       0.50982   0.63898   0.45895   0.50982       ...          0.52851   \n",
      "28       0.47962   0.50144   0.58369   0.62196       ...          0.79120   \n",
      "29       0.41567   0.57632   0.34524   0.58772       ...          0.47177   \n",
      "...          ...       ...       ...       ...       ...              ...   \n",
      "333887   0.63595   0.63072   0.40458   0.60937       ...          0.43822   \n",
      "333888   0.30309   0.59500   0.62593   0.79868       ...          0.51592   \n",
      "333889   0.35304   0.42196   0.54384   0.40673       ...          0.81155   \n",
      "333890   0.19874   0.31669   0.56764   0.27662       ...          0.41892   \n",
      "333891   0.51726   0.32879   0.35747   0.27011       ...          0.40805   \n",
      "333892   0.42903   0.34398   0.17531   0.45940       ...          0.57967   \n",
      "333893   0.19175   0.54352   0.48392   0.58288       ...          0.61240   \n",
      "333894   0.47158   0.43951   0.51312   0.45735       ...          0.49855   \n",
      "333895   0.49231   0.42467   0.50623   0.21764       ...          0.51575   \n",
      "333896   0.42511   0.37806   0.48737   0.46332       ...          0.54794   \n",
      "333897   0.53305   0.54400   0.58841   0.36758       ...          0.53317   \n",
      "333898   0.48969   0.54165   0.67066   0.40535       ...          0.50956   \n",
      "333899   0.56766   0.53374   0.46601   0.33106       ...          0.36488   \n",
      "333900   0.60022   0.45940   0.27945   0.52223       ...          0.42632   \n",
      "333901   0.40462   0.49036   0.55442   0.55895       ...          0.57192   \n",
      "333902   0.37223   0.70834   0.68634   0.65859       ...          0.65068   \n",
      "333903   0.55127   0.47440   0.32557   0.62731       ...          0.32907   \n",
      "333904   0.60970   0.52893   0.42176   0.53982       ...          0.45713   \n",
      "333905   0.41444   0.50771   0.75231   0.51795       ...          0.65090   \n",
      "333906   0.45235   0.68345   0.47711   0.59222       ...          0.44174   \n",
      "333907   0.34386   0.53338   0.49982   0.48504       ...          0.35708   \n",
      "333908   0.24781   0.53824   0.71670   0.53123       ...          0.64067   \n",
      "333909   0.50206   0.44006   0.57703   0.51574       ...          0.46857   \n",
      "333910   0.43345   0.43357   0.26700   0.38337       ...          0.35225   \n",
      "333911   0.63990   0.56944   0.37104   0.36184       ...          0.42273   \n",
      "333912   0.67911   0.49659   0.37694   0.47598       ...          0.28455   \n",
      "333913   0.42583   0.53056   0.51423   0.72328       ...          0.51966   \n",
      "333914   0.41165   0.51681   0.63769   0.67705       ...          0.75881   \n",
      "333915   0.58102   0.57494   0.34050   0.49688       ...          0.40776   \n",
      "333916   0.38863   0.56402   0.47352   0.66432       ...          0.56368   \n",
      "\n",
      "        feature49  feature50  target_bernie  target_elizabeth  target_jordan  \\\n",
      "0         0.64054    0.52182            1.0               1.0            1.0   \n",
      "1         0.62941    0.55010            1.0               1.0            1.0   \n",
      "2         0.54731    0.39061            0.0               0.0            1.0   \n",
      "3         0.47776    0.36835            0.0               0.0            0.0   \n",
      "4         0.49482    0.60452            1.0               1.0            1.0   \n",
      "5         0.31260    0.35691            0.0               0.0            0.0   \n",
      "6         0.52690    0.39767            1.0               1.0            1.0   \n",
      "7         0.51623    0.49104            0.0               0.0            0.0   \n",
      "8         0.44384    0.57269            0.0               0.0            0.0   \n",
      "9         0.42566    0.50082            1.0               1.0            1.0   \n",
      "10        0.43434    0.52720            0.0               0.0            0.0   \n",
      "11        0.30265    0.45951            0.0               0.0            0.0   \n",
      "12        0.46798    0.53359            1.0               1.0            1.0   \n",
      "13        0.42172    0.72972            0.0               0.0            0.0   \n",
      "14        0.48525    0.59524            1.0               1.0            1.0   \n",
      "15        0.50711    0.59955            0.0               0.0            0.0   \n",
      "16        0.57642    0.62569            0.0               0.0            0.0   \n",
      "17        0.22801    0.44203            1.0               1.0            1.0   \n",
      "18        0.49124    0.39731            1.0               1.0            1.0   \n",
      "19        0.41920    0.38751            0.0               0.0            0.0   \n",
      "20        0.31290    0.61816            1.0               1.0            1.0   \n",
      "21        0.40880    0.53372            0.0               1.0            0.0   \n",
      "22        0.17882    0.40123            0.0               0.0            0.0   \n",
      "23        0.46647    0.53402            0.0               0.0            0.0   \n",
      "24        0.33878    0.56262            0.0               0.0            0.0   \n",
      "25        0.52865    0.63495            0.0               0.0            0.0   \n",
      "26        0.47976    0.44513            0.0               0.0            0.0   \n",
      "27        0.43489    0.48806            0.0               0.0            0.0   \n",
      "28        0.72922    0.49451            0.0               0.0            0.0   \n",
      "29        0.55393    0.49058            1.0               1.0            1.0   \n",
      "...           ...        ...            ...               ...            ...   \n",
      "333887    0.56037    0.68570            NaN               NaN            NaN   \n",
      "333888    0.50056    0.47213            NaN               NaN            NaN   \n",
      "333889    0.66506    0.41550            NaN               NaN            NaN   \n",
      "333890    0.56516    0.38036            NaN               NaN            NaN   \n",
      "333891    0.43037    0.49400            NaN               NaN            NaN   \n",
      "333892    0.60226    0.56339            NaN               NaN            NaN   \n",
      "333893    0.53195    0.47703            NaN               NaN            NaN   \n",
      "333894    0.65543    0.56419            NaN               NaN            NaN   \n",
      "333895    0.36139    0.43150            NaN               NaN            NaN   \n",
      "333896    0.58540    0.56483            NaN               NaN            NaN   \n",
      "333897    0.34749    0.43542            NaN               NaN            NaN   \n",
      "333898    0.53561    0.56108            NaN               NaN            NaN   \n",
      "333899    0.52181    0.30183            NaN               NaN            NaN   \n",
      "333900    0.44151    0.68143            NaN               NaN            NaN   \n",
      "333901    0.66334    0.52303            NaN               NaN            NaN   \n",
      "333902    0.48645    0.37984            NaN               NaN            NaN   \n",
      "333903    0.48351    0.58909            NaN               NaN            NaN   \n",
      "333904    0.46877    0.64382            NaN               NaN            NaN   \n",
      "333905    0.60046    0.48012            NaN               NaN            NaN   \n",
      "333906    0.38848    0.68848            NaN               NaN            NaN   \n",
      "333907    0.54039    0.58356            NaN               NaN            NaN   \n",
      "333908    0.49853    0.49006            NaN               NaN            NaN   \n",
      "333909    0.55217    0.52806            NaN               NaN            NaN   \n",
      "333910    0.43661    0.55406            NaN               NaN            NaN   \n",
      "333911    0.67505    0.47899            NaN               NaN            NaN   \n",
      "333912    0.42267    0.60347            NaN               NaN            NaN   \n",
      "333913    0.43774    0.54886            NaN               NaN            NaN   \n",
      "333914    0.48803    0.56248            NaN               NaN            NaN   \n",
      "333915    0.38404    0.64771            NaN               NaN            NaN   \n",
      "333916    0.33147    0.42495            NaN               NaN            NaN   \n",
      "\n",
      "        target_ken  target_charles  target_frank  target_hillary  \n",
      "0              1.0             1.0           1.0             1.0  \n",
      "1              1.0             1.0           1.0             1.0  \n",
      "2              0.0             0.0           0.0             0.0  \n",
      "3              0.0             0.0           0.0             0.0  \n",
      "4              1.0             1.0           1.0             1.0  \n",
      "5              0.0             0.0           0.0             0.0  \n",
      "6              1.0             1.0           1.0             1.0  \n",
      "7              0.0             0.0           0.0             0.0  \n",
      "8              0.0             0.0           0.0             0.0  \n",
      "9              1.0             1.0           1.0             1.0  \n",
      "10             0.0             0.0           0.0             0.0  \n",
      "11             0.0             0.0           0.0             0.0  \n",
      "12             1.0             1.0           1.0             1.0  \n",
      "13             0.0             0.0           0.0             0.0  \n",
      "14             1.0             1.0           1.0             1.0  \n",
      "15             0.0             1.0           1.0             0.0  \n",
      "16             0.0             0.0           0.0             0.0  \n",
      "17             1.0             1.0           1.0             1.0  \n",
      "18             1.0             1.0           1.0             1.0  \n",
      "19             0.0             0.0           0.0             1.0  \n",
      "20             1.0             1.0           1.0             1.0  \n",
      "21             0.0             0.0           1.0             1.0  \n",
      "22             0.0             0.0           0.0             0.0  \n",
      "23             0.0             0.0           0.0             0.0  \n",
      "24             0.0             0.0           0.0             0.0  \n",
      "25             0.0             0.0           0.0             0.0  \n",
      "26             0.0             0.0           0.0             0.0  \n",
      "27             1.0             0.0           0.0             1.0  \n",
      "28             0.0             0.0           0.0             0.0  \n",
      "29             1.0             1.0           1.0             1.0  \n",
      "...            ...             ...           ...             ...  \n",
      "333887         NaN             NaN           NaN             NaN  \n",
      "333888         NaN             NaN           NaN             NaN  \n",
      "333889         NaN             NaN           NaN             NaN  \n",
      "333890         NaN             NaN           NaN             NaN  \n",
      "333891         NaN             NaN           NaN             NaN  \n",
      "333892         NaN             NaN           NaN             NaN  \n",
      "333893         NaN             NaN           NaN             NaN  \n",
      "333894         NaN             NaN           NaN             NaN  \n",
      "333895         NaN             NaN           NaN             NaN  \n",
      "333896         NaN             NaN           NaN             NaN  \n",
      "333897         NaN             NaN           NaN             NaN  \n",
      "333898         NaN             NaN           NaN             NaN  \n",
      "333899         NaN             NaN           NaN             NaN  \n",
      "333900         NaN             NaN           NaN             NaN  \n",
      "333901         NaN             NaN           NaN             NaN  \n",
      "333902         NaN             NaN           NaN             NaN  \n",
      "333903         NaN             NaN           NaN             NaN  \n",
      "333904         NaN             NaN           NaN             NaN  \n",
      "333905         NaN             NaN           NaN             NaN  \n",
      "333906         NaN             NaN           NaN             NaN  \n",
      "333907         NaN             NaN           NaN             NaN  \n",
      "333908         NaN             NaN           NaN             NaN  \n",
      "333909         NaN             NaN           NaN             NaN  \n",
      "333910         NaN             NaN           NaN             NaN  \n",
      "333911         NaN             NaN           NaN             NaN  \n",
      "333912         NaN             NaN           NaN             NaN  \n",
      "333913         NaN             NaN           NaN             NaN  \n",
      "333914         NaN             NaN           NaN             NaN  \n",
      "333915         NaN             NaN           NaN             NaN  \n",
      "333916         NaN             NaN           NaN             NaN  \n",
      "\n",
      "[836649 rows x 60 columns]\n",
      "['feature1', 'feature2', 'feature3', 'feature4', 'feature5', 'feature6', 'feature7', 'feature8', 'feature9', 'feature10', 'feature11', 'feature12', 'feature13', 'feature14', 'feature15', 'feature16', 'feature17', 'feature18', 'feature19', 'feature20', 'feature21', 'feature22', 'feature23', 'feature24', 'feature25', 'feature26', 'feature27', 'feature28', 'feature29', 'feature30', 'feature31', 'feature32', 'feature33', 'feature34', 'feature35', 'feature36', 'feature37', 'feature38', 'feature39', 'feature40', 'feature41', 'feature42', 'feature43', 'feature44', 'feature45', 'feature46', 'feature47', 'feature48', 'feature49', 'feature50']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 20)                220       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 962\n",
      "Trainable params: 962\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 336830 samples, validate on 165902 samples\n",
      "Epoch 1/10\n",
      "336830/336830 [==============================] - 1s 4us/step - loss: 0.6930 - val_loss: 0.6929\n",
      "Epoch 2/10\n",
      "336830/336830 [==============================] - 1s 3us/step - loss: 0.6927 - val_loss: 0.6927\n",
      "Epoch 3/10\n",
      "336830/336830 [==============================] - 1s 3us/step - loss: 0.6926 - val_loss: 0.6930\n",
      "Epoch 4/10\n",
      "336830/336830 [==============================] - 1s 4us/step - loss: 0.6925 - val_loss: 0.6929\n",
      "Epoch 5/10\n",
      "336830/336830 [==============================] - 1s 4us/step - loss: 0.6924 - val_loss: 0.6932\n",
      "Epoch 6/10\n",
      "336830/336830 [==============================] - 1s 3us/step - loss: 0.6924 - val_loss: 0.6929\n",
      "Epoch 7/10\n",
      "336830/336830 [==============================] - 1s 3us/step - loss: 0.6923 - val_loss: 0.6928\n",
      "Epoch 8/10\n",
      "336830/336830 [==============================] - 1s 3us/step - loss: 0.6923 - val_loss: 0.6929\n",
      "Epoch 9/10\n",
      "336830/336830 [==============================] - 1s 4us/step - loss: 0.6923 - val_loss: 0.6928\n",
      "Epoch 10/10\n",
      "336830/336830 [==============================] - 1s 4us/step - loss: 0.6923 - val_loss: 0.6937\n",
      "333917/333917 [==============================] - 5s 14us/step\n",
      "- probabilities: [0.51844    0.48229977 0.50998545 0.51192784 0.4965989 ]\n",
      "- target: 1    1.0\n",
      "2    0.0\n",
      "3    0.0\n",
      "4    0.0\n",
      "5    1.0\n",
      "Name: target_bernie, dtype: float64\n",
      "- rounded probability: [1.0, 0.0, 1.0, 1.0, 0.0]\n",
      "- accuracy:  0.08566200582779553\n",
      "- bernie vs elizabeth corr: [[nan nan]\n",
      " [nan nan]]\n",
      "- elizabeth using bernie: 0.08553023655579081\n",
      "333917/333917 [==============================] - 5s 14us/step\n",
      "- validation logloss: 0.6920370216080699\n",
      "Writing predictions to predictions.csv\n",
      "\n",
      "Writing predictions to predictions_2019-04-05_16h02m12s,MLP.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND' ] = 'tensorflow'\n",
    "os.environ['MKL_THREADING_LAYER'] = 'GNU'\n",
    "import keras as ks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import TensorBoard\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import Dropout, Activation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time \n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Set seed for reproducibility\n",
    "    NAME = \"MLP\"\n",
    "    np.random.seed(0)\n",
    "    print(\"Loading data...\")\n",
    "    # Load the data from the CSV files\n",
    "    training_data = pd.read_csv('numerai_training_data.csv', header=0)\n",
    "    print('original train data shape: {},\\t{} \\n\\n \\t:'.format(training_data.shape[0],training_data.shape[1]))\n",
    "\n",
    "    prediction_data = pd.read_csv('numerai_tournament_data.csv', header=0)\n",
    "    print('original prediction data shape: {},\\t{} \\n\\n \\t:'.format(prediction_data.shape[0],prediction_data.shape[1]))\n",
    "    \n",
    "    complete_training_data = pd.concat([training_data, prediction_data])\n",
    "    print('total training / valdation shape {}'.format(complete_training_data))\n",
    "    \n",
    "    # Transform the loaded CSV data into numpy arrays\n",
    "\n",
    "    features = [f for f in list(training_data) if \"feature\" in f]\n",
    "    print(features)\n",
    "\n",
    "    X = training_data[features]\n",
    "    mini= MinMaxScaler(feature_range=(0,1)) \n",
    "    X = mini.fit_transform(X)\n",
    "\n",
    "    Y = training_data[\"target_frank\"]\n",
    "    Y= keras.utils.to_categorical(Y,2) \n",
    "\n",
    "    x_prediction = prediction_data[features]\n",
    "    x_prediction = mini.fit_transform(x_prediction)\n",
    "\n",
    "    ids = prediction_data[\"id\"]  \n",
    "\n",
    "    batch_size = 710\n",
    "\n",
    "    dropout = 0.2\n",
    "\n",
    "    visible = Input(shape=(50,))\n",
    "    hidden1 = Dense(10, activation='relu')(visible)\n",
    "    hidden2 = Dense(20, activation='relu')(hidden1)\n",
    "    hidden3 = Dense(10, activation='relu')(hidden2)\n",
    "    output = Dense(2, activation='sigmoid')(hidden3)\n",
    "    model = Model(inputs=visible, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy',optimizer='rmsprop')\n",
    "    model.summary()\n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "    model.fit(X,Y,batch_size=batch_size,epochs=10,validation_split=0.33,callbacks=[tensorboard])\n",
    "    \n",
    "\n",
    "    y_prediction = model.predict(x_prediction)\n",
    "    evaluate = model.evaluate(x_prediction,y_prediction)\n",
    "    \n",
    "    probabilities = y_prediction[:, 1]\n",
    "    print(\"- probabilities:\", probabilities[1:6])\n",
    "\n",
    "    # We can see the probability does seem to be good at predicting the\n",
    "    # true target correctly.\n",
    "    print(\"- target:\", prediction_data['target_frank'][1:6])\n",
    "    print(\"- rounded probability:\", [np.round(p) for p in probabilities][1:6])\n",
    "\n",
    "    # But overall the accuracy is very low.\n",
    "    correct = [\n",
    "        np.round(x) == y\n",
    "        for (x, y) in zip(probabilities, prediction_data['target_frank'])\n",
    "    ]\n",
    "    print(\"- accuracy: \", sum(correct) / float(prediction_data.shape[0]))\n",
    "\n",
    "    tournament_corr = np.corrcoef(prediction_data['target_frank'],\n",
    "                                  prediction_data['target_elizabeth'])\n",
    "    print(\"- frank vs elizabeth corr:\", tournament_corr)\n",
    "    # You can see that target_elizabeth is accurate using the frank model as well.\n",
    "    correct = [\n",
    "        np.round(x) == y\n",
    "        for (x, y) in zip(probabilities, prediction_data['target_elizabeth'])\n",
    "    ]\n",
    "    print(\"- elizabeth using frank:\",\n",
    "          sum(correct) / float(prediction_data.shape[0]))\n",
    "\n",
    "    # Numerai measures models on logloss instead of accuracy. The lower the logloss the better.\n",
    "    # Numerai only pays models with logloss < 0.693 on the live portion of the tournament data.)\n",
    "\n",
    "    print(\"- validation logloss:\",\n",
    "          model.evaluate(x_prediction,y_prediction))\n",
    "    \n",
    "    results = y_prediction[:, 1]\n",
    "    results_df = pd.DataFrame(data={'probability_frank':results})\n",
    "\n",
    "    joined = pd.DataFrame(ids).join(results_df)\n",
    "    pd.DataFrame(joined[:5])\n",
    "\n",
    "\n",
    "    print(\"Writing predictions to predictions.csv\")\n",
    "    path = 'predictions_{:},{}'.format(time.strftime(\"%Y-%m-%d_%Hh%Mm%Ss\", time.gmtime()),NAME) + '.csv'\n",
    "    print()\n",
    "    print(\"Writing predictions to \" + path.strip())\n",
    "    joined.to_csv(path,float_format='%.15f', index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T23:44:32.056590Z",
     "start_time": "2019-04-04T23:42:41.816364Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "original train data shape: 502732,\t60 \n",
      "\n",
      " \t:\n",
      "original prediction data shape: 333917,\t60 \n",
      "\n",
      " \t:\n",
      "total training / valdation shape                       id   era data_type  feature1  feature2  feature3  \\\n",
      "0       n0003126ff2349f6  era1     train   0.54836   0.31077   0.37524   \n",
      "1       n003d773d29b57ec  era1     train   0.34712   0.40275   0.42747   \n",
      "2       n0074df2dc6810b6  era1     train   0.50871   0.48639   0.47544   \n",
      "3       n0090630f530903e  era1     train   0.61363   0.40268   0.53779   \n",
      "4       n00af19089546fe9  era1     train   0.30704   0.47273   0.54495   \n",
      "5       n011d2da12b1e735  era1     train   0.52336   0.59136   0.60506   \n",
      "6       n014149cadeee55d  era1     train   0.30875   0.62510   0.35229   \n",
      "7       n0148a4dcf539aba  era1     train   0.40632   0.30590   0.43227   \n",
      "8       n015855690d31908  era1     train   0.48193   0.27060   0.50228   \n",
      "9       n0169447f4d6a10e  era1     train   0.51191   0.53663   0.42109   \n",
      "10      n01703ba4eff8fe7  era1     train   0.51829   0.52928   0.41085   \n",
      "11      n01b43e631083764  era1     train   0.61895   0.44075   0.61466   \n",
      "12      n01d1368b061433f  era1     train   0.45529   0.28276   0.61272   \n",
      "13      n01d5bfde31734cd  era1     train   0.48717   0.28982   0.38631   \n",
      "14      n0236fa89e606631  era1     train   0.42585   0.56616   0.62732   \n",
      "15      n023dfa017b93739  era1     train   0.51657   0.52352   0.41695   \n",
      "16      n0294f04019898e8  era1     train   0.45073   0.56795   0.25294   \n",
      "17      n029adf0fa156932  era1     train   0.53833   0.43619   0.58892   \n",
      "18      n02ad6457589c4af  era1     train   0.33576   0.54933   0.39919   \n",
      "19      n02bd240c01b24bd  era1     train   0.47469   0.38517   0.53743   \n",
      "20      n02c48c74a9c9719  era1     train   0.83431   0.36147   0.69324   \n",
      "21      n02cc6384ea5a025  era1     train   0.49314   0.31000   0.36538   \n",
      "22      n02e0356c793b120  era1     train   0.65183   0.40688   0.52271   \n",
      "23      n02e96eeb774238f  era1     train   0.67869   0.46261   0.38758   \n",
      "24      n030964ace7cfa93  era1     train   0.59925   0.43286   0.46698   \n",
      "25      n0311b410c7f8b9d  era1     train   0.37634   0.43578   0.43619   \n",
      "26      n03158868e546c31  era1     train   0.50558   0.47651   0.53912   \n",
      "27      n032f0f9107b0115  era1     train   0.46657   0.28710   0.44637   \n",
      "28      n03484f670162e63  era1     train   0.33571   0.45678   0.25402   \n",
      "29      n03534209c17edf5  era1     train   0.35224   0.48464   0.47109   \n",
      "...                  ...   ...       ...       ...       ...       ...   \n",
      "333887  nfe90ce695609608  eraX      live   0.22473   0.31813   0.36786   \n",
      "333888  nfe932f6fe046898  eraX      live   0.57944   0.41360   0.34797   \n",
      "333889  nfe93cb92e1f3004  eraX      live   0.62594   0.53724   0.51025   \n",
      "333890  nfe943152f1a5550  eraX      live   0.18081   0.50046   0.44782   \n",
      "333891  nfe9f241941633d3  eraX      live   0.44638   0.41328   0.59583   \n",
      "333892  nfed818ba71b4665  eraX      live   0.45843   0.49678   0.51053   \n",
      "333893  nfeda644836d563c  eraX      live   0.42282   0.57715   0.32823   \n",
      "333894  nfee50cc929893b4  eraX      live   0.22692   0.41532   0.47914   \n",
      "333895  nfeeb39596d24005  eraX      live   0.67745   0.47704   0.69068   \n",
      "333896  nfeee0c1087730ac  eraX      live   0.30302   0.41184   0.37730   \n",
      "333897  nfef761c30da657a  eraX      live   0.77022   0.39439   0.61509   \n",
      "333898  nff0947a55e1d394  eraX      live   0.61578   0.35275   0.51831   \n",
      "333899  nff1774c41a64a26  eraX      live   0.47422   0.46354   0.53371   \n",
      "333900  nff1ea0914cf90f4  eraX      live   0.30684   0.49977   0.64083   \n",
      "333901  nff3fa4347689e62  eraX      live   0.31594   0.43230   0.58978   \n",
      "333902  nff548af92ef8dc6  eraX      live   0.59936   0.51865   0.36845   \n",
      "333903  nff646fb2b7d329e  eraX      live   0.47254   0.33300   0.49866   \n",
      "333904  nff70242e4559ef9  eraX      live   0.35272   0.34020   0.42336   \n",
      "333905  nff77343157e0ab0  eraX      live   0.50806   0.52088   0.40247   \n",
      "333906  nff86f7979a4483f  eraX      live   0.43828   0.50917   0.31763   \n",
      "333907  nff9843ac1881c92  eraX      live   0.33373   0.45027   0.51017   \n",
      "333908  nffa24ee2fa8d8ae  eraX      live   0.58084   0.60522   0.52742   \n",
      "333909  nffb20c86d6252bf  eraX      live   0.38466   0.36028   0.38189   \n",
      "333910  nffb86f0b9948cab  eraX      live   0.20704   0.65434   0.75437   \n",
      "333911  nffc091bd2914bc3  eraX      live   0.17362   0.44136   0.45643   \n",
      "333912  nffc4e712f2a7c7e  eraX      live   0.34347   0.32474   0.39716   \n",
      "333913  nffd1353a5e2baad  eraX      live   0.55615   0.49798   0.46160   \n",
      "333914  nffd9965e9b4fc2f  eraX      live   0.54227   0.40336   0.30860   \n",
      "333915  nffe587f7f3af1e7  eraX      live   0.26457   0.38589   0.48547   \n",
      "333916  nfffc562888c5ef0  eraX      live   0.59965   0.50705   0.46930   \n",
      "\n",
      "        feature4  feature5  feature6  feature7       ...        feature48  \\\n",
      "0        0.49490   0.53217   0.48388   0.50220       ...          0.55239   \n",
      "1        0.44006   0.47866   0.44055   0.59182       ...          0.46029   \n",
      "2        0.40306   0.53436   0.64028   0.51420       ...          0.40596   \n",
      "3        0.37045   0.58711   0.59900   0.62428       ...          0.53878   \n",
      "4        0.48692   0.47348   0.34695   0.41506       ...          0.46431   \n",
      "5        0.30085   0.41742   0.47290   0.56301       ...          0.55917   \n",
      "6        0.48021   0.71347   0.36977   0.57899       ...          0.40538   \n",
      "7        0.61999   0.51016   0.52714   0.74017       ...          0.77664   \n",
      "8        0.63037   0.50734   0.45545   0.41927       ...          0.52887   \n",
      "9        0.38838   0.51222   0.53249   0.70187       ...          0.58353   \n",
      "10       0.44842   0.49727   0.66198   0.53513       ...          0.54271   \n",
      "11       0.51365   0.45339   0.51101   0.39488       ...          0.33293   \n",
      "12       0.48615   0.54590   0.59111   0.55893       ...          0.55932   \n",
      "13       0.58569   0.46994   0.51753   0.41116       ...          0.55347   \n",
      "14       0.40727   0.36868   0.51526   0.29604       ...          0.53480   \n",
      "15       0.39198   0.54481   0.55880   0.48240       ...          0.45665   \n",
      "16       0.37918   0.65944   0.61828   0.62431       ...          0.44610   \n",
      "17       0.53740   0.32951   0.44737   0.39056       ...          0.64044   \n",
      "18       0.51449   0.44705   0.53717   0.53424       ...          0.53355   \n",
      "19       0.46013   0.63523   0.48265   0.50777       ...          0.48970   \n",
      "20       0.49827   0.22947   0.46324   0.49544       ...          0.75846   \n",
      "21       0.49332   0.68842   0.59665   0.57461       ...          0.54245   \n",
      "22       0.69467   0.62700   0.50894   0.44084       ...          0.64244   \n",
      "23       0.33676   0.43296   0.56874   0.67007       ...          0.63132   \n",
      "24       0.39413   0.58728   0.38304   0.60579       ...          0.50472   \n",
      "25       0.45756   0.39833   0.58444   0.48982       ...          0.58255   \n",
      "26       0.48904   0.43819   0.53960   0.55395       ...          0.49973   \n",
      "27       0.50982   0.63898   0.45895   0.50982       ...          0.52851   \n",
      "28       0.47962   0.50144   0.58369   0.62196       ...          0.79120   \n",
      "29       0.41567   0.57632   0.34524   0.58772       ...          0.47177   \n",
      "...          ...       ...       ...       ...       ...              ...   \n",
      "333887   0.63595   0.63072   0.40458   0.60937       ...          0.43822   \n",
      "333888   0.30309   0.59500   0.62593   0.79868       ...          0.51592   \n",
      "333889   0.35304   0.42196   0.54384   0.40673       ...          0.81155   \n",
      "333890   0.19874   0.31669   0.56764   0.27662       ...          0.41892   \n",
      "333891   0.51726   0.32879   0.35747   0.27011       ...          0.40805   \n",
      "333892   0.42903   0.34398   0.17531   0.45940       ...          0.57967   \n",
      "333893   0.19175   0.54352   0.48392   0.58288       ...          0.61240   \n",
      "333894   0.47158   0.43951   0.51312   0.45735       ...          0.49855   \n",
      "333895   0.49231   0.42467   0.50623   0.21764       ...          0.51575   \n",
      "333896   0.42511   0.37806   0.48737   0.46332       ...          0.54794   \n",
      "333897   0.53305   0.54400   0.58841   0.36758       ...          0.53317   \n",
      "333898   0.48969   0.54165   0.67066   0.40535       ...          0.50956   \n",
      "333899   0.56766   0.53374   0.46601   0.33106       ...          0.36488   \n",
      "333900   0.60022   0.45940   0.27945   0.52223       ...          0.42632   \n",
      "333901   0.40462   0.49036   0.55442   0.55895       ...          0.57192   \n",
      "333902   0.37223   0.70834   0.68634   0.65859       ...          0.65068   \n",
      "333903   0.55127   0.47440   0.32557   0.62731       ...          0.32907   \n",
      "333904   0.60970   0.52893   0.42176   0.53982       ...          0.45713   \n",
      "333905   0.41444   0.50771   0.75231   0.51795       ...          0.65090   \n",
      "333906   0.45235   0.68345   0.47711   0.59222       ...          0.44174   \n",
      "333907   0.34386   0.53338   0.49982   0.48504       ...          0.35708   \n",
      "333908   0.24781   0.53824   0.71670   0.53123       ...          0.64067   \n",
      "333909   0.50206   0.44006   0.57703   0.51574       ...          0.46857   \n",
      "333910   0.43345   0.43357   0.26700   0.38337       ...          0.35225   \n",
      "333911   0.63990   0.56944   0.37104   0.36184       ...          0.42273   \n",
      "333912   0.67911   0.49659   0.37694   0.47598       ...          0.28455   \n",
      "333913   0.42583   0.53056   0.51423   0.72328       ...          0.51966   \n",
      "333914   0.41165   0.51681   0.63769   0.67705       ...          0.75881   \n",
      "333915   0.58102   0.57494   0.34050   0.49688       ...          0.40776   \n",
      "333916   0.38863   0.56402   0.47352   0.66432       ...          0.56368   \n",
      "\n",
      "        feature49  feature50  target_bernie  target_elizabeth  target_jordan  \\\n",
      "0         0.64054    0.52182            1.0               1.0            1.0   \n",
      "1         0.62941    0.55010            1.0               1.0            1.0   \n",
      "2         0.54731    0.39061            0.0               0.0            1.0   \n",
      "3         0.47776    0.36835            0.0               0.0            0.0   \n",
      "4         0.49482    0.60452            1.0               1.0            1.0   \n",
      "5         0.31260    0.35691            0.0               0.0            0.0   \n",
      "6         0.52690    0.39767            1.0               1.0            1.0   \n",
      "7         0.51623    0.49104            0.0               0.0            0.0   \n",
      "8         0.44384    0.57269            0.0               0.0            0.0   \n",
      "9         0.42566    0.50082            1.0               1.0            1.0   \n",
      "10        0.43434    0.52720            0.0               0.0            0.0   \n",
      "11        0.30265    0.45951            0.0               0.0            0.0   \n",
      "12        0.46798    0.53359            1.0               1.0            1.0   \n",
      "13        0.42172    0.72972            0.0               0.0            0.0   \n",
      "14        0.48525    0.59524            1.0               1.0            1.0   \n",
      "15        0.50711    0.59955            0.0               0.0            0.0   \n",
      "16        0.57642    0.62569            0.0               0.0            0.0   \n",
      "17        0.22801    0.44203            1.0               1.0            1.0   \n",
      "18        0.49124    0.39731            1.0               1.0            1.0   \n",
      "19        0.41920    0.38751            0.0               0.0            0.0   \n",
      "20        0.31290    0.61816            1.0               1.0            1.0   \n",
      "21        0.40880    0.53372            0.0               1.0            0.0   \n",
      "22        0.17882    0.40123            0.0               0.0            0.0   \n",
      "23        0.46647    0.53402            0.0               0.0            0.0   \n",
      "24        0.33878    0.56262            0.0               0.0            0.0   \n",
      "25        0.52865    0.63495            0.0               0.0            0.0   \n",
      "26        0.47976    0.44513            0.0               0.0            0.0   \n",
      "27        0.43489    0.48806            0.0               0.0            0.0   \n",
      "28        0.72922    0.49451            0.0               0.0            0.0   \n",
      "29        0.55393    0.49058            1.0               1.0            1.0   \n",
      "...           ...        ...            ...               ...            ...   \n",
      "333887    0.56037    0.68570            NaN               NaN            NaN   \n",
      "333888    0.50056    0.47213            NaN               NaN            NaN   \n",
      "333889    0.66506    0.41550            NaN               NaN            NaN   \n",
      "333890    0.56516    0.38036            NaN               NaN            NaN   \n",
      "333891    0.43037    0.49400            NaN               NaN            NaN   \n",
      "333892    0.60226    0.56339            NaN               NaN            NaN   \n",
      "333893    0.53195    0.47703            NaN               NaN            NaN   \n",
      "333894    0.65543    0.56419            NaN               NaN            NaN   \n",
      "333895    0.36139    0.43150            NaN               NaN            NaN   \n",
      "333896    0.58540    0.56483            NaN               NaN            NaN   \n",
      "333897    0.34749    0.43542            NaN               NaN            NaN   \n",
      "333898    0.53561    0.56108            NaN               NaN            NaN   \n",
      "333899    0.52181    0.30183            NaN               NaN            NaN   \n",
      "333900    0.44151    0.68143            NaN               NaN            NaN   \n",
      "333901    0.66334    0.52303            NaN               NaN            NaN   \n",
      "333902    0.48645    0.37984            NaN               NaN            NaN   \n",
      "333903    0.48351    0.58909            NaN               NaN            NaN   \n",
      "333904    0.46877    0.64382            NaN               NaN            NaN   \n",
      "333905    0.60046    0.48012            NaN               NaN            NaN   \n",
      "333906    0.38848    0.68848            NaN               NaN            NaN   \n",
      "333907    0.54039    0.58356            NaN               NaN            NaN   \n",
      "333908    0.49853    0.49006            NaN               NaN            NaN   \n",
      "333909    0.55217    0.52806            NaN               NaN            NaN   \n",
      "333910    0.43661    0.55406            NaN               NaN            NaN   \n",
      "333911    0.67505    0.47899            NaN               NaN            NaN   \n",
      "333912    0.42267    0.60347            NaN               NaN            NaN   \n",
      "333913    0.43774    0.54886            NaN               NaN            NaN   \n",
      "333914    0.48803    0.56248            NaN               NaN            NaN   \n",
      "333915    0.38404    0.64771            NaN               NaN            NaN   \n",
      "333916    0.33147    0.42495            NaN               NaN            NaN   \n",
      "\n",
      "        target_ken  target_charles  target_frank  target_hillary  \n",
      "0              1.0             1.0           1.0             1.0  \n",
      "1              1.0             1.0           1.0             1.0  \n",
      "2              0.0             0.0           0.0             0.0  \n",
      "3              0.0             0.0           0.0             0.0  \n",
      "4              1.0             1.0           1.0             1.0  \n",
      "5              0.0             0.0           0.0             0.0  \n",
      "6              1.0             1.0           1.0             1.0  \n",
      "7              0.0             0.0           0.0             0.0  \n",
      "8              0.0             0.0           0.0             0.0  \n",
      "9              1.0             1.0           1.0             1.0  \n",
      "10             0.0             0.0           0.0             0.0  \n",
      "11             0.0             0.0           0.0             0.0  \n",
      "12             1.0             1.0           1.0             1.0  \n",
      "13             0.0             0.0           0.0             0.0  \n",
      "14             1.0             1.0           1.0             1.0  \n",
      "15             0.0             1.0           1.0             0.0  \n",
      "16             0.0             0.0           0.0             0.0  \n",
      "17             1.0             1.0           1.0             1.0  \n",
      "18             1.0             1.0           1.0             1.0  \n",
      "19             0.0             0.0           0.0             1.0  \n",
      "20             1.0             1.0           1.0             1.0  \n",
      "21             0.0             0.0           1.0             1.0  \n",
      "22             0.0             0.0           0.0             0.0  \n",
      "23             0.0             0.0           0.0             0.0  \n",
      "24             0.0             0.0           0.0             0.0  \n",
      "25             0.0             0.0           0.0             0.0  \n",
      "26             0.0             0.0           0.0             0.0  \n",
      "27             1.0             0.0           0.0             1.0  \n",
      "28             0.0             0.0           0.0             0.0  \n",
      "29             1.0             1.0           1.0             1.0  \n",
      "...            ...             ...           ...             ...  \n",
      "333887         NaN             NaN           NaN             NaN  \n",
      "333888         NaN             NaN           NaN             NaN  \n",
      "333889         NaN             NaN           NaN             NaN  \n",
      "333890         NaN             NaN           NaN             NaN  \n",
      "333891         NaN             NaN           NaN             NaN  \n",
      "333892         NaN             NaN           NaN             NaN  \n",
      "333893         NaN             NaN           NaN             NaN  \n",
      "333894         NaN             NaN           NaN             NaN  \n",
      "333895         NaN             NaN           NaN             NaN  \n",
      "333896         NaN             NaN           NaN             NaN  \n",
      "333897         NaN             NaN           NaN             NaN  \n",
      "333898         NaN             NaN           NaN             NaN  \n",
      "333899         NaN             NaN           NaN             NaN  \n",
      "333900         NaN             NaN           NaN             NaN  \n",
      "333901         NaN             NaN           NaN             NaN  \n",
      "333902         NaN             NaN           NaN             NaN  \n",
      "333903         NaN             NaN           NaN             NaN  \n",
      "333904         NaN             NaN           NaN             NaN  \n",
      "333905         NaN             NaN           NaN             NaN  \n",
      "333906         NaN             NaN           NaN             NaN  \n",
      "333907         NaN             NaN           NaN             NaN  \n",
      "333908         NaN             NaN           NaN             NaN  \n",
      "333909         NaN             NaN           NaN             NaN  \n",
      "333910         NaN             NaN           NaN             NaN  \n",
      "333911         NaN             NaN           NaN             NaN  \n",
      "333912         NaN             NaN           NaN             NaN  \n",
      "333913         NaN             NaN           NaN             NaN  \n",
      "333914         NaN             NaN           NaN             NaN  \n",
      "333915         NaN             NaN           NaN             NaN  \n",
      "333916         NaN             NaN           NaN             NaN  \n",
      "\n",
      "[836649 rows x 60 columns]\n",
      "['feature1', 'feature2', 'feature3', 'feature4', 'feature5', 'feature6', 'feature7', 'feature8', 'feature9', 'feature10', 'feature11', 'feature12', 'feature13', 'feature14', 'feature15', 'feature16', 'feature17', 'feature18', 'feature19', 'feature20', 'feature21', 'feature22', 'feature23', 'feature24', 'feature25', 'feature26', 'feature27', 'feature28', 'feature29', 'feature30', 'feature31', 'feature32', 'feature33', 'feature34', 'feature35', 'feature36', 'feature37', 'feature38', 'feature39', 'feature40', 'feature41', 'feature42', 'feature43', 'feature44', 'feature45', 'feature46', 'feature47', 'feature48', 'feature49', 'feature50']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 336830 samples, validate on 165902 samples\n",
      "Epoch 1/10\n",
      "336830/336830 [==============================] - 7s 21us/step - loss: 0.7100 - val_loss: 0.6933\n",
      "Epoch 2/10\n",
      "336830/336830 [==============================] - 7s 20us/step - loss: 0.6951 - val_loss: 0.6930\n",
      "Epoch 3/10\n",
      "336830/336830 [==============================] - 6s 19us/step - loss: 0.6928 - val_loss: 0.6937\n",
      "Epoch 4/10\n",
      "336830/336830 [==============================] - 6s 19us/step - loss: 0.6922 - val_loss: 0.6933\n",
      "Epoch 5/10\n",
      "336830/336830 [==============================] - 6s 19us/step - loss: 0.6920 - val_loss: 0.6951\n",
      "Epoch 6/10\n",
      "336830/336830 [==============================] - 7s 19us/step - loss: 0.6918 - val_loss: 0.6949\n",
      "Epoch 7/10\n",
      "336830/336830 [==============================] - 6s 19us/step - loss: 0.6916 - val_loss: 0.6941\n",
      "Epoch 8/10\n",
      "336830/336830 [==============================] - 6s 19us/step - loss: 0.6914 - val_loss: 0.6937\n",
      "Epoch 9/10\n",
      "336830/336830 [==============================] - 6s 19us/step - loss: 0.6912 - val_loss: 0.6936\n",
      "Epoch 10/10\n",
      "336830/336830 [==============================] - 6s 19us/step - loss: 0.6910 - val_loss: 0.6944\n",
      "333917/333917 [==============================] - 8s 23us/step\n",
      "- probabilities: [0.5038235  0.47747394 0.52584994 0.50304914 0.508566  ]\n",
      "- target: 1    1.0\n",
      "2    0.0\n",
      "3    0.0\n",
      "4    0.0\n",
      "5    1.0\n",
      "Name: target_bernie, dtype: float64\n",
      "- rounded probability: [1.0, 0.0, 1.0, 1.0, 1.0]\n",
      "- accuracy:  0.08481448982831062\n",
      "- bernie vs elizabeth corr: [[nan nan]\n",
      " [nan nan]]\n",
      "- elizabeth using bernie: 0.08516787105777783\n",
      "333917/333917 [==============================] - 8s 23us/step\n",
      "- validation logloss: 0.6917378780136868\n",
      "Writing predictions to predictions.csv\n",
      "\n",
      "Writing predictions to predictions_2019-04-04_23h44m30s.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND' ] = 'tensorflow'\n",
    "os.environ['MKL_THREADING_LAYER'] = 'GNU'\n",
    "import keras as ks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import TensorBoard\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import Dropout, Activation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time \n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Set seed for reproducibility\n",
    "    NAME = \"MLP\"\n",
    "    np.random.seed(0)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Loading data...\")\n",
    "\n",
    "    # Load the data from the CSV files\n",
    "\n",
    "    training_data = pd.read_csv('numerai_training_data.csv', header=0)\n",
    "    print('original train data shape: {},\\t{} \\n\\n \\t:'.format(training_data.shape[0],training_data.shape[1]))\n",
    "\n",
    "    prediction_data = pd.read_csv('numerai_tournament_data.csv', header=0)\n",
    "    print('original prediction data shape: {},\\t{} \\n\\n \\t:'.format(prediction_data.shape[0],prediction_data.shape[1]))\n",
    "    \n",
    "    complete_training_data = pd.concat([training_data, prediction_data])\n",
    "    print('total training / valdation shape {}'.format(complete_training_data))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Transform the loaded CSV data into numpy arrays\n",
    "\n",
    "    features = [f for f in list(training_data) if \"feature\" in f]\n",
    "    print(features)\n",
    "\n",
    "    X = training_data[features]\n",
    "    mini= MinMaxScaler(feature_range=(0,1)) \n",
    "    X = mini.fit_transform(X)\n",
    "#     X = X.values\n",
    "    Y = training_data[\"target_bernie\"]\n",
    "    Y= keras.utils.to_categorical(Y,2) \n",
    "\n",
    "    x_prediction = prediction_data[features]\n",
    "    x_prediction = mini.fit_transform(x_prediction)\n",
    "\n",
    "    ids = prediction_data[\"id\"]\n",
    "\n",
    "#     X = X.values\n",
    "\n",
    "#     Y = to_categorical(Y, num_classes=2)\n",
    "\n",
    "    \n",
    "\n",
    "    batch_size = 710\n",
    "\n",
    "    dropout = 0.2\n",
    "\n",
    "    \n",
    "\n",
    "    m_in = Input(shape=(50,))\n",
    "\n",
    "    m1 = Dense(50,)(m_in)\n",
    "    m1 = Activation('relu')(m1)\n",
    "    m1 = BatchNormalization(momentum=.99999,axis=-1)(m1)\n",
    "\n",
    "    m2 = Dense(100)(m1)\n",
    "    m2 = Activation('relu')(m2)\n",
    "    m2 = BatchNormalization(momentum=.999,axis=-1)(m2)\n",
    "    \n",
    "    m3 = Dense(25)(m2)\n",
    "    m3 = Activation('relu')(m3)\n",
    "    \n",
    "    m3 = Dense(25)(m3)\n",
    "    m3 = Dropout(dropout)(m3) \n",
    "    m3 = Activation('relu')(m3)\n",
    "    \n",
    "    m3 = Dense(25)(m3)\n",
    "    m3 = Activation('relu')(m3)\n",
    "    m3 = BatchNormalization(momentum=.99,axis=-1)(m3)\n",
    "    \n",
    "    m3 = Dense(100)(m3)\n",
    "    m3 = Activation('relu')(m3)\n",
    "    \n",
    "    m3 = Dense(25)(m3)\n",
    "    m3 = Activation('relu')(m3) \n",
    "    \n",
    "    m4 = Dense(25)(m3)\n",
    "    m4 = Activation('relu')(m4) \n",
    "    m4 = Dropout(dropout)(m4) \n",
    "    m4 = BatchNormalization(momentum=.9,axis=-1)(m4)\n",
    "\n",
    "    \n",
    "    \n",
    "    m5 = Dense(2)(m4)\n",
    "    m_out = Activation('sigmoid')(m5)\n",
    "\n",
    "    model = Model(inputs=m_in, outputs=m_out)\n",
    "\n",
    "\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',optimizer='rmsprop')\n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "    model.fit(X,Y,batch_size=batch_size,epochs=10,validation_split=0.33,callbacks=[tensorboard])\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    y_prediction = model.predict(x_prediction)\n",
    "    evaluate = model.evaluate(x_prediction,y_prediction)\n",
    "    \n",
    "    probabilities = y_prediction[:, 1]\n",
    "    print(\"- probabilities:\", probabilities[1:6])\n",
    "\n",
    "    # We can see the probability does seem to be good at predicting the\n",
    "    # true target correctly.\n",
    "    print(\"- target:\", prediction_data['target_bernie'][1:6])\n",
    "    print(\"- rounded probability:\", [np.round(p) for p in probabilities][1:6])\n",
    "\n",
    "    # But overall the accuracy is very low.\n",
    "    correct = [\n",
    "        np.round(x) == y\n",
    "        for (x, y) in zip(probabilities, prediction_data['target_bernie'])\n",
    "    ]\n",
    "    print(\"- accuracy: \", sum(correct) / float(prediction_data.shape[0]))\n",
    "\n",
    "    # The targets for each of the tournaments are very correlated.\n",
    "    tournament_corr = np.corrcoef(prediction_data['target_bernie'],\n",
    "                                  prediction_data['target_elizabeth'])\n",
    "    print(\"- bernie vs elizabeth corr:\", tournament_corr)\n",
    "    # You can see that target_elizabeth is accurate using the bernie model as well.\n",
    "    correct = [\n",
    "        np.round(x) == y\n",
    "        for (x, y) in zip(probabilities, prediction_data['target_elizabeth'])\n",
    "    ]\n",
    "    print(\"- elizabeth using bernie:\",\n",
    "          sum(correct) / float(prediction_data.shape[0]))\n",
    "\n",
    "    # Numerai measures models on logloss instead of accuracy. The lower the logloss the better.\n",
    "    # Numerai only pays models with logloss < 0.693 on the live portion of the tournament data.)\n",
    "\n",
    "    print(\"- validation logloss:\",\n",
    "          model.evaluate(x_prediction,y_prediction))\n",
    "\n",
    "    results = y_prediction[:, 1]\n",
    "\n",
    "    # -----\n",
    "\n",
    "    \n",
    "\n",
    "    results_df = pd.DataFrame(data={'probability_bernie':results})\n",
    "\n",
    "    joined = pd.DataFrame(ids).join(results_df)\n",
    "    pd.DataFrame(joined[:5])\n",
    "\n",
    "\n",
    "    print(\"Writing predictions to predictions.csv\")\n",
    "\n",
    "    # Save the predictions out to a CSV file\n",
    "    path = 'predictions_{:}'.format(time.strftime(\"%Y-%m-%d_%Hh%Mm%Ss\", time.gmtime())) + '.csv'\n",
    "    print()\n",
    "    print(\"Writing predictions to \" + path.strip())\n",
    "    # # Save the predictions out to a CSV file\n",
    "    joined.to_csv(path,float_format='%.15f', index=False)\n",
    "\n",
    "\n",
    "    # Now you can upload these predictions on numer.ai\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    m4 = Dense(25)(m3)\n",
    "    m4 = Activation('sigmoid')(m4)\n",
    "    m4 = Dropout(dropout)(m4) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
