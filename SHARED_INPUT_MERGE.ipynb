{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T16:22:27.909162Z",
     "start_time": "2019-04-21T16:18:05.869683Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "original train data shape: 502732,\t60 \n",
      "\n",
      " \t:\n",
      "original prediction data shape: 333924,\t60 \n",
      "\n",
      " \t:\n",
      "total training / valdation shape                       id   era data_type  feature1  feature2  feature3  \\\n",
      "0       n0003126ff2349f6  era1     train   0.54836   0.31077   0.37524   \n",
      "1       n003d773d29b57ec  era1     train   0.34712   0.40275   0.42747   \n",
      "2       n0074df2dc6810b6  era1     train   0.50871   0.48639   0.47544   \n",
      "3       n0090630f530903e  era1     train   0.61363   0.40268   0.53779   \n",
      "4       n00af19089546fe9  era1     train   0.30704   0.47273   0.54495   \n",
      "5       n011d2da12b1e735  era1     train   0.52336   0.59136   0.60506   \n",
      "6       n014149cadeee55d  era1     train   0.30875   0.62510   0.35229   \n",
      "7       n0148a4dcf539aba  era1     train   0.40632   0.30590   0.43227   \n",
      "8       n015855690d31908  era1     train   0.48193   0.27060   0.50228   \n",
      "9       n0169447f4d6a10e  era1     train   0.51191   0.53663   0.42109   \n",
      "10      n01703ba4eff8fe7  era1     train   0.51829   0.52928   0.41085   \n",
      "11      n01b43e631083764  era1     train   0.61895   0.44075   0.61466   \n",
      "12      n01d1368b061433f  era1     train   0.45529   0.28276   0.61272   \n",
      "13      n01d5bfde31734cd  era1     train   0.48717   0.28982   0.38631   \n",
      "14      n0236fa89e606631  era1     train   0.42585   0.56616   0.62732   \n",
      "15      n023dfa017b93739  era1     train   0.51657   0.52352   0.41695   \n",
      "16      n0294f04019898e8  era1     train   0.45073   0.56795   0.25294   \n",
      "17      n029adf0fa156932  era1     train   0.53833   0.43619   0.58892   \n",
      "18      n02ad6457589c4af  era1     train   0.33576   0.54933   0.39919   \n",
      "19      n02bd240c01b24bd  era1     train   0.47469   0.38517   0.53743   \n",
      "20      n02c48c74a9c9719  era1     train   0.83431   0.36147   0.69324   \n",
      "21      n02cc6384ea5a025  era1     train   0.49314   0.31000   0.36538   \n",
      "22      n02e0356c793b120  era1     train   0.65183   0.40688   0.52271   \n",
      "23      n02e96eeb774238f  era1     train   0.67869   0.46261   0.38758   \n",
      "24      n030964ace7cfa93  era1     train   0.59925   0.43286   0.46698   \n",
      "25      n0311b410c7f8b9d  era1     train   0.37634   0.43578   0.43619   \n",
      "26      n03158868e546c31  era1     train   0.50558   0.47651   0.53912   \n",
      "27      n032f0f9107b0115  era1     train   0.46657   0.28710   0.44637   \n",
      "28      n03484f670162e63  era1     train   0.33571   0.45678   0.25402   \n",
      "29      n03534209c17edf5  era1     train   0.35224   0.48464   0.47109   \n",
      "...                  ...   ...       ...       ...       ...       ...   \n",
      "333894  nfec21cd5c5b33ad  eraX      live   0.38965   0.45030   0.52700   \n",
      "333895  nfec98c25ae22d54  eraX      live   0.69356   0.47442   0.50694   \n",
      "333896  nfed0b30d2d442cc  eraX      live   0.38492   0.42707   0.44937   \n",
      "333897  nfed69bed5d087e9  eraX      live   0.48369   0.62619   0.38787   \n",
      "333898  nfed8728787c35f7  eraX      live   0.49617   0.44814   0.45066   \n",
      "333899  nfeda5d5d6cfa79e  eraX      live   0.62005   0.29275   0.30218   \n",
      "333900  nfeeb6e4cd56ca3a  eraX      live   0.51599   0.49497   0.68660   \n",
      "333901  nfef1f9ddc8df2af  eraX      live   0.34322   0.31093   0.49169   \n",
      "333902  nff08e8747d575c2  eraX      live   0.49919   0.54032   0.71305   \n",
      "333903  nff1e7fb2384e709  eraX      live   0.62816   0.54499   0.67274   \n",
      "333904  nff3a0d69e6a871e  eraX      live   0.76577   0.46736   0.48493   \n",
      "333905  nff430c26ed109de  eraX      live   0.47557   0.35675   0.57149   \n",
      "333906  nff62b58beb64c4f  eraX      live   0.42865   0.60958   0.56111   \n",
      "333907  nff7234fd21aa8ed  eraX      live   0.39509   0.46559   0.51895   \n",
      "333908  nff73210358cd504  eraX      live   0.65093   0.43259   0.42214   \n",
      "333909  nff770cbee3cd1bf  eraX      live   0.27143   0.73245   0.57361   \n",
      "333910  nff939fd6397eb8d  eraX      live   0.37668   0.35264   0.45309   \n",
      "333911  nff94fb2bec22d22  eraX      live   0.31030   0.40159   0.46879   \n",
      "333912  nff9af0d05671f92  eraX      live   0.46025   0.33584   0.45436   \n",
      "333913  nffba5bfdde304d5  eraX      live   0.40388   0.34720   0.62989   \n",
      "333914  nffbe892b3bf9fe7  eraX      live   0.65109   0.27549   0.19570   \n",
      "333915  nffc2a2b2ef54a97  eraX      live   0.45028   0.43775   0.64112   \n",
      "333916  nffd3c0fbc2de0e8  eraX      live   0.40023   0.39563   0.37794   \n",
      "333917  nffd754673e8f726  eraX      live   0.55810   0.50070   0.37339   \n",
      "333918  nffdb5c4428c68c3  eraX      live   0.47117   0.49835   0.43437   \n",
      "333919  nffdb97eb02f3e90  eraX      live   0.78880   0.47460   0.61443   \n",
      "333920  nffe15031d080c66  eraX      live   0.49492   0.50500   0.57896   \n",
      "333921  nffedbfc9ba8f6b2  eraX      live   0.31902   0.52280   0.52200   \n",
      "333922  nfff54490e982036  eraX      live   0.48698   0.59598   0.56342   \n",
      "333923  nfffcee8f1fddd45  eraX      live   0.56285   0.57647   0.52050   \n",
      "\n",
      "        feature4  feature5  feature6  feature7  ...  feature48  feature49  \\\n",
      "0        0.49490   0.53217   0.48388   0.50220  ...    0.55239    0.64054   \n",
      "1        0.44006   0.47866   0.44055   0.59182  ...    0.46029    0.62941   \n",
      "2        0.40306   0.53436   0.64028   0.51420  ...    0.40596    0.54731   \n",
      "3        0.37045   0.58711   0.59900   0.62428  ...    0.53878    0.47776   \n",
      "4        0.48692   0.47348   0.34695   0.41506  ...    0.46431    0.49482   \n",
      "5        0.30085   0.41742   0.47290   0.56301  ...    0.55917    0.31260   \n",
      "6        0.48021   0.71347   0.36977   0.57899  ...    0.40538    0.52690   \n",
      "7        0.61999   0.51016   0.52714   0.74017  ...    0.77664    0.51623   \n",
      "8        0.63037   0.50734   0.45545   0.41927  ...    0.52887    0.44384   \n",
      "9        0.38838   0.51222   0.53249   0.70187  ...    0.58353    0.42566   \n",
      "10       0.44842   0.49727   0.66198   0.53513  ...    0.54271    0.43434   \n",
      "11       0.51365   0.45339   0.51101   0.39488  ...    0.33293    0.30265   \n",
      "12       0.48615   0.54590   0.59111   0.55893  ...    0.55932    0.46798   \n",
      "13       0.58569   0.46994   0.51753   0.41116  ...    0.55347    0.42172   \n",
      "14       0.40727   0.36868   0.51526   0.29604  ...    0.53480    0.48525   \n",
      "15       0.39198   0.54481   0.55880   0.48240  ...    0.45665    0.50711   \n",
      "16       0.37918   0.65944   0.61828   0.62431  ...    0.44610    0.57642   \n",
      "17       0.53740   0.32951   0.44737   0.39056  ...    0.64044    0.22801   \n",
      "18       0.51449   0.44705   0.53717   0.53424  ...    0.53355    0.49124   \n",
      "19       0.46013   0.63523   0.48265   0.50777  ...    0.48970    0.41920   \n",
      "20       0.49827   0.22947   0.46324   0.49544  ...    0.75846    0.31290   \n",
      "21       0.49332   0.68842   0.59665   0.57461  ...    0.54245    0.40880   \n",
      "22       0.69467   0.62700   0.50894   0.44084  ...    0.64244    0.17882   \n",
      "23       0.33676   0.43296   0.56874   0.67007  ...    0.63132    0.46647   \n",
      "24       0.39413   0.58728   0.38304   0.60579  ...    0.50472    0.33878   \n",
      "25       0.45756   0.39833   0.58444   0.48982  ...    0.58255    0.52865   \n",
      "26       0.48904   0.43819   0.53960   0.55395  ...    0.49973    0.47976   \n",
      "27       0.50982   0.63898   0.45895   0.50982  ...    0.52851    0.43489   \n",
      "28       0.47962   0.50144   0.58369   0.62196  ...    0.79120    0.72922   \n",
      "29       0.41567   0.57632   0.34524   0.58772  ...    0.47177    0.55393   \n",
      "...          ...       ...       ...       ...  ...        ...        ...   \n",
      "333894   0.37210   0.46008   0.43412   0.63377  ...    0.57366    0.62540   \n",
      "333895   0.41392   0.47895   0.66038   0.49995  ...    0.62853    0.29633   \n",
      "333896   0.42662   0.54812   0.49072   0.66651  ...    0.48536    0.48631   \n",
      "333897   0.14462   0.56918   0.42828   0.64804  ...    0.54346    0.46759   \n",
      "333898   0.42089   0.68011   0.53420   0.57086  ...    0.43619    0.35083   \n",
      "333899   0.48158   0.45471   0.62305   0.59370  ...    0.43859    0.48730   \n",
      "333900   0.51868   0.35298   0.41149   0.45650  ...    0.50594    0.37331   \n",
      "333901   0.59407   0.61595   0.40386   0.65190  ...    0.49023    0.60424   \n",
      "333902   0.49465   0.71118   0.48220   0.53299  ...    0.41879    0.64955   \n",
      "333903   0.39532   0.30184   0.20910   0.33041  ...    0.59970    0.59045   \n",
      "333904   0.37545   0.50035   0.64585   0.55769  ...    0.59497    0.38776   \n",
      "333905   0.42187   0.47466   0.37525   0.58125  ...    0.52464    0.57618   \n",
      "333906   0.35536   0.48186   0.49876   0.36463  ...    0.58487    0.44765   \n",
      "333907   0.32089   0.69535   0.53294   0.43689  ...    0.40198    0.54004   \n",
      "333908   0.43413   0.64938   0.66657   0.62922  ...    0.64106    0.46986   \n",
      "333909   0.40323   0.49161   0.38526   0.35060  ...    0.46227    0.52369   \n",
      "333910   0.55636   0.50846   0.47913   0.55327  ...    0.41672    0.52240   \n",
      "333911   0.45420   0.58696   0.25287   0.48149  ...    0.44196    0.68500   \n",
      "333912   0.46719   0.55669   0.59600   0.53443  ...    0.54857    0.59703   \n",
      "333913   0.50223   0.51333   0.59573   0.19037  ...    0.38403    0.52610   \n",
      "333914   0.29505   0.64040   0.65133   0.82701  ...    0.68128    0.66397   \n",
      "333915   0.62979   0.49950   0.31329   0.29323  ...    0.47624    0.54389   \n",
      "333916   0.41597   0.58451   0.46737   0.42167  ...    0.36073    0.65194   \n",
      "333917   0.22398   0.53908   0.69242   0.73459  ...    0.69416    0.50256   \n",
      "333918   0.40502   0.52326   0.55660   0.46356  ...    0.54289    0.53682   \n",
      "333919   0.29016   0.54173   0.49867   0.54847  ...    0.48866    0.35555   \n",
      "333920   0.39936   0.27492   0.42211   0.20965  ...    0.56749    0.54948   \n",
      "333921   0.42636   0.55033   0.51607   0.55940  ...    0.47780    0.53887   \n",
      "333922   0.56995   0.54684   0.36167   0.45931  ...    0.67469    0.58980   \n",
      "333923   0.41839   0.56140   0.59366   0.51704  ...    0.62566    0.45478   \n",
      "\n",
      "        feature50  target_bernie  target_elizabeth  target_jordan  target_ken  \\\n",
      "0         0.52182            1.0               1.0            1.0         1.0   \n",
      "1         0.55010            1.0               1.0            1.0         1.0   \n",
      "2         0.39061            0.0               0.0            1.0         0.0   \n",
      "3         0.36835            0.0               0.0            0.0         0.0   \n",
      "4         0.60452            1.0               1.0            1.0         1.0   \n",
      "5         0.35691            0.0               0.0            0.0         0.0   \n",
      "6         0.39767            1.0               1.0            1.0         1.0   \n",
      "7         0.49104            0.0               0.0            0.0         0.0   \n",
      "8         0.57269            0.0               0.0            0.0         0.0   \n",
      "9         0.50082            1.0               1.0            1.0         1.0   \n",
      "10        0.52720            0.0               0.0            0.0         0.0   \n",
      "11        0.45951            0.0               0.0            0.0         0.0   \n",
      "12        0.53359            1.0               1.0            1.0         1.0   \n",
      "13        0.72972            0.0               0.0            0.0         0.0   \n",
      "14        0.59524            1.0               1.0            1.0         1.0   \n",
      "15        0.59955            0.0               0.0            0.0         0.0   \n",
      "16        0.62569            0.0               0.0            0.0         0.0   \n",
      "17        0.44203            1.0               1.0            1.0         1.0   \n",
      "18        0.39731            1.0               1.0            1.0         1.0   \n",
      "19        0.38751            0.0               0.0            0.0         0.0   \n",
      "20        0.61816            1.0               1.0            1.0         1.0   \n",
      "21        0.53372            0.0               1.0            0.0         0.0   \n",
      "22        0.40123            0.0               0.0            0.0         0.0   \n",
      "23        0.53402            0.0               0.0            0.0         0.0   \n",
      "24        0.56262            0.0               0.0            0.0         0.0   \n",
      "25        0.63495            0.0               0.0            0.0         0.0   \n",
      "26        0.44513            0.0               0.0            0.0         0.0   \n",
      "27        0.48806            0.0               0.0            0.0         1.0   \n",
      "28        0.49451            0.0               0.0            0.0         0.0   \n",
      "29        0.49058            1.0               1.0            1.0         1.0   \n",
      "...           ...            ...               ...            ...         ...   \n",
      "333894    0.60923            NaN               NaN            NaN         NaN   \n",
      "333895    0.44433            NaN               NaN            NaN         NaN   \n",
      "333896    0.48360            NaN               NaN            NaN         NaN   \n",
      "333897    0.35499            NaN               NaN            NaN         NaN   \n",
      "333898    0.58263            NaN               NaN            NaN         NaN   \n",
      "333899    0.59974            NaN               NaN            NaN         NaN   \n",
      "333900    0.53860            NaN               NaN            NaN         NaN   \n",
      "333901    0.54947            NaN               NaN            NaN         NaN   \n",
      "333902    0.39121            NaN               NaN            NaN         NaN   \n",
      "333903    0.55970            NaN               NaN            NaN         NaN   \n",
      "333904    0.58773            NaN               NaN            NaN         NaN   \n",
      "333905    0.60564            NaN               NaN            NaN         NaN   \n",
      "333906    0.34334            NaN               NaN            NaN         NaN   \n",
      "333907    0.43746            NaN               NaN            NaN         NaN   \n",
      "333908    0.42161            NaN               NaN            NaN         NaN   \n",
      "333909    0.61633            NaN               NaN            NaN         NaN   \n",
      "333910    0.62184            NaN               NaN            NaN         NaN   \n",
      "333911    0.49960            NaN               NaN            NaN         NaN   \n",
      "333912    0.58098            NaN               NaN            NaN         NaN   \n",
      "333913    0.47645            NaN               NaN            NaN         NaN   \n",
      "333914    0.49876            NaN               NaN            NaN         NaN   \n",
      "333915    0.30022            NaN               NaN            NaN         NaN   \n",
      "333916    0.60008            NaN               NaN            NaN         NaN   \n",
      "333917    0.49471            NaN               NaN            NaN         NaN   \n",
      "333918    0.32950            NaN               NaN            NaN         NaN   \n",
      "333919    0.50956            NaN               NaN            NaN         NaN   \n",
      "333920    0.40728            NaN               NaN            NaN         NaN   \n",
      "333921    0.42454            NaN               NaN            NaN         NaN   \n",
      "333922    0.41426            NaN               NaN            NaN         NaN   \n",
      "333923    0.37560            NaN               NaN            NaN         NaN   \n",
      "\n",
      "        target_charles  target_frank  target_hillary  \n",
      "0                  1.0           1.0             1.0  \n",
      "1                  1.0           1.0             1.0  \n",
      "2                  0.0           0.0             0.0  \n",
      "3                  0.0           0.0             0.0  \n",
      "4                  1.0           1.0             1.0  \n",
      "5                  0.0           0.0             0.0  \n",
      "6                  1.0           1.0             1.0  \n",
      "7                  0.0           0.0             0.0  \n",
      "8                  0.0           0.0             0.0  \n",
      "9                  1.0           1.0             1.0  \n",
      "10                 0.0           0.0             0.0  \n",
      "11                 0.0           0.0             0.0  \n",
      "12                 1.0           1.0             1.0  \n",
      "13                 0.0           0.0             0.0  \n",
      "14                 1.0           1.0             1.0  \n",
      "15                 1.0           1.0             0.0  \n",
      "16                 0.0           0.0             0.0  \n",
      "17                 1.0           1.0             1.0  \n",
      "18                 1.0           1.0             1.0  \n",
      "19                 0.0           0.0             1.0  \n",
      "20                 1.0           1.0             1.0  \n",
      "21                 0.0           1.0             1.0  \n",
      "22                 0.0           0.0             0.0  \n",
      "23                 0.0           0.0             0.0  \n",
      "24                 0.0           0.0             0.0  \n",
      "25                 0.0           0.0             0.0  \n",
      "26                 0.0           0.0             0.0  \n",
      "27                 0.0           0.0             1.0  \n",
      "28                 0.0           0.0             0.0  \n",
      "29                 1.0           1.0             1.0  \n",
      "...                ...           ...             ...  \n",
      "333894             NaN           NaN             NaN  \n",
      "333895             NaN           NaN             NaN  \n",
      "333896             NaN           NaN             NaN  \n",
      "333897             NaN           NaN             NaN  \n",
      "333898             NaN           NaN             NaN  \n",
      "333899             NaN           NaN             NaN  \n",
      "333900             NaN           NaN             NaN  \n",
      "333901             NaN           NaN             NaN  \n",
      "333902             NaN           NaN             NaN  \n",
      "333903             NaN           NaN             NaN  \n",
      "333904             NaN           NaN             NaN  \n",
      "333905             NaN           NaN             NaN  \n",
      "333906             NaN           NaN             NaN  \n",
      "333907             NaN           NaN             NaN  \n",
      "333908             NaN           NaN             NaN  \n",
      "333909             NaN           NaN             NaN  \n",
      "333910             NaN           NaN             NaN  \n",
      "333911             NaN           NaN             NaN  \n",
      "333912             NaN           NaN             NaN  \n",
      "333913             NaN           NaN             NaN  \n",
      "333914             NaN           NaN             NaN  \n",
      "333915             NaN           NaN             NaN  \n",
      "333916             NaN           NaN             NaN  \n",
      "333917             NaN           NaN             NaN  \n",
      "333918             NaN           NaN             NaN  \n",
      "333919             NaN           NaN             NaN  \n",
      "333920             NaN           NaN             NaN  \n",
      "333921             NaN           NaN             NaN  \n",
      "333922             NaN           NaN             NaN  \n",
      "333923             NaN           NaN             NaN  \n",
      "\n",
      "[836656 rows x 60 columns]\n",
      "['feature1', 'feature2', 'feature3', 'feature4', 'feature5', 'feature6', 'feature7', 'feature8', 'feature9', 'feature10', 'feature11', 'feature12', 'feature13', 'feature14', 'feature15', 'feature16', 'feature17', 'feature18', 'feature19', 'feature20', 'feature21', 'feature22', 'feature23', 'feature24', 'feature25', 'feature26', 'feature27', 'feature28', 'feature29', 'feature30', 'feature31', 'feature32', 'feature33', 'feature34', 'feature35', 'feature36', 'feature37', 'feature38', 'feature39', 'feature40', 'feature41', 'feature42', 'feature43', 'feature44', 'feature45', 'feature46', 'feature47', 'feature48', 'feature49', 'feature50']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_201 (Dense)               (None, 5)            255         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_209 (Dense)               (None, 5)            255         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_202 (Dense)               (None, 50)           300         dense_201[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_205 (Dense)               (None, 5)            255         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_210 (Dense)               (None, 5)            30          dense_209[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 50)           0           dense_202[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_206 (Dense)               (None, 5)            30          dense_205[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 5)            0           dense_210[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_203 (Dense)               (None, 5)            255         dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_207 (Dense)               (None, 5)            30          dense_206[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_211 (Dense)               (None, 5)            30          dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_204 (Dense)               (None, 5)            30          dense_203[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_208 (Dense)               (None, 5)            30          dense_207[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_212 (Dense)               (None, 5)            30          dense_211[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 15)           0           dense_204[0][0]                  \n",
      "                                                                 dense_208[0][0]                  \n",
      "                                                                 dense_212[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_224 (Dense)               (None, 5)            80          concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_229 (Dense)               (None, 20)           1020        input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_225 (Dense)               (None, 2)            12          dense_224[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_230 (Dense)               (None, 20)           420         dense_229[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_226 (Dense)               (None, 20)           60          dense_225[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_231 (Dense)               (None, 20)           420         dense_230[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_227 (Dense)               (None, 20)           420         dense_226[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_232 (Dense)               (None, 10)           210         dense_231[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 50)           0           dense_227[0][0]                  \n",
      "                                                                 dense_230[0][0]                  \n",
      "                                                                 dense_232[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_233 (Dense)               (None, 10)           510         concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_234 (Dense)               (None, 10)           110         dense_233[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_235 (Dense)               (None, 2)            22          dense_234[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_236 (Dense)               (None, 5)            15          dense_235[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_237 (Dense)               (None, 5)            30          dense_236[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_238 (Dense)               (None, 5)            30          dense_237[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 10)           0           dense_237[0][0]                  \n",
      "                                                                 dense_238[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_239 (Dense)               (None, 50)           550         concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_240 (Dense)               (None, 2)            102         dense_239[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 5,541\n",
      "Trainable params: 5,541\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 336830 samples, validate on 165902 samples\n",
      "Epoch 1/30\n",
      "336830/336830 [==============================] - 12s 37us/step - loss: 0.6932 - val_loss: 0.6928 \n",
      "Epoch 2/30\n",
      "336830/336830 [==============================] - 6s 18us/step - loss: 0.6926 - val_loss: 0.6932\n",
      "Epoch 3/30\n",
      "336830/336830 [==============================] - 6s 17us/step - loss: 0.6926 - val_loss: 0.6932\n",
      "Epoch 4/30\n",
      "336830/336830 [==============================] - 6s 17us/step - loss: 0.6924 - val_loss: 0.6939\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336830/336830 [==============================] - 6s 18us/step - loss: 0.6924 - val_loss: 0.6935\n",
      "Epoch 6/30\n",
      "336830/336830 [==============================] - 6s 18us/step - loss: 0.6923 - val_loss: 0.6935\n",
      "Epoch 7/30\n",
      "336830/336830 [==============================] - 6s 18us/step - loss: 0.6923 - val_loss: 0.6929\n",
      "Epoch 8/30\n",
      "336830/336830 [==============================] - 6s 17us/step - loss: 0.6923 - val_loss: 0.6932\n",
      "Epoch 9/30\n",
      "336830/336830 [==============================] - 6s 18us/step - loss: 0.6923 - val_loss: 0.6930\n",
      "Epoch 10/30\n",
      "336830/336830 [==============================] - 6s 17us/step - loss: 0.6922 - val_loss: 0.6930\n",
      "Epoch 11/30\n",
      "336830/336830 [==============================] - 6s 18us/step - loss: 0.6922 - val_loss: 0.6929\n",
      "Epoch 12/30\n",
      "336830/336830 [==============================] - 6s 18us/step - loss: 0.6922 - val_loss: 0.6938\n",
      "Epoch 13/30\n",
      "336830/336830 [==============================] - 6s 18us/step - loss: 0.6921 - val_loss: 0.6931\n",
      "Epoch 14/30\n",
      "336830/336830 [==============================] - 6s 17us/step - loss: 0.6921 - val_loss: 0.6940\n",
      "Epoch 15/30\n",
      "336830/336830 [==============================] - 6s 17us/step - loss: 0.6920 - val_loss: 0.6942\n",
      "Epoch 16/30\n",
      "336830/336830 [==============================] - 6s 18us/step - loss: 0.6921 - val_loss: 0.6932\n",
      "Epoch 17/30\n",
      "336830/336830 [==============================] - 6s 18us/step - loss: 0.6921 - val_loss: 0.6933\n",
      "Epoch 18/30\n",
      "336830/336830 [==============================] - 6s 17us/step - loss: 0.6921 - val_loss: 0.6930\n",
      "Epoch 19/30\n",
      "336830/336830 [==============================] - 6s 17us/step - loss: 0.6920 - val_loss: 0.6931\n",
      "Epoch 20/30\n",
      "336830/336830 [==============================] - 6s 17us/step - loss: 0.6920 - val_loss: 0.6933\n",
      "Epoch 21/30\n",
      "336830/336830 [==============================] - 6s 17us/step - loss: 0.6920 - val_loss: 0.6931\n",
      "Epoch 22/30\n",
      "336830/336830 [==============================] - 6s 17us/step - loss: 0.6920 - val_loss: 0.6929\n",
      "Epoch 23/30\n",
      "336830/336830 [==============================] - 6s 18us/step - loss: 0.6919 - val_loss: 0.6934\n",
      "Epoch 24/30\n",
      "336830/336830 [==============================] - 6s 18us/step - loss: 0.6920 - val_loss: 0.6944\n",
      "Epoch 25/30\n",
      "336830/336830 [==============================] - 6s 18us/step - loss: 0.6920 - val_loss: 0.6936\n",
      "Epoch 26/30\n",
      "336830/336830 [==============================] - 6s 18us/step - loss: 0.6920 - val_loss: 0.6936\n",
      "Epoch 27/30\n",
      "336830/336830 [==============================] - 6s 18us/step - loss: 0.6920 - val_loss: 0.6930\n",
      "Epoch 28/30\n",
      "336830/336830 [==============================] - 6s 18us/step - loss: 0.6919 - val_loss: 0.6934\n",
      "Epoch 29/30\n",
      "336830/336830 [==============================] - 6s 18us/step - loss: 0.6919 - val_loss: 0.6933\n",
      "Epoch 30/30\n",
      "336830/336830 [==============================] - 6s 17us/step - loss: 0.6919 - val_loss: 0.6932\n",
      "333924/333924 [==============================] - 18s 55us/step\n",
      "era121: loss - 0.6922194180347387 consistent: True\n",
      "era122: loss - 0.6922194180347387 consistent: True\n",
      "era123: loss - 0.6922194180347387 consistent: True\n",
      "era124: loss - 0.6922194180347387 consistent: True\n",
      "era125: loss - 0.6922194180347387 consistent: True\n",
      "era126: loss - 0.6922194180347387 consistent: True\n",
      "era127: loss - 0.6922194180347387 consistent: True\n",
      "era128: loss - 0.6922194180347387 consistent: True\n",
      "era129: loss - 0.6922194180347387 consistent: True\n",
      "era130: loss - 0.6922194180347387 consistent: True\n",
      "era131: loss - 0.6922194180347387 consistent: True\n",
      "era132: loss - 0.6922194180347387 consistent: True\n",
      "era133: loss - 0.6922194180347387 consistent: True\n",
      "era134: loss - 0.6922194180347387 consistent: True\n",
      "era135: loss - 0.6922194180347387 consistent: True\n",
      "era136: loss - 0.6922194180347387 consistent: True\n",
      "era137: loss - 0.6922194180347387 consistent: True\n",
      "era138: loss - 0.6922194180347387 consistent: True\n",
      "era139: loss - 0.6922194180347387 consistent: True\n",
      "era140: loss - 0.6922194180347387 consistent: True\n",
      "era141: loss - 0.6922194180347387 consistent: True\n",
      "era142: loss - 0.6922194180347387 consistent: True\n",
      "era143: loss - 0.6922194180347387 consistent: True\n",
      "era144: loss - 0.6922194180347387 consistent: True\n",
      "era145: loss - 0.6922194180347387 consistent: True\n",
      "era146: loss - 0.6922194180347387 consistent: True\n",
      "era147: loss - 0.6922194180347387 consistent: True\n",
      "era148: loss - 0.6922194180347387 consistent: True\n",
      "era149: loss - 0.6922194180347387 consistent: True\n",
      "era150: loss - 0.6922194180347387 consistent: True\n",
      "era151: loss - 0.6922194180347387 consistent: True\n",
      "era152: loss - 0.6922194180347387 consistent: True\n",
      "era153: loss - 0.6922194180347387 consistent: True\n",
      "era154: loss - 0.6922194180347387 consistent: True\n",
      "era155: loss - 0.6922194180347387 consistent: True\n",
      "era156: loss - 0.6922194180347387 consistent: True\n",
      "era157: loss - 0.6922194180347387 consistent: True\n",
      "era158: loss - 0.6922194180347387 consistent: True\n",
      "era159: loss - 0.6922194180347387 consistent: True\n",
      "era160: loss - 0.6922194180347387 consistent: True\n",
      "era161: loss - 0.6922194180347387 consistent: True\n",
      "era162: loss - 0.6922194180347387 consistent: True\n",
      "era163: loss - 0.6922194180347387 consistent: True\n",
      "era164: loss - 0.6922194180347387 consistent: True\n",
      "era165: loss - 0.6922194180347387 consistent: True\n",
      "era166: loss - 0.6922194180347387 consistent: True\n",
      "era167: loss - 0.6922194180347387 consistent: True\n",
      "era168: loss - 0.6922194180347387 consistent: True\n",
      "era169: loss - 0.6922194180347387 consistent: True\n",
      "era170: loss - 0.6922194180347387 consistent: True\n",
      "era171: loss - 0.6922194180347387 consistent: True\n",
      "era172: loss - 0.6922194180347387 consistent: True\n",
      "era173: loss - 0.6922194180347387 consistent: True\n",
      "era174: loss - 0.6922194180347387 consistent: True\n",
      "era175: loss - 0.6922194180347387 consistent: True\n",
      "era176: loss - 0.6922194180347387 consistent: True\n",
      "era177: loss - 0.6922194180347387 consistent: True\n",
      "era178: loss - 0.6922194180347387 consistent: True\n",
      "era179: loss - 0.6922194180347387 consistent: True\n",
      "era180: loss - 0.6922194180347387 consistent: True\n",
      "era181: loss - 0.6922194180347387 consistent: True\n",
      "era182: loss - 0.6922194180347387 consistent: True\n",
      "era183: loss - 0.6922194180347387 consistent: True\n",
      "era184: loss - 0.6922194180347387 consistent: True\n",
      "era185: loss - 0.6922194180347387 consistent: True\n",
      "era186: loss - 0.6922194180347387 consistent: True\n",
      "eraX: loss - 0.6922194180347387 consistent: True\n",
      "Consistency: 1.0\n",
      "- probabilities: [0.50687575 0.46936944 0.51821333 0.49461275 0.48853993]\n",
      "- target: 1    1.0\n",
      "2    0.0\n",
      "3    0.0\n",
      "4    0.0\n",
      "5    1.0\n",
      "Name: target_bernie, dtype: float64\n",
      "- rounded probability: [1.0, 0.0, 1.0, 0.0, 0.0]\n",
      "- accuracy:  0.08549550197050826\n",
      "- bernie vs elizabeth corr: [[nan nan]\n",
      " [nan nan]]\n",
      "- elizabeth using bernie: 0.08605251494352008\n",
      "333924/333924 [==============================] - 18s 55us/step\n",
      "- validation logloss: 0.692\n",
      "Writing predictions to predictions.csv\n",
      "\n",
      "Writing predictions to predictions_2019-04-21_16h22m26s_Shared_Input_MultiMerge_Multioutput_Bernie_1.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND' ] = 'tensorflow'\n",
    "os.environ['MKL_THREADING_LAYER'] = 'GNU'\n",
    "import keras as ks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import TensorBoard\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import Dropout, Activation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time \n",
    "\n",
    "from keras.layers.merge import concatenate\n",
    "def main():\n",
    "\n",
    "    # Set seed for reproducibility\n",
    "    NAME = \"Shared_Input_MultiMerge_Multioutput_Bernie\"\n",
    "    np.random.seed(0)\n",
    "\n",
    "    print(\"Loading data...\")\n",
    "    # Load the data from the CSV files\n",
    "    training_data = pd.read_csv('numerai_training_data.csv', header=0)\n",
    "    print('original train data shape: {},\\t{} \\n\\n \\t:'.format(training_data.shape[0],training_data.shape[1]))\n",
    "\n",
    "    prediction_data = pd.read_csv('numerai_tournament_data.csv', header=0)\n",
    "    print('original prediction data shape: {},\\t{} \\n\\n \\t:'.format(prediction_data.shape[0],prediction_data.shape[1]))\n",
    "    \n",
    "    complete_training_data = pd.concat([training_data, prediction_data])\n",
    "    print('total training / valdation shape {}'.format(complete_training_data))\n",
    "    \n",
    "    # Transform the loaded CSV data into numpy arrays\n",
    "\n",
    "    features = [f for f in list(training_data) if \"feature\" in f]\n",
    "    print(features)\n",
    "\n",
    "    X = training_data[features]\n",
    "    mini= MinMaxScaler(feature_range=(0,1)) \n",
    "    X = mini.fit_transform(X)\n",
    "\n",
    "    Y = training_data[\"target_bernie\"]\n",
    "    Y= keras.utils.to_categorical(Y,2) \n",
    "\n",
    "    x_prediction = prediction_data[features]\n",
    "    x_prediction = mini.fit_transform(x_prediction)\n",
    "\n",
    "    ids = prediction_data[\"id\"]  \n",
    "\n",
    "\n",
    "    batch_size = 710\n",
    "\n",
    "    dropout = 0.47\n",
    "    visible = Input(shape=(50,))\n",
    "    m1 = Dense(5, activation='tanh')(visible)\n",
    "    m1 = Dense(50, activation='tanh')(m1)\n",
    "    m1 = Dropout(dropout)(m1) \n",
    "    m1 = Dense(5, activation='tanh')(m1)\n",
    "    m1 = Dense(5, activation='tanh')(m1)\n",
    "    \n",
    "    m2 = Dense(5, activation='tanh')(visible)\n",
    "    m2 = Dense(5, activation='tanh')(m2)\n",
    "    m2 = Dense(5, activation='tanh')(m2)\n",
    "    m2 = Dense(5, activation='tanh')(m2)\n",
    "    \n",
    "    m3 = Dense(5, activation='tanh')(visible)\n",
    "    m3 = Dense(5, activation='tanh')(m3)\n",
    "    m3 = Dropout(dropout)(m3)\n",
    "    m3 = Dense(5, activation='tanh')(m3)\n",
    "    m3 = Dense(5, activation='tanh')(m3)\n",
    "    \n",
    "    merge1 = concatenate([m1,m2,m3],axis=1)\n",
    "    m4 = Dense(10, activation='tanh')(merge1)\n",
    "    m4 = Dense(10, activation='tanh')(m4)\n",
    "    m4 = Dense(10, activation='tanh')(m4)\n",
    "    m4 = Dense(10, activation='tanh')(m4)\n",
    "    output1 = Dense(2, activation='sigmoid')(m4)\n",
    "    \n",
    "    #second shared input multi merge/output\n",
    "    m5 = Dense(10, activation='tanh')(output1)\n",
    "    m5 = Dense(10, activation='tanh')(m5)\n",
    "    m5 = Dense(10, activation='tanh')(m5)\n",
    "    m6 = Dense(10, activation='tanh')(m5)\n",
    "    m6 = Dense(10,activation='tanh')(m6)\n",
    "    m6 = Dense(10,activation='tanh')(m6)\n",
    "    merge2 = concatenate([m4,m5,m6],axis=1)\n",
    "    \n",
    "    m7 = Dense(5, activation='tanh')(merge1)\n",
    "    output = Dense(2, activation='sigmoid')(m7)\n",
    "    \n",
    "    sig1 = Dense(20, activation='tanh')(output)\n",
    "    sig1 = Dense(20, activation='tanh')(sig1)\n",
    "    sig2 = Dense(20, activation='tanh')(sig1)\n",
    "    \n",
    "    sig2 = Dense(20, activation='tanh')(visible)\n",
    "    sig2 = Dense(20, activation='tanh')(sig2)\n",
    "    sig3 = Dense(20, activation='tanh')(sig2)\n",
    "    sig3 = Dense(10, activation='tanh')(sig3)\n",
    "    sig_merge1 = concatenate([sig1,sig2,sig3])\n",
    "    \n",
    "    sig4 = Dense(10, activation='tanh')(sig_merge1)\n",
    "    sig4 = Dense(10, activation='tanh')(sig4)\n",
    "    sig_output1 = Dense(2, activation='sigmoid')(sig4)\n",
    "    \n",
    "    #second shared input multi sig_merge/sig_output\n",
    "    sig5 = Dense(5, activation='tanh')(sig_output1)\n",
    "    sig5 = Dense(5, activation='tanh')(sig5)\n",
    "    sig6 = Dense(5, activation='tanh')(sig5)\n",
    "    sig_merge2= concatenate([sig5,sig6])\n",
    "    \n",
    "    sig7 = Dense(50, activation='tanh')(sig_merge2)\n",
    "    sig_output = Dense(2, activation='sigmoid')(sig7)\n",
    "\n",
    "    model = Model(inputs=visible, outputs=sig_output)\n",
    "    model.compile(loss='binary_crossentropy',optimizer='rmsprop')\n",
    "    model.summary()\n",
    "    \n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "    \n",
    "    model.fit(X,Y,batch_size=batch_size,epochs=30,validation_split=0.33,callbacks=[tensorboard])\n",
    "    y_prediction = model.predict(x_prediction)\n",
    "    evaluate = model.evaluate(x_prediction,y_prediction)\n",
    "    \n",
    "    eras = prediction_data.era.unique()\n",
    "    count = 0\n",
    "    count_consistent = 0\n",
    "        \n",
    "    for era in eras:\n",
    "        count += 1\n",
    "        current_valid_data = prediction_data[prediction_data.era==era]\n",
    "        features = [f for f in list(complete_training_data) if \"feature\" in f]\n",
    "        X_valid = current_valid_data[features]\n",
    "        Y_valid = current_valid_data[\"target_bernie\"]\n",
    "        loss = evaluate\n",
    "        if (loss < -np.log(.5)):\n",
    "            consistent = True\n",
    "            count_consistent += 1\n",
    "        else:\n",
    "            consistent = False\n",
    "        print(\"{}: loss - {} consistent: {}\".format(era, loss, consistent))\n",
    "    print (\"Consistency: {}\".format(count_consistent/count))\n",
    "        \n",
    "    \n",
    "    probabilities = y_prediction[:, 1]\n",
    "    print(\"- probabilities:\", probabilities[1:6])\n",
    "\n",
    "    # We can see the probability does seem to be good at predicting the\n",
    "    # true target correctly.\n",
    "    print(\"- target:\", prediction_data['target_bernie'][1:6])\n",
    "    print(\"- rounded probability:\", [np.round(p) for p in probabilities][1:6])\n",
    "\n",
    "    # But overall the accuracy is very low.\n",
    "    correct = [\n",
    "        np.round(x) == y\n",
    "        for (x, y) in zip(probabilities, prediction_data['target_bernie'])\n",
    "    ]\n",
    "    print(\"- accuracy: \", np.sum(correct) / np.float(prediction_data.shape[0]))\n",
    "\n",
    "    tournament_corr = np.corrcoef(prediction_data['target_bernie'],\n",
    "                                  prediction_data['target_elizabeth'])\n",
    "    print(\"- bernie vs elizabeth corr:\", tournament_corr)\n",
    "    # You can see that target_elizabeth is accurate using the bernie model as well.\n",
    "    correct = [\n",
    "        np.round(x) == y\n",
    "        for (x, y) in zip(probabilities, prediction_data['target_elizabeth'])\n",
    "    ]\n",
    "    print(\"- elizabeth using bernie:\",\n",
    "          np.sum(correct) / np.float(prediction_data.shape[0]))\n",
    "\n",
    "    # Numerai measures models on logloss instead of accuracy. The lower the logloss the better.\n",
    "    # Numerai only pays models with logloss < 0.693 on the live portion of the tournament data.)\n",
    "    evalu = model.evaluate(x_prediction,y_prediction)\n",
    "    print(\"- validation logloss: {:.3f}\".format(evalu))\n",
    "    \n",
    "    results = y_prediction[:, 1]\n",
    "    results_df = pd.DataFrame(data={'target':results})\n",
    "\n",
    "    joined = pd.DataFrame(ids).join(results_df)\n",
    "    pd.DataFrame(joined[:5])\n",
    "\n",
    "\n",
    "    print(\"Writing predictions to predictions.csv\")\n",
    "    path = 'predictions_{:}_{}_1'.format(time.strftime(\"%Y-%m-%d_%Hh%Mm%Ss\", time.gmtime()),NAME) + '.csv'\n",
    "    print()\n",
    "    print(\"Writing predictions to \" + path.strip())\n",
    "    joined.to_csv(path,float_format='%.15f', index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    sig1 = Dense(10, activation='relu')(output)\n",
    "    sig2 = Dense(20, activation='relu')(sig1)\n",
    "    sig3 = Dense(10, activation='relu')(sig2)\n",
    "    \n",
    "    sig4 = Dense(10, activation='relu')(visible)\n",
    "    sig5 = Dense(20, activation='relu')(sig4)\n",
    "    sig6 = Dense(10, activation='relu')(sig5)\n",
    "    sig_merge1 = concatenate([sig3,sig6])\n",
    "    \n",
    "    sig7 = Dense(10, activation='relu')(sig_merge1)\n",
    "    sig8 = Dense(10, activation='relu')(sig7)\n",
    "    sig_output1 = Dense(2, activation='sigmoid')(sig8)\n",
    "    \n",
    "    #second shared input multi sig_merge/sig_output\n",
    "    sig9 = Dense(10, activation='relu')(sig_output1)\n",
    "    sig10 = Dense(10, activation='relu')(sig9)\n",
    "    sig11 = Dense(10, activation='relu')(sig10)\n",
    "    sig_merge2= concatenate([sig7,sig11])\n",
    "    \n",
    "    sig12 = Dense(102, activation='relu')(sig_merge2)\n",
    "    sig_output = Dense(2, activation='sigmoid')(sig12)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
