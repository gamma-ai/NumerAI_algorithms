{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T15:53:05.647456Z",
     "start_time": "2019-04-21T15:52:56.978542Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "original train data shape: 250,\t302 \n",
      "\n",
      " \t:\n",
      "original prediction data shape: 19750,\t301 \n",
      "\n",
      " \t:\n",
      "total training / valdation shape            0      1     10    100    101    102    103    104    105    106  \\\n",
      "0     -0.098  2.165 -0.912  1.329  0.452 -0.704  2.218 -1.844  0.158 -1.649   \n",
      "1      1.081 -0.973  2.907 -1.208  0.893  0.379  1.396  0.581 -0.475 -0.056   \n",
      "2     -0.523 -0.089  0.459 -0.901  2.895  0.651  1.006 -0.587  0.208 -0.106   \n",
      "3      0.067 -0.021  0.335 -0.546 -1.125 -0.418  0.281 -0.193  0.764  1.282   \n",
      "4      2.347 -0.831  0.190  0.680  1.515 -0.617 -0.918 -0.243  0.689 -0.465   \n",
      "5     -0.641 -0.576 -1.369 -1.019  1.358 -0.310 -1.891 -0.700 -2.351 -0.785   \n",
      "6     -0.490  0.557  1.168  1.314 -0.207  1.033 -0.461  0.821  0.673  0.056   \n",
      "7      1.252 -1.370  0.626  0.306  0.117  0.174  0.742 -0.556  0.144  1.034   \n",
      "8      1.410 -1.097 -1.256  1.250  1.508  0.277 -0.364 -1.202 -0.088 -0.512   \n",
      "9     -1.811  0.566  2.173  0.060  1.888  0.755  2.024 -0.541 -0.026  1.378   \n",
      "10     0.184  0.661  0.701 -0.790 -0.247  0.674  1.305  0.070 -0.546 -1.369   \n",
      "11    -1.845 -0.246  0.284 -1.043 -1.300 -0.947  1.008  1.267 -0.818 -0.483   \n",
      "12    -0.077  1.642  1.107 -0.966 -1.638 -2.808 -0.694  1.238 -0.912 -1.958   \n",
      "13    -0.378  0.326  0.497  0.467 -1.332  0.020  1.035  0.042 -0.384  1.940   \n",
      "14    -0.625  0.629  1.098 -1.170  1.067  0.041 -0.566 -0.212 -0.400 -0.061   \n",
      "15    -0.453 -0.164  0.400  0.183 -0.077  0.838 -0.079  0.692  0.116 -2.374   \n",
      "16     0.461  0.234  0.408 -0.479  0.833 -0.086 -0.340  0.195  0.123  1.124   \n",
      "17    -1.176 -0.359  1.468  0.217  0.142 -0.430  0.723  0.743 -1.120 -0.217   \n",
      "18    -0.265  0.749 -0.185 -0.691  1.052 -0.361  1.429  0.025 -1.326 -0.525   \n",
      "19     0.833 -0.596  0.085 -0.456 -1.023 -0.187  0.131  1.344  0.183 -0.871   \n",
      "20    -1.011  0.652  0.010 -0.521 -0.901  0.564  3.581 -0.344 -0.368  0.404   \n",
      "21     1.021  1.624  1.707  0.239 -0.495  0.806  0.308 -1.276 -0.557 -0.066   \n",
      "22     1.166  0.129  1.100  0.141 -1.933  0.898  1.058  1.219 -0.741  0.308   \n",
      "23     0.635 -0.468 -0.577  0.213  1.252 -0.234  1.692 -0.456  0.995 -2.369   \n",
      "24     0.128 -0.536 -1.381 -0.222  0.758 -0.277 -1.206  1.207 -0.133 -0.022   \n",
      "25     0.891  0.506  0.714  0.811  0.765 -1.251  0.267  0.840  1.782  1.608   \n",
      "26    -1.317 -1.837  0.996 -1.652  0.159  0.190  1.886  0.918 -0.660 -0.314   \n",
      "27    -1.222  0.389  0.903 -1.864 -0.642 -0.084 -1.011 -0.012 -0.661  1.447   \n",
      "28    -0.119 -1.269 -0.010 -0.422  1.066  0.060 -2.155 -0.093  1.972 -0.088   \n",
      "29    -1.430 -0.497 -0.258  0.055 -1.497 -0.944 -0.609 -0.612 -0.150 -0.732   \n",
      "...      ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
      "19720  0.414 -0.596 -0.179 -0.356 -0.489  0.523 -1.458 -0.307  2.007  0.279   \n",
      "19721 -2.124 -0.371  0.846  0.669  3.554 -1.566 -2.185  0.808 -1.152  1.526   \n",
      "19722 -0.363  0.914 -0.148  1.522  0.846 -0.691 -1.417  0.555 -1.137  0.053   \n",
      "19723 -0.180  0.615  0.358 -0.157  0.105 -1.397 -0.774 -0.364 -0.581  0.853   \n",
      "19724 -0.252  2.050 -0.126 -1.062  0.679  0.412  0.343  0.946 -1.397 -0.779   \n",
      "19725 -0.508 -0.953  1.262 -1.570 -0.260  0.378 -0.789  0.554 -0.013 -1.808   \n",
      "19726 -2.291 -0.480  0.717 -0.195  1.122 -0.737 -0.484  0.633  0.433 -0.234   \n",
      "19727  2.164 -1.744  0.356  0.120  0.193  0.045 -0.570 -0.485  0.055 -0.632   \n",
      "19728  0.556 -0.169 -0.632  0.717 -0.721  0.272 -0.315 -0.171 -0.874 -0.459   \n",
      "19729 -1.537 -0.349  0.963  2.394 -0.711 -1.205  0.283  1.169 -0.719 -1.391   \n",
      "19730 -1.726  0.623 -0.541  0.693 -2.056  0.251 -0.014  0.111  0.693  0.012   \n",
      "19731 -1.020  0.993 -0.729  1.251 -0.542  0.096  0.475 -0.420  1.710  0.315   \n",
      "19732  1.018  1.101  0.789 -1.616  1.311 -0.263  0.153  0.247 -0.076 -1.286   \n",
      "19733 -0.696  0.428 -0.196  0.301  3.118  1.048  0.612  0.773  1.015  0.428   \n",
      "19734 -0.197 -1.734 -1.233 -0.291 -0.209 -0.613 -1.271 -0.459  1.729 -1.142   \n",
      "19735 -0.222 -0.707  0.456 -2.512 -0.144 -0.262 -0.685  0.292  0.156 -0.053   \n",
      "19736 -0.621 -1.417  0.309 -1.515 -1.293  0.232 -0.746  0.781  0.086  0.115   \n",
      "19737  0.966 -0.576  1.692 -0.265  0.004  1.547 -0.539 -0.155  0.156 -0.154   \n",
      "19738 -0.050  0.597  0.833  0.703  0.374  1.305  0.184 -2.168  0.870  2.024   \n",
      "19739 -0.106 -0.087  0.711  0.440  2.229  0.856 -0.980  3.001  0.860  0.106   \n",
      "19740 -1.211 -0.957 -1.123  2.031  0.670 -0.607  0.813 -2.158 -0.274 -0.443   \n",
      "19741  0.347  0.493 -0.822 -0.502 -0.063  0.334 -0.330 -0.472 -1.789 -0.587   \n",
      "19742  0.130  1.880 -1.323 -1.451 -0.787 -0.790  0.224  0.781  0.627  1.247   \n",
      "19743 -1.375  0.549  1.093  0.105 -0.259  0.383  2.058 -1.295 -0.858 -0.147   \n",
      "19744  0.138 -0.954  0.911 -0.964 -0.109 -0.207 -2.556  1.492 -0.580  0.779   \n",
      "19745  1.069  0.517  2.191  1.438 -2.081  0.643 -0.351  0.942  0.245  0.037   \n",
      "19746 -0.529  0.438 -0.188 -0.045 -0.488 -0.679 -0.012 -0.008 -1.564  0.574   \n",
      "19747 -0.554 -0.936  1.524  0.566  0.756 -0.829 -1.204 -0.458  0.738 -2.080   \n",
      "19748 -0.746  1.205  0.014  0.309  1.109 -1.428  0.148  0.337 -0.719  0.121   \n",
      "19749  0.736 -0.216 -0.994 -1.045 -0.286 -0.472  1.740 -0.351 -0.003  0.024   \n",
      "\n",
      "       ...     92     93     94     95     96     97     98     99     id  \\\n",
      "0      ...  0.060  0.768  2.563  0.638  1.164  0.407 -1.556 -0.903      0   \n",
      "1      ... -0.006 -0.279  1.914  0.620 -1.495  1.787 -0.305  0.602      1   \n",
      "2      ... -1.476  0.545  0.636  0.857 -1.796  2.540  0.074 -0.768      2   \n",
      "3      ... -1.823  0.863 -0.447 -1.108 -1.151 -0.919 -2.284  2.001      3   \n",
      "4      ...  0.475  0.313  0.518  0.114  0.527  1.438  0.749 -2.087      4   \n",
      "5      ... -2.524 -0.574 -0.930 -0.574  0.403 -0.865 -2.098  1.909      5   \n",
      "6      ...  0.308 -0.797  0.593  0.322  1.669 -0.044  0.005  0.942      6   \n",
      "7      ... -0.260  0.373  0.778 -0.553  1.806 -0.187  1.591  0.417      7   \n",
      "8      ... -0.505 -0.537  1.214 -0.393 -0.092 -0.860  0.252  1.278      8   \n",
      "9      ... -0.292  0.812  0.013  0.118 -0.061  0.439 -0.889  0.559      9   \n",
      "10     ...  0.321 -0.110 -0.594 -0.883  0.139 -0.667  0.655 -1.350     10   \n",
      "11     ...  1.543  0.408  0.728  0.614  0.146  1.437  0.476  0.158     11   \n",
      "12     ... -1.369 -1.629  0.134  0.556 -2.986  0.657 -0.666  1.355     12   \n",
      "13     ... -0.180 -1.112 -1.185 -1.138  0.078 -0.095  0.865 -1.109     13   \n",
      "14     ...  0.643 -2.120 -1.942  0.703 -0.160  0.148 -0.720 -0.149     14   \n",
      "15     ...  2.313  1.044 -1.372  0.135 -0.238 -0.478  1.302  0.708     15   \n",
      "16     ...  1.010  0.189 -0.545 -0.426 -0.825  1.923 -0.949 -1.177     16   \n",
      "17     ... -0.101  0.319 -1.190 -0.946 -1.467 -1.253  1.339 -1.614     17   \n",
      "18     ...  0.279 -0.533 -1.329 -1.044 -0.606  1.346 -0.109 -1.180     18   \n",
      "19     ... -0.564 -1.307 -1.209  0.566 -0.023  0.068 -0.162  0.873     19   \n",
      "20     ... -0.192 -0.087 -0.013  0.296  1.465  0.103  0.847 -0.846     20   \n",
      "21     ...  0.673  2.623 -1.606 -0.635 -2.188  1.939 -0.811  0.265     21   \n",
      "22     ...  1.444  2.416 -1.112  0.392  1.137  1.013 -0.459  3.266     22   \n",
      "23     ...  0.968 -0.206 -0.447 -0.419  0.664  0.672 -0.095  1.723     23   \n",
      "24     ...  0.338 -0.325  1.080 -1.169  0.412  0.341  0.443  2.418     24   \n",
      "25     ... -1.215 -1.104 -1.397  1.237  0.459  1.856 -2.033  0.717     25   \n",
      "26     ... -0.624  0.010  0.855  0.556 -1.331 -1.606  1.060 -1.651     26   \n",
      "27     ... -1.103  0.586 -1.281  0.645  0.260 -0.227 -0.978  0.318     27   \n",
      "28     ... -1.471 -2.085 -0.951 -0.393 -0.120  0.142  1.960  0.485     28   \n",
      "29     ... -0.206  0.921  2.225 -0.327 -0.203 -2.203  0.533  0.171     29   \n",
      "...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
      "19720  ...  0.081  1.044 -0.885 -1.283  0.478  1.694  0.548 -0.316  19970   \n",
      "19721  ... -1.019 -0.278  2.097 -0.347 -0.457  2.237  0.255  1.000  19971   \n",
      "19722  ... -1.254 -0.401  1.431  0.164 -1.391  0.253 -1.496 -0.960  19972   \n",
      "19723  ... -1.268  0.361  0.164  0.264  0.195 -0.093 -0.137  2.017  19973   \n",
      "19724  ... -0.884  1.206  1.029  0.874 -0.505  0.684 -0.706 -1.398  19974   \n",
      "19725  ... -0.419  1.406  0.751  1.816  1.647  0.182  1.129 -0.690  19975   \n",
      "19726  ... -0.243 -0.063 -1.083  0.595  0.998  0.176 -0.390  0.729  19976   \n",
      "19727  ...  1.058 -1.262  0.783 -0.264 -1.205  1.773 -0.618  0.063  19977   \n",
      "19728  ... -0.190  0.909  0.281 -0.457 -1.541 -0.157  2.682  2.541  19978   \n",
      "19729  ... -0.432 -2.846  0.230  0.389 -0.825  1.539  0.715 -0.856  19979   \n",
      "19730  ... -1.206 -1.512 -1.092 -1.142  1.289  0.028 -0.771  1.949  19980   \n",
      "19731  ... -0.830 -0.468  1.231  1.285 -0.432  0.502 -0.510  0.634  19981   \n",
      "19732  ... -1.172  1.206  1.522 -0.659 -0.187  0.013 -2.432 -0.023  19982   \n",
      "19733  ...  0.526 -1.026 -1.348 -1.126 -0.737  0.581 -0.042  0.248  19983   \n",
      "19734  ...  0.711 -0.422  1.920 -0.765 -0.520 -1.263  0.792  0.089  19984   \n",
      "19735  ...  1.375 -0.090 -1.540 -0.276 -2.045 -0.292  1.720 -1.217  19985   \n",
      "19736  ...  0.671 -0.893  1.486 -0.611 -0.340 -0.870  0.271  0.546  19986   \n",
      "19737  ... -0.917  0.314  0.779 -0.720  1.469 -1.578  0.550 -0.492  19987   \n",
      "19738  ... -0.906  2.342 -0.129  1.727 -0.038 -0.268  0.503 -1.771  19988   \n",
      "19739  ... -1.055  0.595 -2.208  0.755  0.465 -0.409 -0.267  0.562  19989   \n",
      "19740  ... -1.607 -1.953 -1.001 -1.522  1.088  1.708 -1.800 -0.591  19990   \n",
      "19741  ...  0.882 -0.295 -0.647 -0.437 -1.941  1.454  0.410  2.713  19991   \n",
      "19742  ...  0.901 -0.342  1.789  0.171 -0.146 -2.216  1.534  0.966  19992   \n",
      "19743  ...  1.351  0.637  1.683  1.201  0.317 -1.009 -1.692  0.886  19993   \n",
      "19744  ... -2.077 -1.012 -0.296  0.430 -0.156  1.322  0.342  0.732  19994   \n",
      "19745  ...  0.108 -0.691  1.073 -3.217  0.447 -0.814  0.916  0.740  19995   \n",
      "19746  ...  1.783  0.645  2.084 -1.488 -1.414 -0.438  0.889 -0.557  19996   \n",
      "19747  ... -0.236  1.111  1.595 -0.860 -0.072 -2.241 -0.698 -0.537  19997   \n",
      "19748  ...  1.650  0.060 -2.203 -1.025 -0.009  0.027  0.774  1.234  19998   \n",
      "19749  ... -1.346  0.180 -0.838 -2.029  1.347 -1.213  1.063 -0.144  19999   \n",
      "\n",
      "       target  \n",
      "0         1.0  \n",
      "1         0.0  \n",
      "2         1.0  \n",
      "3         1.0  \n",
      "4         1.0  \n",
      "5         1.0  \n",
      "6         1.0  \n",
      "7         1.0  \n",
      "8         1.0  \n",
      "9         1.0  \n",
      "10        0.0  \n",
      "11        1.0  \n",
      "12        1.0  \n",
      "13        0.0  \n",
      "14        0.0  \n",
      "15        1.0  \n",
      "16        1.0  \n",
      "17        0.0  \n",
      "18        1.0  \n",
      "19        1.0  \n",
      "20        1.0  \n",
      "21        1.0  \n",
      "22        0.0  \n",
      "23        1.0  \n",
      "24        1.0  \n",
      "25        1.0  \n",
      "26        1.0  \n",
      "27        1.0  \n",
      "28        1.0  \n",
      "29        1.0  \n",
      "...       ...  \n",
      "19720     NaN  \n",
      "19721     NaN  \n",
      "19722     NaN  \n",
      "19723     NaN  \n",
      "19724     NaN  \n",
      "19725     NaN  \n",
      "19726     NaN  \n",
      "19727     NaN  \n",
      "19728     NaN  \n",
      "19729     NaN  \n",
      "19730     NaN  \n",
      "19731     NaN  \n",
      "19732     NaN  \n",
      "19733     NaN  \n",
      "19734     NaN  \n",
      "19735     NaN  \n",
      "19736     NaN  \n",
      "19737     NaN  \n",
      "19738     NaN  \n",
      "19739     NaN  \n",
      "19740     NaN  \n",
      "19741     NaN  \n",
      "19742     NaN  \n",
      "19743     NaN  \n",
      "19744     NaN  \n",
      "19745     NaN  \n",
      "19746     NaN  \n",
      "19747     NaN  \n",
      "19748     NaN  \n",
      "19749     NaN  \n",
      "\n",
      "[20000 rows x 302 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peace\\Anaconda3_2\\lib\\site-packages\\ipykernel_launcher.py:34: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "C:\\Users\\peace\\Anaconda3_2\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\peace\\Anaconda3_2\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 301)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_79 (Dense)                (None, 5)            1510        input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_80 (Dense)                (None, 5)            30          dense_79[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_81 (Dense)                (None, 5)            30          dense_80[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_82 (Dense)                (None, 5)            30          dense_81[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_83 (Dense)                (None, 5)            30          dense_82[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_84 (Dense)                (None, 5)            30          dense_83[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_85 (Dense)                (None, 5)            30          dense_84[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_86 (Dense)                (None, 5)            30          dense_85[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_87 (Dense)                (None, 5)            30          dense_86[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_88 (Dense)                (None, 5)            30          dense_87[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_89 (Dense)                (None, 5)            30          dense_88[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_90 (Dense)                (None, 5)            30          dense_89[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_91 (Dense)                (None, 5)            30          dense_90[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_92 (Dense)                (None, 5)            30          dense_91[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_93 (Dense)                (None, 5)            30          dense_92[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_94 (Dense)                (None, 5)            30          dense_93[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_95 (Dense)                (None, 5)            30          dense_94[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 15)           0           dense_84[0][0]                   \n",
      "                                                                 dense_91[0][0]                   \n",
      "                                                                 dense_95[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_96 (Dense)                (None, 5)            80          concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_97 (Dense)                (None, 2)            12          dense_96[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,082\n",
      "Trainable params: 2,082\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 167 samples, validate on 83 samples\n",
      "Epoch 1/10\n",
      "167/167 [==============================] - 1s 4ms/step - loss: 0.6962 - val_loss: 0.6929\n",
      "Epoch 2/10\n",
      "167/167 [==============================] - 0s 74us/step - loss: 0.6925 - val_loss: 0.6926\n",
      "Epoch 3/10\n",
      "167/167 [==============================] - 0s 72us/step - loss: 0.6920 - val_loss: 0.6926\n",
      "Epoch 4/10\n",
      "167/167 [==============================] - 0s 80us/step - loss: 0.6916 - val_loss: 0.6923\n",
      "Epoch 5/10\n",
      "167/167 [==============================] - 0s 83us/step - loss: 0.6913 - val_loss: 0.6923\n",
      "Epoch 6/10\n",
      "167/167 [==============================] - 0s 80us/step - loss: 0.6910 - val_loss: 0.6922\n",
      "Epoch 7/10\n",
      "167/167 [==============================] - 0s 80us/step - loss: 0.6906 - val_loss: 0.6921\n",
      "Epoch 8/10\n",
      "167/167 [==============================] - 0s 92us/step - loss: 0.6903 - val_loss: 0.6920\n",
      "Epoch 9/10\n",
      "167/167 [==============================] - 0s 83us/step - loss: 0.6900 - val_loss: 0.6918\n",
      "Epoch 10/10\n",
      "167/167 [==============================] - 0s 71us/step - loss: 0.6897 - val_loss: 0.6916\n",
      "19750/19750 [==============================] - 1s 52us/step\n",
      "- probabilities: [0.50458527 0.5040335  0.505642   0.5042218  0.5043473 ]\n",
      "19750/19750 [==============================] - 1s 54us/step\n",
      "- validation logloss: 0.6930970701447016\n",
      "Writing predictions to predictions.csv\n",
      "\n",
      "Writing predictions to predictions_2019-04-21_15h53m05s_MLP_1.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND' ] = 'tensorflow'\n",
    "os.environ['MKL_THREADING_LAYER'] = 'GNU'\n",
    "import keras as ks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import TensorBoard\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import Dropout, Activation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time \n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Set seed for reproducibility\n",
    "    NAME = \"MLP\"\n",
    "    np.random.seed(0)\n",
    "    print(\"Loading data...\")\n",
    "    # Load the data from the CSV files\n",
    "    training_data = pd.read_csv('train.csv', header=0)\n",
    "    print('original train data shape: {},\\t{} \\n\\n \\t:'.format(training_data.shape[0],training_data.shape[1]))\n",
    "\n",
    "    prediction_data = pd.read_csv('test.csv', header=0)\n",
    "    print('original prediction data shape: {},\\t{} \\n\\n \\t:'.format(prediction_data.shape[0],prediction_data.shape[1]))\n",
    "    \n",
    "    complete_training_data = pd.concat([training_data, prediction_data])\n",
    "    print('total training / valdation shape {}'.format(complete_training_data))\n",
    "    \n",
    "    # Transform the loaded CSV data into numpy arrays\n",
    "\n",
    "    X = training_data.drop([\"target\"], axis=1)\n",
    "    mini= MinMaxScaler(feature_range=(0,1)) \n",
    "    X = mini.fit_transform(X)\n",
    "\n",
    "    Y = training_data[\"target\"]\n",
    "    Y= keras.utils.to_categorical(Y,2) \n",
    "\n",
    "    x_prediction = prediction_data\n",
    "    x_prediction = mini.fit_transform(x_prediction)\n",
    "\n",
    "    ids = prediction_data[\"id\"]  \n",
    "\n",
    "    batch_size = 710\n",
    "\n",
    "    dropout = 0.2\n",
    "\n",
    "    visible = Input(shape=(301,))\n",
    "    hidden1 = Dense(5, activation='relu')(visible)\n",
    "    hidden1 = Dense(5, activation='relu')(hidden1)\n",
    "    hidden1 = Dense(5, activation='relu')(hidden1)\n",
    "    hidden1 = Dense(5, activation='relu')(hidden1)\n",
    "    hidden1 = Dense(5, activation='relu')(hidden1)\n",
    "    hidden1 = Dense(5, activation='relu')(hidden1)\n",
    "    hidden2 = Dense(5, activation='relu')(hidden1)\n",
    "    hidden2 = Dense(5, activation='relu')(hidden2)\n",
    "    hidden2 = Dense(5, activation='relu')(hidden2)\n",
    "    hidden2 = Dense(5, activation='relu')(hidden2)\n",
    "    hidden2 = Dense(5, activation='relu')(hidden2)\n",
    "    hidden2 = Dense(5, activation='relu')(hidden2)\n",
    "    hidden2 = Dense(5, activation='relu')(hidden2)\n",
    "    hidden3 = Dense(5, activation='relu')(hidden2)\n",
    "    hidden3 = Dense(5, activation='relu')(hidden3)\n",
    "    hidden3 = Dense(5, activation='relu')(hidden3)\n",
    "    hidden3 = Dense(5, activation='relu')(hidden3)\n",
    "    merge = keras.layers.concatenate([hidden1,hidden2,hidden3], axis=1)\n",
    "    hidden4 = Dense(5,activation='relu')(merge)\n",
    "    output = Dense(2, activation='sigmoid')(hidden4)\n",
    "    model = Model(inputs=visible, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy',optimizer='rmsprop')\n",
    "    model.summary()\n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "    model.fit(X,Y,batch_size=batch_size,epochs=10,validation_split=0.33,callbacks=[tensorboard])\n",
    "    \n",
    "\n",
    "    y_prediction = model.predict(x_prediction)\n",
    "    evaluate = model.evaluate(x_prediction,y_prediction)\n",
    "    \n",
    "    probabilities = y_prediction[:, 1]\n",
    "    print(\"- probabilities:\", probabilities[1:6])\n",
    "\n",
    "    # We can see the probability does seem to be good at predicting the\n",
    "    # true target correctly.\n",
    "#     print(\"- target:\", prediction_data['target_bernie'][1:6])\n",
    "#     print(\"- rounded probability:\", [np.round(p) for p in probabilities][1:6])\n",
    "\n",
    "#     # But overall the accuracy is very low.\n",
    "#     correct = [\n",
    "#         np.round(x) == y\n",
    "#         for (x, y) in zip(probabilities, prediction_data['target_bernie'])\n",
    "#     ]\n",
    "#     print(\"- accuracy: \", sum(correct) / float(prediction_data.shape[0]))\n",
    "\n",
    "#     tournament_corr = np.corrcoef(prediction_data['target_bernie'],\n",
    "#                                   prediction_data['target_elizabeth'])\n",
    "#     print(\"- bernie vs elizabeth corr:\", tournament_corr)\n",
    "#     # You can see that target_elizabeth is accurate using the bernie model as well.\n",
    "#     correct = [\n",
    "#         np.round(x) == y\n",
    "#         for (x, y) in zip(probabilities, prediction_data['target_elizabeth'])\n",
    "#     ]\n",
    "#     print(\"- elizabeth using bernie:\",\n",
    "#           sum(correct) / float(prediction_data.shape[0]))\n",
    "\n",
    "    # Numerai measures models on logloss instead of accuracy. The lower the logloss the better.\n",
    "    # Numerai only pays models with logloss < 0.693 on the live portion of the tournament data.)\n",
    "\n",
    "    print(\"- validation logloss:\",\n",
    "          model.evaluate(x_prediction,y_prediction))\n",
    "    \n",
    "    results = y_prediction[:, 1]\n",
    "    results_df = pd.DataFrame(data={'target':results})\n",
    "\n",
    "    joined = pd.DataFrame(ids).join(results_df)\n",
    "    pd.DataFrame(joined[:5])\n",
    "\n",
    "\n",
    "    print(\"Writing predictions to predictions.csv\")\n",
    "    path = 'predictions_{:}_{}_1'.format(time.strftime(\"%Y-%m-%d_%Hh%Mm%Ss\", time.gmtime()),NAME) + '.csv'\n",
    "    print()\n",
    "    print(\"Writing predictions to \" + path.strip())\n",
    "    joined.to_csv(path,float_format='%.15f', index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T23:44:32.056590Z",
     "start_time": "2019-04-04T23:42:41.816364Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "original train data shape: 502732,\t60 \n",
      "\n",
      " \t:\n",
      "original prediction data shape: 333917,\t60 \n",
      "\n",
      " \t:\n",
      "total training / valdation shape                       id   era data_type  feature1  feature2  feature3  \\\n",
      "0       n0003126ff2349f6  era1     train   0.54836   0.31077   0.37524   \n",
      "1       n003d773d29b57ec  era1     train   0.34712   0.40275   0.42747   \n",
      "2       n0074df2dc6810b6  era1     train   0.50871   0.48639   0.47544   \n",
      "3       n0090630f530903e  era1     train   0.61363   0.40268   0.53779   \n",
      "4       n00af19089546fe9  era1     train   0.30704   0.47273   0.54495   \n",
      "5       n011d2da12b1e735  era1     train   0.52336   0.59136   0.60506   \n",
      "6       n014149cadeee55d  era1     train   0.30875   0.62510   0.35229   \n",
      "7       n0148a4dcf539aba  era1     train   0.40632   0.30590   0.43227   \n",
      "8       n015855690d31908  era1     train   0.48193   0.27060   0.50228   \n",
      "9       n0169447f4d6a10e  era1     train   0.51191   0.53663   0.42109   \n",
      "10      n01703ba4eff8fe7  era1     train   0.51829   0.52928   0.41085   \n",
      "11      n01b43e631083764  era1     train   0.61895   0.44075   0.61466   \n",
      "12      n01d1368b061433f  era1     train   0.45529   0.28276   0.61272   \n",
      "13      n01d5bfde31734cd  era1     train   0.48717   0.28982   0.38631   \n",
      "14      n0236fa89e606631  era1     train   0.42585   0.56616   0.62732   \n",
      "15      n023dfa017b93739  era1     train   0.51657   0.52352   0.41695   \n",
      "16      n0294f04019898e8  era1     train   0.45073   0.56795   0.25294   \n",
      "17      n029adf0fa156932  era1     train   0.53833   0.43619   0.58892   \n",
      "18      n02ad6457589c4af  era1     train   0.33576   0.54933   0.39919   \n",
      "19      n02bd240c01b24bd  era1     train   0.47469   0.38517   0.53743   \n",
      "20      n02c48c74a9c9719  era1     train   0.83431   0.36147   0.69324   \n",
      "21      n02cc6384ea5a025  era1     train   0.49314   0.31000   0.36538   \n",
      "22      n02e0356c793b120  era1     train   0.65183   0.40688   0.52271   \n",
      "23      n02e96eeb774238f  era1     train   0.67869   0.46261   0.38758   \n",
      "24      n030964ace7cfa93  era1     train   0.59925   0.43286   0.46698   \n",
      "25      n0311b410c7f8b9d  era1     train   0.37634   0.43578   0.43619   \n",
      "26      n03158868e546c31  era1     train   0.50558   0.47651   0.53912   \n",
      "27      n032f0f9107b0115  era1     train   0.46657   0.28710   0.44637   \n",
      "28      n03484f670162e63  era1     train   0.33571   0.45678   0.25402   \n",
      "29      n03534209c17edf5  era1     train   0.35224   0.48464   0.47109   \n",
      "...                  ...   ...       ...       ...       ...       ...   \n",
      "333887  nfe90ce695609608  eraX      live   0.22473   0.31813   0.36786   \n",
      "333888  nfe932f6fe046898  eraX      live   0.57944   0.41360   0.34797   \n",
      "333889  nfe93cb92e1f3004  eraX      live   0.62594   0.53724   0.51025   \n",
      "333890  nfe943152f1a5550  eraX      live   0.18081   0.50046   0.44782   \n",
      "333891  nfe9f241941633d3  eraX      live   0.44638   0.41328   0.59583   \n",
      "333892  nfed818ba71b4665  eraX      live   0.45843   0.49678   0.51053   \n",
      "333893  nfeda644836d563c  eraX      live   0.42282   0.57715   0.32823   \n",
      "333894  nfee50cc929893b4  eraX      live   0.22692   0.41532   0.47914   \n",
      "333895  nfeeb39596d24005  eraX      live   0.67745   0.47704   0.69068   \n",
      "333896  nfeee0c1087730ac  eraX      live   0.30302   0.41184   0.37730   \n",
      "333897  nfef761c30da657a  eraX      live   0.77022   0.39439   0.61509   \n",
      "333898  nff0947a55e1d394  eraX      live   0.61578   0.35275   0.51831   \n",
      "333899  nff1774c41a64a26  eraX      live   0.47422   0.46354   0.53371   \n",
      "333900  nff1ea0914cf90f4  eraX      live   0.30684   0.49977   0.64083   \n",
      "333901  nff3fa4347689e62  eraX      live   0.31594   0.43230   0.58978   \n",
      "333902  nff548af92ef8dc6  eraX      live   0.59936   0.51865   0.36845   \n",
      "333903  nff646fb2b7d329e  eraX      live   0.47254   0.33300   0.49866   \n",
      "333904  nff70242e4559ef9  eraX      live   0.35272   0.34020   0.42336   \n",
      "333905  nff77343157e0ab0  eraX      live   0.50806   0.52088   0.40247   \n",
      "333906  nff86f7979a4483f  eraX      live   0.43828   0.50917   0.31763   \n",
      "333907  nff9843ac1881c92  eraX      live   0.33373   0.45027   0.51017   \n",
      "333908  nffa24ee2fa8d8ae  eraX      live   0.58084   0.60522   0.52742   \n",
      "333909  nffb20c86d6252bf  eraX      live   0.38466   0.36028   0.38189   \n",
      "333910  nffb86f0b9948cab  eraX      live   0.20704   0.65434   0.75437   \n",
      "333911  nffc091bd2914bc3  eraX      live   0.17362   0.44136   0.45643   \n",
      "333912  nffc4e712f2a7c7e  eraX      live   0.34347   0.32474   0.39716   \n",
      "333913  nffd1353a5e2baad  eraX      live   0.55615   0.49798   0.46160   \n",
      "333914  nffd9965e9b4fc2f  eraX      live   0.54227   0.40336   0.30860   \n",
      "333915  nffe587f7f3af1e7  eraX      live   0.26457   0.38589   0.48547   \n",
      "333916  nfffc562888c5ef0  eraX      live   0.59965   0.50705   0.46930   \n",
      "\n",
      "        feature4  feature5  feature6  feature7       ...        feature48  \\\n",
      "0        0.49490   0.53217   0.48388   0.50220       ...          0.55239   \n",
      "1        0.44006   0.47866   0.44055   0.59182       ...          0.46029   \n",
      "2        0.40306   0.53436   0.64028   0.51420       ...          0.40596   \n",
      "3        0.37045   0.58711   0.59900   0.62428       ...          0.53878   \n",
      "4        0.48692   0.47348   0.34695   0.41506       ...          0.46431   \n",
      "5        0.30085   0.41742   0.47290   0.56301       ...          0.55917   \n",
      "6        0.48021   0.71347   0.36977   0.57899       ...          0.40538   \n",
      "7        0.61999   0.51016   0.52714   0.74017       ...          0.77664   \n",
      "8        0.63037   0.50734   0.45545   0.41927       ...          0.52887   \n",
      "9        0.38838   0.51222   0.53249   0.70187       ...          0.58353   \n",
      "10       0.44842   0.49727   0.66198   0.53513       ...          0.54271   \n",
      "11       0.51365   0.45339   0.51101   0.39488       ...          0.33293   \n",
      "12       0.48615   0.54590   0.59111   0.55893       ...          0.55932   \n",
      "13       0.58569   0.46994   0.51753   0.41116       ...          0.55347   \n",
      "14       0.40727   0.36868   0.51526   0.29604       ...          0.53480   \n",
      "15       0.39198   0.54481   0.55880   0.48240       ...          0.45665   \n",
      "16       0.37918   0.65944   0.61828   0.62431       ...          0.44610   \n",
      "17       0.53740   0.32951   0.44737   0.39056       ...          0.64044   \n",
      "18       0.51449   0.44705   0.53717   0.53424       ...          0.53355   \n",
      "19       0.46013   0.63523   0.48265   0.50777       ...          0.48970   \n",
      "20       0.49827   0.22947   0.46324   0.49544       ...          0.75846   \n",
      "21       0.49332   0.68842   0.59665   0.57461       ...          0.54245   \n",
      "22       0.69467   0.62700   0.50894   0.44084       ...          0.64244   \n",
      "23       0.33676   0.43296   0.56874   0.67007       ...          0.63132   \n",
      "24       0.39413   0.58728   0.38304   0.60579       ...          0.50472   \n",
      "25       0.45756   0.39833   0.58444   0.48982       ...          0.58255   \n",
      "26       0.48904   0.43819   0.53960   0.55395       ...          0.49973   \n",
      "27       0.50982   0.63898   0.45895   0.50982       ...          0.52851   \n",
      "28       0.47962   0.50144   0.58369   0.62196       ...          0.79120   \n",
      "29       0.41567   0.57632   0.34524   0.58772       ...          0.47177   \n",
      "...          ...       ...       ...       ...       ...              ...   \n",
      "333887   0.63595   0.63072   0.40458   0.60937       ...          0.43822   \n",
      "333888   0.30309   0.59500   0.62593   0.79868       ...          0.51592   \n",
      "333889   0.35304   0.42196   0.54384   0.40673       ...          0.81155   \n",
      "333890   0.19874   0.31669   0.56764   0.27662       ...          0.41892   \n",
      "333891   0.51726   0.32879   0.35747   0.27011       ...          0.40805   \n",
      "333892   0.42903   0.34398   0.17531   0.45940       ...          0.57967   \n",
      "333893   0.19175   0.54352   0.48392   0.58288       ...          0.61240   \n",
      "333894   0.47158   0.43951   0.51312   0.45735       ...          0.49855   \n",
      "333895   0.49231   0.42467   0.50623   0.21764       ...          0.51575   \n",
      "333896   0.42511   0.37806   0.48737   0.46332       ...          0.54794   \n",
      "333897   0.53305   0.54400   0.58841   0.36758       ...          0.53317   \n",
      "333898   0.48969   0.54165   0.67066   0.40535       ...          0.50956   \n",
      "333899   0.56766   0.53374   0.46601   0.33106       ...          0.36488   \n",
      "333900   0.60022   0.45940   0.27945   0.52223       ...          0.42632   \n",
      "333901   0.40462   0.49036   0.55442   0.55895       ...          0.57192   \n",
      "333902   0.37223   0.70834   0.68634   0.65859       ...          0.65068   \n",
      "333903   0.55127   0.47440   0.32557   0.62731       ...          0.32907   \n",
      "333904   0.60970   0.52893   0.42176   0.53982       ...          0.45713   \n",
      "333905   0.41444   0.50771   0.75231   0.51795       ...          0.65090   \n",
      "333906   0.45235   0.68345   0.47711   0.59222       ...          0.44174   \n",
      "333907   0.34386   0.53338   0.49982   0.48504       ...          0.35708   \n",
      "333908   0.24781   0.53824   0.71670   0.53123       ...          0.64067   \n",
      "333909   0.50206   0.44006   0.57703   0.51574       ...          0.46857   \n",
      "333910   0.43345   0.43357   0.26700   0.38337       ...          0.35225   \n",
      "333911   0.63990   0.56944   0.37104   0.36184       ...          0.42273   \n",
      "333912   0.67911   0.49659   0.37694   0.47598       ...          0.28455   \n",
      "333913   0.42583   0.53056   0.51423   0.72328       ...          0.51966   \n",
      "333914   0.41165   0.51681   0.63769   0.67705       ...          0.75881   \n",
      "333915   0.58102   0.57494   0.34050   0.49688       ...          0.40776   \n",
      "333916   0.38863   0.56402   0.47352   0.66432       ...          0.56368   \n",
      "\n",
      "        feature49  feature50  target_bernie  target_elizabeth  target_jordan  \\\n",
      "0         0.64054    0.52182            1.0               1.0            1.0   \n",
      "1         0.62941    0.55010            1.0               1.0            1.0   \n",
      "2         0.54731    0.39061            0.0               0.0            1.0   \n",
      "3         0.47776    0.36835            0.0               0.0            0.0   \n",
      "4         0.49482    0.60452            1.0               1.0            1.0   \n",
      "5         0.31260    0.35691            0.0               0.0            0.0   \n",
      "6         0.52690    0.39767            1.0               1.0            1.0   \n",
      "7         0.51623    0.49104            0.0               0.0            0.0   \n",
      "8         0.44384    0.57269            0.0               0.0            0.0   \n",
      "9         0.42566    0.50082            1.0               1.0            1.0   \n",
      "10        0.43434    0.52720            0.0               0.0            0.0   \n",
      "11        0.30265    0.45951            0.0               0.0            0.0   \n",
      "12        0.46798    0.53359            1.0               1.0            1.0   \n",
      "13        0.42172    0.72972            0.0               0.0            0.0   \n",
      "14        0.48525    0.59524            1.0               1.0            1.0   \n",
      "15        0.50711    0.59955            0.0               0.0            0.0   \n",
      "16        0.57642    0.62569            0.0               0.0            0.0   \n",
      "17        0.22801    0.44203            1.0               1.0            1.0   \n",
      "18        0.49124    0.39731            1.0               1.0            1.0   \n",
      "19        0.41920    0.38751            0.0               0.0            0.0   \n",
      "20        0.31290    0.61816            1.0               1.0            1.0   \n",
      "21        0.40880    0.53372            0.0               1.0            0.0   \n",
      "22        0.17882    0.40123            0.0               0.0            0.0   \n",
      "23        0.46647    0.53402            0.0               0.0            0.0   \n",
      "24        0.33878    0.56262            0.0               0.0            0.0   \n",
      "25        0.52865    0.63495            0.0               0.0            0.0   \n",
      "26        0.47976    0.44513            0.0               0.0            0.0   \n",
      "27        0.43489    0.48806            0.0               0.0            0.0   \n",
      "28        0.72922    0.49451            0.0               0.0            0.0   \n",
      "29        0.55393    0.49058            1.0               1.0            1.0   \n",
      "...           ...        ...            ...               ...            ...   \n",
      "333887    0.56037    0.68570            NaN               NaN            NaN   \n",
      "333888    0.50056    0.47213            NaN               NaN            NaN   \n",
      "333889    0.66506    0.41550            NaN               NaN            NaN   \n",
      "333890    0.56516    0.38036            NaN               NaN            NaN   \n",
      "333891    0.43037    0.49400            NaN               NaN            NaN   \n",
      "333892    0.60226    0.56339            NaN               NaN            NaN   \n",
      "333893    0.53195    0.47703            NaN               NaN            NaN   \n",
      "333894    0.65543    0.56419            NaN               NaN            NaN   \n",
      "333895    0.36139    0.43150            NaN               NaN            NaN   \n",
      "333896    0.58540    0.56483            NaN               NaN            NaN   \n",
      "333897    0.34749    0.43542            NaN               NaN            NaN   \n",
      "333898    0.53561    0.56108            NaN               NaN            NaN   \n",
      "333899    0.52181    0.30183            NaN               NaN            NaN   \n",
      "333900    0.44151    0.68143            NaN               NaN            NaN   \n",
      "333901    0.66334    0.52303            NaN               NaN            NaN   \n",
      "333902    0.48645    0.37984            NaN               NaN            NaN   \n",
      "333903    0.48351    0.58909            NaN               NaN            NaN   \n",
      "333904    0.46877    0.64382            NaN               NaN            NaN   \n",
      "333905    0.60046    0.48012            NaN               NaN            NaN   \n",
      "333906    0.38848    0.68848            NaN               NaN            NaN   \n",
      "333907    0.54039    0.58356            NaN               NaN            NaN   \n",
      "333908    0.49853    0.49006            NaN               NaN            NaN   \n",
      "333909    0.55217    0.52806            NaN               NaN            NaN   \n",
      "333910    0.43661    0.55406            NaN               NaN            NaN   \n",
      "333911    0.67505    0.47899            NaN               NaN            NaN   \n",
      "333912    0.42267    0.60347            NaN               NaN            NaN   \n",
      "333913    0.43774    0.54886            NaN               NaN            NaN   \n",
      "333914    0.48803    0.56248            NaN               NaN            NaN   \n",
      "333915    0.38404    0.64771            NaN               NaN            NaN   \n",
      "333916    0.33147    0.42495            NaN               NaN            NaN   \n",
      "\n",
      "        target_ken  target_charles  target_frank  target_hillary  \n",
      "0              1.0             1.0           1.0             1.0  \n",
      "1              1.0             1.0           1.0             1.0  \n",
      "2              0.0             0.0           0.0             0.0  \n",
      "3              0.0             0.0           0.0             0.0  \n",
      "4              1.0             1.0           1.0             1.0  \n",
      "5              0.0             0.0           0.0             0.0  \n",
      "6              1.0             1.0           1.0             1.0  \n",
      "7              0.0             0.0           0.0             0.0  \n",
      "8              0.0             0.0           0.0             0.0  \n",
      "9              1.0             1.0           1.0             1.0  \n",
      "10             0.0             0.0           0.0             0.0  \n",
      "11             0.0             0.0           0.0             0.0  \n",
      "12             1.0             1.0           1.0             1.0  \n",
      "13             0.0             0.0           0.0             0.0  \n",
      "14             1.0             1.0           1.0             1.0  \n",
      "15             0.0             1.0           1.0             0.0  \n",
      "16             0.0             0.0           0.0             0.0  \n",
      "17             1.0             1.0           1.0             1.0  \n",
      "18             1.0             1.0           1.0             1.0  \n",
      "19             0.0             0.0           0.0             1.0  \n",
      "20             1.0             1.0           1.0             1.0  \n",
      "21             0.0             0.0           1.0             1.0  \n",
      "22             0.0             0.0           0.0             0.0  \n",
      "23             0.0             0.0           0.0             0.0  \n",
      "24             0.0             0.0           0.0             0.0  \n",
      "25             0.0             0.0           0.0             0.0  \n",
      "26             0.0             0.0           0.0             0.0  \n",
      "27             1.0             0.0           0.0             1.0  \n",
      "28             0.0             0.0           0.0             0.0  \n",
      "29             1.0             1.0           1.0             1.0  \n",
      "...            ...             ...           ...             ...  \n",
      "333887         NaN             NaN           NaN             NaN  \n",
      "333888         NaN             NaN           NaN             NaN  \n",
      "333889         NaN             NaN           NaN             NaN  \n",
      "333890         NaN             NaN           NaN             NaN  \n",
      "333891         NaN             NaN           NaN             NaN  \n",
      "333892         NaN             NaN           NaN             NaN  \n",
      "333893         NaN             NaN           NaN             NaN  \n",
      "333894         NaN             NaN           NaN             NaN  \n",
      "333895         NaN             NaN           NaN             NaN  \n",
      "333896         NaN             NaN           NaN             NaN  \n",
      "333897         NaN             NaN           NaN             NaN  \n",
      "333898         NaN             NaN           NaN             NaN  \n",
      "333899         NaN             NaN           NaN             NaN  \n",
      "333900         NaN             NaN           NaN             NaN  \n",
      "333901         NaN             NaN           NaN             NaN  \n",
      "333902         NaN             NaN           NaN             NaN  \n",
      "333903         NaN             NaN           NaN             NaN  \n",
      "333904         NaN             NaN           NaN             NaN  \n",
      "333905         NaN             NaN           NaN             NaN  \n",
      "333906         NaN             NaN           NaN             NaN  \n",
      "333907         NaN             NaN           NaN             NaN  \n",
      "333908         NaN             NaN           NaN             NaN  \n",
      "333909         NaN             NaN           NaN             NaN  \n",
      "333910         NaN             NaN           NaN             NaN  \n",
      "333911         NaN             NaN           NaN             NaN  \n",
      "333912         NaN             NaN           NaN             NaN  \n",
      "333913         NaN             NaN           NaN             NaN  \n",
      "333914         NaN             NaN           NaN             NaN  \n",
      "333915         NaN             NaN           NaN             NaN  \n",
      "333916         NaN             NaN           NaN             NaN  \n",
      "\n",
      "[836649 rows x 60 columns]\n",
      "['feature1', 'feature2', 'feature3', 'feature4', 'feature5', 'feature6', 'feature7', 'feature8', 'feature9', 'feature10', 'feature11', 'feature12', 'feature13', 'feature14', 'feature15', 'feature16', 'feature17', 'feature18', 'feature19', 'feature20', 'feature21', 'feature22', 'feature23', 'feature24', 'feature25', 'feature26', 'feature27', 'feature28', 'feature29', 'feature30', 'feature31', 'feature32', 'feature33', 'feature34', 'feature35', 'feature36', 'feature37', 'feature38', 'feature39', 'feature40', 'feature41', 'feature42', 'feature43', 'feature44', 'feature45', 'feature46', 'feature47', 'feature48', 'feature49', 'feature50']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 336830 samples, validate on 165902 samples\n",
      "Epoch 1/10\n",
      "336830/336830 [==============================] - 7s 21us/step - loss: 0.7100 - val_loss: 0.6933\n",
      "Epoch 2/10\n",
      "336830/336830 [==============================] - 7s 20us/step - loss: 0.6951 - val_loss: 0.6930\n",
      "Epoch 3/10\n",
      "336830/336830 [==============================] - 6s 19us/step - loss: 0.6928 - val_loss: 0.6937\n",
      "Epoch 4/10\n",
      "336830/336830 [==============================] - 6s 19us/step - loss: 0.6922 - val_loss: 0.6933\n",
      "Epoch 5/10\n",
      "336830/336830 [==============================] - 6s 19us/step - loss: 0.6920 - val_loss: 0.6951\n",
      "Epoch 6/10\n",
      "336830/336830 [==============================] - 7s 19us/step - loss: 0.6918 - val_loss: 0.6949\n",
      "Epoch 7/10\n",
      "336830/336830 [==============================] - 6s 19us/step - loss: 0.6916 - val_loss: 0.6941\n",
      "Epoch 8/10\n",
      "336830/336830 [==============================] - 6s 19us/step - loss: 0.6914 - val_loss: 0.6937\n",
      "Epoch 9/10\n",
      "336830/336830 [==============================] - 6s 19us/step - loss: 0.6912 - val_loss: 0.6936\n",
      "Epoch 10/10\n",
      "336830/336830 [==============================] - 6s 19us/step - loss: 0.6910 - val_loss: 0.6944\n",
      "333917/333917 [==============================] - 8s 23us/step\n",
      "- probabilities: [0.5038235  0.47747394 0.52584994 0.50304914 0.508566  ]\n",
      "- target: 1    1.0\n",
      "2    0.0\n",
      "3    0.0\n",
      "4    0.0\n",
      "5    1.0\n",
      "Name: target_bernie, dtype: float64\n",
      "- rounded probability: [1.0, 0.0, 1.0, 1.0, 1.0]\n",
      "- accuracy:  0.08481448982831062\n",
      "- bernie vs elizabeth corr: [[nan nan]\n",
      " [nan nan]]\n",
      "- elizabeth using bernie: 0.08516787105777783\n",
      "333917/333917 [==============================] - 8s 23us/step\n",
      "- validation logloss: 0.6917378780136868\n",
      "Writing predictions to predictions.csv\n",
      "\n",
      "Writing predictions to predictions_2019-04-04_23h44m30s.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND' ] = 'tensorflow'\n",
    "os.environ['MKL_THREADING_LAYER'] = 'GNU'\n",
    "import keras as ks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import TensorBoard\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import Dropout, Activation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time \n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Set seed for reproducibility\n",
    "    NAME = \"MLP\"\n",
    "    np.random.seed(0)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Loading data...\")\n",
    "\n",
    "    # Load the data from the CSV files\n",
    "\n",
    "    training_data = pd.read_csv('numerai_training_data.csv', header=0)\n",
    "    print('original train data shape: {},\\t{} \\n\\n \\t:'.format(training_data.shape[0],training_data.shape[1]))\n",
    "\n",
    "    prediction_data = pd.read_csv('numerai_tournament_data.csv', header=0)\n",
    "    print('original prediction data shape: {},\\t{} \\n\\n \\t:'.format(prediction_data.shape[0],prediction_data.shape[1]))\n",
    "    \n",
    "    complete_training_data = pd.concat([training_data, prediction_data])\n",
    "    print('total training / valdation shape {}'.format(complete_training_data))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Transform the loaded CSV data into numpy arrays\n",
    "\n",
    "    features = [f for f in list(training_data) if \"feature\" in f]\n",
    "    print(features)\n",
    "\n",
    "    X = training_data[features]\n",
    "    mini= MinMaxScaler(feature_range=(0,1)) \n",
    "    X = mini.fit_transform(X)\n",
    "#     X = X.values\n",
    "    Y = training_data[\"target\"]\n",
    "    Y= keras.utils.to_categorical(Y,2) \n",
    "\n",
    "    x_prediction = prediction_data[features]\n",
    "    x_prediction = mini.fit_transform(x_prediction)\n",
    "\n",
    "    ids = prediction_data[\"id\"]\n",
    "\n",
    "#     X = X.values\n",
    "\n",
    "#     Y = to_categorical(Y, num_classes=2)\n",
    "\n",
    "    \n",
    "\n",
    "    batch_size = 710\n",
    "\n",
    "    dropout = 0.2\n",
    "\n",
    "    \n",
    "\n",
    "    m_in = Input(shape=(50,))\n",
    "\n",
    "    m1 = Dense(50,)(m_in)\n",
    "    m1 = Activation('relu')(m1)\n",
    "    m1 = BatchNormalization(momentum=.99999,axis=-1)(m1)\n",
    "\n",
    "    m2 = Dense(100)(m1)\n",
    "    m2 = Activation('relu')(m2)\n",
    "    m2 = BatchNormalization(momentum=.999,axis=-1)(m2)\n",
    "    \n",
    "    m3 = Dense(25)(m2)\n",
    "    m3 = Activation('relu')(m3)\n",
    "    \n",
    "    m3 = Dense(25)(m3)\n",
    "    m3 = Dropout(dropout)(m3) \n",
    "    m3 = Activation('relu')(m3)\n",
    "    \n",
    "    m3 = Dense(25)(m3)\n",
    "    m3 = Activation('relu')(m3)\n",
    "    m3 = BatchNormalization(momentum=.99,axis=-1)(m3)\n",
    "    \n",
    "    m3 = Dense(100)(m3)\n",
    "    m3 = Activation('relu')(m3)\n",
    "    \n",
    "    m3 = Dense(25)(m3)\n",
    "    m3 = Activation('relu')(m3) \n",
    "    \n",
    "    m4 = Dense(25)(m3)\n",
    "    m4 = Activation('relu')(m4) \n",
    "    m4 = Dropout(dropout)(m4) \n",
    "    m4 = BatchNormalization(momentum=.9,axis=-1)(m4)\n",
    "\n",
    "    \n",
    "    \n",
    "    m5 = Dense(2)(m4)\n",
    "    m_out = Activation('sigmoid')(m5)\n",
    "\n",
    "    model = Model(inputs=m_in, outputs=m_out)\n",
    "\n",
    "\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',optimizer='rmsprop')\n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "    model.fit(X,Y,batch_size=batch_size,epochs=10,validation_split=0.33,callbacks=[tensorboard])\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    y_prediction = model.predict(x_prediction)\n",
    "    evaluate = model.evaluate(x_prediction,y_prediction)\n",
    "    \n",
    "    probabilities = y_prediction[:, 1]\n",
    "    print(\"- probabilities:\", probabilities[1:6])\n",
    "\n",
    "    # We can see the probability does seem to be good at predicting the\n",
    "    # true target correctly.\n",
    "    print(\"- target:\", prediction_data['target'][1:6])\n",
    "    print(\"- rounded probability:\", [np.round(p) for p in probabilities][1:6])\n",
    "\n",
    "    # But overall the accuracy is very low.\n",
    "    correct = [\n",
    "        np.round(x) == y\n",
    "        for (x, y) in zip(probabilities, prediction_data['target'])\n",
    "    ]\n",
    "    print(\"- accuracy: \", sum(correct) / float(prediction_data.shape[0]))\n",
    "\n",
    "    # The targets for each of the tournaments are very correlated.\n",
    "    tournament_corr = np.corrcoef(prediction_data['target'],\n",
    "                                  prediction_data['target_elizabeth'])\n",
    "    print(\"- bernie vs elizabeth corr:\", tournament_corr)\n",
    "    # You can see that target_elizabeth is accurate using the bernie model as well.\n",
    "    correct = [\n",
    "        np.round(x) == y\n",
    "        for (x, y) in zip(probabilities, prediction_data['target_elizabeth'])\n",
    "    ]\n",
    "    print(\"- elizabeth using bernie:\",\n",
    "          sum(correct) / float(prediction_data.shape[0]))\n",
    "\n",
    "    # Numerai measures models on logloss instead of accuracy. The lower the logloss the better.\n",
    "    # Numerai only pays models with logloss < 0.693 on the live portion of the tournament data.)\n",
    "\n",
    "    print(\"- validation logloss:\",\n",
    "          model.evaluate(x_prediction,y_prediction))\n",
    "\n",
    "    results = y_prediction[:, 1]\n",
    "\n",
    "    # -----\n",
    "\n",
    "    \n",
    "\n",
    "    results_df = pd.DataFrame(data={'probability_bernie':results})\n",
    "\n",
    "    joined = pd.DataFrame(ids).join(results_df)\n",
    "    pd.DataFrame(joined[:5])\n",
    "\n",
    "\n",
    "    print(\"Writing predictions to predictions.csv\")\n",
    "\n",
    "    # Save the predictions out to a CSV file\n",
    "    path = 'predictions_{:}'.format(time.strftime(\"%Y-%m-%d_%Hh%Mm%Ss\", time.gmtime())) + '.csv'\n",
    "    print()\n",
    "    print(\"Writing predictions to \" + path.strip())\n",
    "    # # Save the predictions out to a CSV file\n",
    "    joined.to_csv(path,float_format='%.15f', index=False)\n",
    "\n",
    "\n",
    "    # Now you can upload these predictions on numer.ai\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    m4 = Dense(25)(m3)\n",
    "    m4 = Activation('sigmoid')(m4)\n",
    "    m4 = Dropout(dropout)(m4) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
